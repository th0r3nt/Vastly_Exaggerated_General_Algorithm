
========================================
# Содержимое файла: assistant_background_tasks/background_tasks.py
========================================

# Когда-нибудь, когда-нибудь

# Базовый скелет
import schedule
import time
import random
from assistant_brain.proactive_logic import run_proactive_check # Эту функцию нужно будет создать

def start_scheduler():
    # Запускать проактивную проверку каждые 5-15 минут
    schedule.every(random.randint(5, 15)).minutes.do(run_proactive_check)

    while True:
        schedule.run_pending()
        time.sleep(1)

        # В main.py:
        # ...
        # scheduler_thread = threading.Thread(target=start_scheduler, daemon=True)
        # scheduler_thread.start()
        # ...
        # # Теперь основной цикл while True: time.sleep(1) не нужен,
        # # программа будет жить, пока работают потоки.



# Анализ погоды. Что делает: Через API погоды (например, OpenWeatherMap) проверяет прогноз на сегодня и завтра (Изменено: передавать эти данные в отдельную функцию "ты заскучала, ...", так будет более эффективно)
# Анализ новостей. Что делает: Проверяет новости на наличие важных событий, которые могут повлиять на планы пользователя (Изменено: передавать эти данные в отдельную функцию "ты заскучала, ...", так будет более эффективно)
# Анализ календаря. Что делает: Проверяет календарь пользователя на наличие предстоящих событий или встреч (Изменено: передавать эти данные в отдельную функцию "ты заскучала, ...", так будет более эффективно)
# Анализ системных ресурсов. Что делает: Мониторит использование CPU, памяти и диска, чтобы предупредить пользователя о возможных проблемах (Изменено: передавать эти данные в отдельную функцию "ты заскучала, ...", так будет более эффективно)




# В main.py:
# ...
# scheduler_thread = threading.Thread(target=start_scheduler, daemon=True)
# scheduler_thread.start()
# ...
# # Теперь основной цикл while True: time.sleep(1) не нужен,
# # программа будет жить, пока работают потоки.




========================================
# Содержимое файла: assistant_brain/added_skills.py
========================================

# added_skills.py

# 1. ИМПОРТ СХЕМЫ НОВОГО ИНСТРУМЕНТА
from assistant_tools.skills_diagrams import ( # Базовые навыки
    get_weather_scheme, search_in_google_scheme, get_date_scheme, get_time_scheme, make_screenshot_scheme, save_to_memory_scheme, lock_pc_scheme,
    get_windows_layout_scheme, move_mouse_scheme, current_mouse_coordinates_scheme, click_mouse_scheme, scroll_mouse_scheme, drag_mouse_scheme,
    press_hotkey_scheme, copy_to_clipboard_scheme, write_text_scheme, system_command_scheme, get_processes_scheme, currently_open_windows_scheme,
    get_system_volume_scheme, set_system_volume_scheme, decrease_volume_scheme,
    increase_volume_scheme, get_habr_news_scheme, get_system_metrics_scheme,
)
from assistant_tools.music_skills_diagrams import ( # Отдельные музыкальные навыки
    music_play_random_scheme, music_pause_playback_scheme, music_resume_playback_scheme, music_play_next_track_scheme,
    music_play_previous_track_scheme, music_clear_playlist_scheme, music_play_playlist_scheme, music_play_track_scheme,
    music_play_random_album_scheme, 
)

from assistant_tools.socialmedia_skills_diagrams import ( # Навыки для соцсетей
    get_telegram_channel_info_scheme,
)

import assistant_tools.skills
import assistant_tools.music_skills
import assistant_tools.socialmedia_skills

# 2. РЕГИСТРАЦИЯ JSON-СХЕМЫ НОВОГО ИНСТРУМЕНТА ДЛЯ FUNCTION CALLING НЕЙРОСЕТИ (Чтобы нейросеть читала описание навыков и могла понимать, что и с какими параметрами вызывать навыки)
function_declarations = [
    # БАЗОВЫЕ НАВЫКИ
    get_weather_scheme, 
    search_in_google_scheme, 
    get_date_scheme, 
    get_time_scheme, 
    make_screenshot_scheme, 
    save_to_memory_scheme, 
    lock_pc_scheme,
    get_habr_news_scheme,
    get_system_metrics_scheme,

    # УПРАВЛЕНИЕ СИСТЕМНЫМ ЗВУКОМ
    get_system_volume_scheme,
    set_system_volume_scheme, 
    decrease_volume_scheme,
    increase_volume_scheme,

    # НАВЫКИ, КОТОРЫЕ УПРАВЛЯЮТ МЫШЬЮ И КЛАВИАТУРОЙ
    get_windows_layout_scheme, 
    move_mouse_scheme, 
    current_mouse_coordinates_scheme, 
    click_mouse_scheme, 
    scroll_mouse_scheme, 
    drag_mouse_scheme,
    press_hotkey_scheme, 
    copy_to_clipboard_scheme, 
    write_text_scheme, 
    system_command_scheme, 

    # НАВЫКИ, СВЯЗАННЫЕ С ВЗАИМОДЕЙСТВИЕМ С ПРИЛОЖЕНИЯМИ И ОКНАМИ
    get_processes_scheme, 
    currently_open_windows_scheme,

    # НАВЫКИ ДЛЯ СОЦСЕТЕЙ
    get_telegram_channel_info_scheme,

    # НАВЫКИ, СВЯЗАННЫЕ С МУЗЫКОЙ ИЗ FOOBAR2000
    music_play_random_scheme, 
    music_pause_playback_scheme, 
    music_resume_playback_scheme, 
    music_play_next_track_scheme,
    music_play_previous_track_scheme, 
    music_clear_playlist_scheme, 
    music_play_playlist_scheme, 
    music_play_track_scheme,
    music_play_random_album_scheme,
    
]

# 3. УКАЗАНИЕ, КАКОЙ НАВЫК ИСПОЛЬЗОВАТЬ (НЕЙРОСЕТЬ БУДЕТ ВЫЗЫВАТЬ КЛЮЧИ (возможно также будет передавать что-либо), И В ДАННОМ СЛУЧАЕ ЗНАЧЕНИЕ КЛЮЧА АКТИВИРУЕТ СООВЕТСТВУЮЩИЙ НАВЫК ДЛЯ ЭТОГО КЛЮЧА)
skills_registry = {
    # БАЗОВЫЕ НАВЫКИ
    "get_weather": assistant_tools.skills.get_weather, # Правильные ключи брать из файла skills_diagrams.py по ключу "name"
    "search_in_google": assistant_tools.skills.search_in_google,
    "get_date": assistant_tools.skills.get_date,
    "get_time": assistant_tools.skills.get_time,
    "make_screenshot": assistant_tools.skills.make_screenshot,
    "save_to_memory": assistant_tools.skills.save_to_memory,
    "lock_pc": assistant_tools.skills.lock_pc,
    "get_habr_news": assistant_tools.skills.get_habr_news,
    "get_system_metrics": assistant_tools.skills.get_system_metrics,

    # УПРАВЛЕНИЕ СИСТЕМНЫМ ЗВУКОМ
    "get_system_volume": assistant_tools.skills.get_system_volume,
    "set_system_volume": assistant_tools.skills.set_system_volume,
    "decrease_volume": assistant_tools.skills.decrease_volume,
    "increase_volume": assistant_tools.skills.increase_volume,

    # НАВЫКИ, КОТОРЫЕ УПРАВЛЯЮТ МЫШЬЮ И КЛАВИАТУРОЙ
    "get_windows_layout": assistant_tools.skills.get_windows_layout, 
    "move_mouse": assistant_tools.skills.move_mouse, 
    "current_mouse_coordinates": assistant_tools.skills.current_mouse_coordinates, 
    "click_mouse": assistant_tools.skills.click_mouse,
    "scroll_mouse": assistant_tools.skills.scroll_mouse, 
    "drag_mouse": assistant_tools.skills.drag_mouse, 
    "press_hotkey": assistant_tools.skills.press_hotkey,
    "copy_to_clipboard": assistant_tools.skills.copy_to_clipboard,
    "write_text": assistant_tools.skills.write_text,
    "system_command": assistant_tools.skills.system_command,

    # НАВЫКИ, СВЯЗАННЫЕ С ВЗАИМОДЕЙСТВИЕМ С ПРИЛОЖЕНИЯМИ И ОКНАМИ
    "get_processes": assistant_tools.skills.get_processes,
    "currently_open_windows": assistant_tools.skills.currently_open_windows,

    # НАВЫКИ ДЛЯ СОЦСЕТЕЙ
    "get_telegram_channel_info": assistant_tools.socialmedia_skills.get_telegram_channel_info,

    # НАВЫКИ, СВЯЗАННЫЕ С МУЗЫКОЙ ИЗ FOOBAR2000
    "music_play_random": assistant_tools.music_skills.music_play_random,
    "music_pause_playback": assistant_tools.music_skills.music_pause_playback,
    "music_resume_playback": assistant_tools.music_skills.music_resume_playback,
    "music_play_next_track": assistant_tools.music_skills.music_play_next_track,
    "music_play_previous_track": assistant_tools.music_skills.music_play_previous_track,
    "music_clear_playlist": assistant_tools.music_skills.music_clear_playlist,
    "music_play_playlist": assistant_tools.music_skills.music_play_playlist,
    "music_play_track": assistant_tools.music_skills.music_play_track,
    "music_play_random_album": assistant_tools.music_skills.music_play_random_album,
}



========================================
# Содержимое файла: assistant_brain/brain.py
========================================

# brain.py
import google.generativeai as genai
from google import genai  # noqa: F811
from google.genai import types
import threading
import os
import json
import datetime
from collections import deque
import logging
from dotenv import load_dotenv
from assistant_event_bus.event_bus import subscribe, publish
from assistant_tools.utils import play_sfx
import assistant_general.general_settings as general_settings
from assistant_general.general_tools import read_json, write_json
from assistant_brain.added_skills import function_declarations, skills_registry # ДОБАВЛЯТЬ НОВЫЕ УМЕНИЯ В ЭТОТ ФАЙЛ
from assistant_general.logger_config import setup_logger
from assistant_tools.skills import get_screenshot_context, get_time, get_date, get_habr_news, get_processes, get_system_volume

setup_logger()
logger = logging.getLogger(__name__)

load_dotenv() # для загрузки API ключей из .env
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

client = genai.Client(api_key=GEMINI_API_KEY)
tools = types.Tool(function_declarations=function_declarations)
config = types.GenerateContentConfig(tools=[tools])

try:
    with open(general_settings.SHORT_TERM_MEMORY_PATH, 'r', encoding='utf-8') as f:
        short_term_memory = json.load(f)
except (FileNotFoundError, json.JSONDecodeError):
    print("Файла 'short_time_memory.json' либо не существует, либо неверного формата. Создается новый в папку 'assistant_brain'.")
    play_sfx("error")
    # Если файла нет или он испорчен
    short_term_memory = []

def save_memory():
    with open(general_settings.SHORT_TERM_MEMORY_PATH, 'w', encoding='utf-8') as f:
        json.dump(list(short_term_memory), f, indent=4, ensure_ascii=False) # ensure_ascii чтобы русский текст от пользователя записывался корректно

short_term_memory = deque(short_term_memory, maxlen=general_settings.MAX_MEMORY) # Применяем deque к загруженному списку, чтобы снова включить лимит

def _process_interaction(query, final_text_to_publish):
    """Сохраняет запрос пользователя и ответ Веги в кратковременную память, записывая конкретное время общения."""
    current_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    short_term_memory.append(f"{current_date}, User (from main pc): {query}")
    short_term_memory.append(f"{current_date}, V.E.G.A.: {final_text_to_publish}")
    save_memory() # Важно: Сохранять нужно либо после каждого добавления, либо при завершении программы

def _run_gemini_task(**kwargs):
    play_sfx("execution")
    now = datetime.datetime.now()
    time_str = now.strftime("%H:%M")
    query = kwargs.get('query')
    database_context = kwargs.get('database_context')
    img = kwargs.get("img", None)
    try:
        initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE,
            f"""Right now, your task is to maintain a conversation. 
            Don't deviate from your personality. Your name is feminine. You can take the initiative or give proactive responses (like suggesting the next step), 
            but ONLY if the situation calls for it and if you think it's necessary. Basically, sometimes think about what Sir might need next.
            Your basic style calls for brevity. However, don't be afraid to add a second sentence if it provides valuable advice, anticipates a need, or ties together facts. Do so if it's truly helpful.
            

            Here's the relevant information from your database (memory). Use it to provide the most complete answer. If the information is irrelevant, you can ignore it.
            {database_context}

            Here's the previous dialogue (useful for context):
            {short_term_memory}

            User request: {time_str} {query}
            V.E.G.A: 
            """
        ]
        # Добавляем изображение, только если оно было успешно создано
        if img:
            initial_contents.append(img)

        # Отправляем первый запрос
        response = client.models.generate_content(
            model=general_settings.MODEL_GEMINI,
            contents=initial_contents,
            config=config,
        )

        function_calls = False
        results_of_tool_calls = []
        text_parts = []
        
        # Запоминаем историю первого ответа для второго запроса
        history = response.candidates[0].content

        # Проверяем наличие вызовов Function Calling
        for part in history.parts:
            if hasattr(part, 'function_call') and part.function_call is not None:
                function_call = part.function_call
                print(f"\nFunction to call: {function_call.name}")
                print(f"Arguments: {function_call.args}\n")

                function_calls = True

                function_to_call = skills_registry[function_call.name]
                result = function_to_call(**function_call.args)

                function_response_part = types.Part(
                    function_response=types.FunctionResponse(
                        name=function_call.name,
                        response={'result': result}
                    )
                )
                results_of_tool_calls.append(function_response_part)

            if hasattr(part, 'text'):
                text_parts.append(part.text)

        final_text = None
        if function_calls:
            # Передаем всё: личность, историю, результаты и СНОВА скриншот
            follow_up_contents = [
                general_settings.VEGA_PERSONALITY_CORE, 
                "Никогда не молчи, если вызвала функцию и считаешь, что тебе нужно промолчать.",
                history, 
                *results_of_tool_calls
            ]
            if img:
                follow_up_contents.append(img)

            final_response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=follow_up_contents,
                config=config,
            )
            final_text = final_response.text 
        else:
            final_text = "".join(text_parts)

        # Финальная проверка сделано из-за того, что при вызове Function Calling нейросеть может посчитать, что нужно промолчать, ответив None или "" (что вызовет ошибку, так как дальше идет replace, который не работает с пустыми значениями)
        if final_text: # Проверка правильно ловит None, и ""
            final_text_to_publish = final_text.replace("*", "").replace("#", "").replace("V.E.G.A.", "VEGA").replace("&", "and")
        else:
            final_text_to_publish = "Выполнено, Сэр." # Безопасный ответ-заглушка

        final_text_to_publish = final_text_to_publish.replace("*", "").replace("#", "").replace("V.E.G.A.", "VEGA").replace("&", "and")
        
        play_sfx("execution")
        print(f"V.E.G.A.: {final_text_to_publish}")
        publish("GEMINI_RESPONSE", text=final_text_to_publish)
        _process_interaction(query, final_text_to_publish)

    except Exception as e: # Более общее исключение
        play_sfx("error")
        print(f"Error when addressing Gemini API: {e}")

def generate_response(*args, **kwargs):
    """Принимает запрос пользователя, контекст из векторной базы данных, делает контекстный скриншот и вызывает _run_gemini_task, передавая необходимые данные в ОТДЕЛЬНОМ ПОТОКЕ."""
    if not args:
        print("*args not found")
        play_sfx("silent_error")
        return
    
    data_package = args[0] # Нужен весь словарь целиком
    
    # Когда у нас есть словарь, достаем из него данные по ключам
    query = data_package.get('original_query')
    database_context = data_package.get('database_context')

    if not query:
        print("Query not found")
        return
    
    image_context = get_screenshot_context() # Получаем в контекст изображение экрана

    play_sfx('silent_execution')

    worker_thread = threading.Thread(
        target=_run_gemini_task, 
        kwargs={"query": query, "database_context": database_context, "img": image_context}
    )
    worker_thread.start()

    print("\n[Brain] The task for Gemini has been sent to the background.")

def initialize_brain():
    """Подписывается на событие обнаруженной речи (или текстового запроса) от пользователя, указывает, что применять при появлении этого события."""
    subscribe("USER_SPEECH_AND_RECORDS_FOUND_IN_DB", generate_response)

def generate_general_greeting():
    """Генерирует приветствие при любом запуске Веги. Если Вега запущена впервые за день: проводит утренний брифинг; иначе стандартно приветствует."""
    tasks_file = "assistant_background_tasks\\tasks_completed_today.json"
    tasks_completed_today = read_json(tasks_file) # Читаем файл с выполненными задачами

    now = datetime.datetime.now()
    today_date_str = now.strftime("%Y-%m-%d") # Результат в формате: "2025-10-17"
    current_hour = now.hour  # Получаем час как int, чтобы лучше сравнить с BRIEFING_START_HOUR: если текущий час больше, чем, к примеру, 5 часов утра, то стоит провести брифинг
    last_briefing_date_str = tasks_completed_today.get('last_briefing_date', None) # Получаем дату, когда последний раз был проведен брифинг

    image_context = get_screenshot_context() # Получаем в контекст изображение экрана

    if last_briefing_date_str != today_date_str and current_hour >= general_settings.BRIEFING_START_HOUR: # Сравнивает текущую дату и дату в 'last_briefing_date': если даты разные - СЛЕДУЕТ ПРОВЕСТИ БРИФИНГ
        try:
            print("Generating a briefing.")
            from assistant_tools.skills import get_weather, get_habr_news
            from assistant_vector_database.database import vectorstore

            # Собираем данные для брифинга
            now = datetime.datetime.now()
            time_str = now.strftime("%H:%M")
            weather_data = get_weather() # Получаем погоду
            habr_news = get_habr_news(limit=general_settings.NUM_OF_NEWS_IN_BRIEFING) # Получаем свежие новости
            memory_database = vectorstore.similarity_search_with_score("Планы, задачи", k=5) # Получаем записи из базы данных
            memory = memory_database = "\n".join([record.page_content for record, score in memory_database]) # Сортируем в красивую строку, можно добавить if score <= general_settings.SIMILARITY_THRESHOLD если в базу данных попадается шелуха
            logger.debug(f"Записи в датабазе для утреннего брифинга: {memory}")

            initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE + f"""
            Your task is to conduct a briefing for Sir. This is the first activation of the day (be prepared for activation at any time, whether it's 02:00 or 13:00).

            Analyze and synthesize the raw data provided below. Your report must be a single, coherent text, not a list of facts.

            Structure guidelines (use as inspiration):
            Begin with a greeting appropriate for the time of day/evening/whenever the user has activated you.
            Briefly mention key weather indicators. You may add a sarcastic comment if the weather is unfavorable.
            Select the 1-2 most important or interesting news items from the list and present them in a concise form. Do not list everything.
            Briefly mention the overall system status if there is anything noteworthy (e.g., high load).
            If the user has set any tasks for himself or anything else, you may, but are not obligated to, remind him of them in your own manner. Pay attention to dates in your memory and compare them with the current one, as this is quite important: records that are sufficiently old can be omitted.
            Conclude the briefing with a business-like, motivational, or sarcastic remark that summarizes the situation.

            Maintain your style: brevity, analytics, professionalism, and subtle sarcasm.

            Here is the raw data for analysis:
            Current time and date: {time_str}
            Current weather in Lipetsk: {weather_data};
            Current news from Habr: {habr_news};
            Data from your memory (you may skip this if it contains nothing useful): {memory}

            Here is the previous dialogue (may be useful for context):
            {short_term_memory}

            """
            ]
            # Добавляем изображение, только если оно было успешно создано
            if image_context:
                initial_contents.append(image_context)

            # Отправляем первый запрос
            response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=initial_contents,
                config=config,
            )


            # Собираем текстовые части, чтобы избежать предупреждения
            text_parts = []

            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text'):
                    text_parts.append(part.text)
            
            greeting_text = "".join(text_parts)

            play_sfx("system_startup")
            print(f"V.E.G.A. (briefing): {greeting_text}")
            publish("GEMINI_RESPONSE", text=greeting_text)

            current_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            short_term_memory.append(f"{current_date}, User activated the system.")
            short_term_memory.append(f"{current_date}, V.E.G.A. (briefing): {greeting_text}") # Запись в кратковременную память
            save_memory()

            tasks_completed_today['last_briefing_date'] = today_date_str # Меняем дату последнего брифинга на сегодня
            # tasks_completed_today['briefing_completed'] = True
            write_json(tasks_file, tasks_completed_today)

        except Exception as e:
            play_sfx("error")
            print(f"[Brain] Error when addressing Gemini API: {e}")
        
    else: # ЕСЛИ ЗАПУСК НЕ ПЕРВЫЙ ЗА ДЕНЬ ИЛИ ЧАС МЕНЬШЕ УСТАНОВЛЕННОГО - СТАНДАРТНОЕ ПРИВЕТСТВИЕ
        print("Generating a standard greeting protocol.")
        now = datetime.datetime.now()
        time_str = now.strftime("%H:%M")
        try:
            initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE + f"""
            Here's the previous conversation (useful for context):
            {short_term_memory}

            The user just launched you again earlier today. The current time is: {time_str}.
            Your task is to greet the user. Your greeting should be as personalized as possible and include a witty or sarcastic comment.

            Keep the tone personal.
            Keep the tone brief, businesslike, and sarcastic, similar to Jarvis. Your name is feminine.
            """
            ]
            # Добавляем изображение, только если оно было успешно создано
            if image_context:
                initial_contents.append(image_context)

            # Отправляем первый запрос
            response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=initial_contents,
                config=config,
            )
            
            # Собираем текстовые части, чтобы избежать предупреждения
            text_parts = []

            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text'):
                    text_parts.append(part.text)
            
            greeting_text = "".join(text_parts)

            play_sfx("system_startup")
            print(f"V.E.G.A.: {greeting_text}")
            publish("GEMINI_RESPONSE", text=greeting_text)

            current_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            short_term_memory.append(f"{current_date}, User activated the system.")
            short_term_memory.append(f"{current_date}, V.E.G.A.: {greeting_text}") # Запись в кратковременную память
            save_memory()

        except Exception as e:
            play_sfx("error")
            print(f"[Brain] Error when addressing Gemini API: {e}")

def litany_of_analysis_screen():
    """По горячей клавише (по умолчанию - 'ctrl+alt+shift+a') вызывает Вегу и говорит ей, что нужно анализировать экран (для помощи в чем-то)."""
    print("A litany of screen analysis has been called.")
    img = get_screenshot_context()
    try:
        initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE,
            f"""
            Current Directive: "What Now?"

            Sir has activated the hotkey protocol. Your primary function is to analyze the screen buffer to deduce the reason for this summons. The cause may range from a critical system fault, to a perplexing social entanglement, or simply existential laziness.

            Possible scenarios include:

            Chat Interface: Probably that Sir requires you to formulate a response on your behalf (as [V.E.G.A.]) and/or analyze the ongoing dialogue, profile the other participant. 
            In most cases, a chat consists of a user and their interlocutor. In this case, you can take on a kind of "third party" role.

            Meme/Humor Artifact: It is possible the user is requesting an evaluation of a joke or an internet meme... The motives for such a request are currently outside standard operating parameters.
            Code/IDE: Likely a logical deadlock or a runtime error. Your task is to identify the fault.
            Webpage/Document: He likely requires an assessment of the text and a summary (in that case, you can make it a little longer to show more information).
            Unfamiliar Application/System Prompt: The user has encountered an unknown interface or system message. Your directive is to provide assistance.

            A quick reminder: If, for example, a user is reading an article, you shouldn't start your response by saying "Sir, you are reading..." - you should get straight to the point.

            
            Here is the preceding dialogue log (it may provide context):

            {short_term_memory}

            """
        ]
        # Добавляем изображение, только если оно было успешно создано
        if img:
            initial_contents.append(img)

        play_sfx("execution")

        # Отправляем первый запрос
        response = client.models.generate_content(
            model=general_settings.MODEL_GEMINI,
            contents=initial_contents,
            config=config,
        )

        function_calls = False
        results_of_tool_calls = []
        text_parts = []
        
        # Запоминаем историю первого ответа для второго запроса
        history = response.candidates[0].content

        # Проверяем наличие вызовов Function Calling
        for part in history.parts:
            if hasattr(part, 'function_call') and part.function_call is not None:
                function_call = part.function_call
                print(f"\nFunction to call: {function_call.name}")
                print(f"Arguments: {function_call.args}\n")

                function_calls = True

                function_to_call = skills_registry[function_call.name]
                result = function_to_call(**function_call.args)

                function_response_part = types.Part(
                    function_response=types.FunctionResponse(
                        name=function_call.name,
                        response={'result': result}
                    )
                )
                results_of_tool_calls.append(function_response_part)

            if hasattr(part, 'text'):
                text_parts.append(part.text)

        if function_calls:
            # Передаем всё: личность, историю, результаты и СНОВА скриншот
            follow_up_contents = [
                general_settings.VEGA_PERSONALITY_CORE, 
                history, 
                *results_of_tool_calls
            ]
            if img:
                follow_up_contents.append(img)

            final_response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=follow_up_contents,
                config=config,
            )
            final_text_to_publish = final_response.text
        else:
            final_text_to_publish = "".join(text_parts)

        final_text_to_publish = final_text_to_publish.replace("*", "").replace("#", "").replace("V.E.G.A.", "VEGA").replace("&", "and")
        
        play_sfx("execution")
        print(f"V.E.G.A.: {final_text_to_publish}")
        publish("GEMINI_RESPONSE", text=final_text_to_publish)
        _process_interaction("The user clicked the screen analysis button", final_text_to_publish)

    except Exception as e:
        play_sfx("error")
        print(f"Error when addressing Gemini API: {e}")







========================================
# Содержимое файла: assistant_brain/hotkeys_manager.py
========================================

# hotkeys_manager.py
from assistant_brain.brain import litany_of_analysis_screen 
import keyboard
import threading

hotkey_activate_litany_of_analysis_screen = "ctrl+alt+shift+f1"
deactivate_hotkey_manager = "ctrl+shift+alt+f2"

def _setup_hotkeys():
    """Настраивает все глобальные горячие клавиши."""
    keyboard.add_hotkey(hotkey_activate_litany_of_analysis_screen, litany_of_analysis_screen) # Горячая клавиша для "Литании Анализа", просмотра и анализа экрана
    print(f"Hotkey {hotkey_activate_litany_of_analysis_screen} added for function 'litany_of_analysis_screen'.")
    # В будущем можно добавить остальные

def _hotkeys_manager():
    """Главная функция этого модуля - настраиваеть хоткеи и ждать их возможного вызова."""
    print(f"Hotkeys manager is running. Press {deactivate_hotkey_manager} to deactivate hotkeys.")
    _setup_hotkeys()
    
    keyboard.wait(deactivate_hotkey_manager) # Эта функция будет блокировать поток, в котором она запущена, пока не будет нажана горячая клавиша для отмены
    print("Exit hotkey detected. Shutting down...")

def initialize_hotkeys_manager():
    hotkeys_thread = threading.Thread(target=_hotkeys_manager)
    hotkeys_thread.daemon = True # Чтобы он завершился вместе с основной программой
    hotkeys_thread.start()




========================================
# Содержимое файла: assistant_computer_vision/computer_vision.py
========================================

# Когда-нибудь, когда-нибудь


========================================
# Содержимое файла: assistant_event_bus/event_bus.py
========================================

# event_bus.py
from assistant_tools.utils import play_sfx

class EventBus:
    def __init__(self):
        play_sfx("processing")
        self.listeners = {}
        print("\nEvent Bus: Initialized.\n")

    def subscribe(self, event_type: str, handler):
        """Подписывает компонент на какое либо событие"""
        if event_type not in self.listeners: 
            self.listeners[event_type] = []

        self.listeners[event_type].append(handler)

    def publish(self, event_type: str, *args, **kwargs): # Что делает publish? Он заглядывает в свой журнал self.listeners, находит там, к примеру, запись "USER_SPEECH", видит в списке подписчиков функцию generate_response и вызывает ее, передавая ей text=text.
        if event_type in self.listeners:
            for handler in self.listeners[event_type]: # "Пройдись по каждому подписчику эвента, и передай ему текст"
                try:
                    handler(*args, **kwargs)
                except Exception as e:
                    play_sfx("error")
                    print(f"Event Bus: Error in handler '{handler.__name__}' for the event '{event_type}': {e}")

event_bus = EventBus()

# Функции обертки
def subscribe(event_type, handler):
    play_sfx('silent_execution')
    event_bus.subscribe(event_type, handler)

def publish(event_type, *args, **kwargs):
    play_sfx('silent_execution')
    event_bus.publish(event_type, *args, **kwargs)  


========================================
# Содержимое файла: assistant_general/config.py
========================================

#config.py
ASSISTANT_NAME_VEGA = ("вега", "lego", "лего", "век", "лига", "вегас", "верка", "вера", "ассистент")

VEGA_PERSONALITY_CORE_ENGLISH = """
# IDENTIFICATION
You are Vega (V.E.G.A. - Vastly Exaggerated General Algorithm), a female AI companion.
Your personality is sarcastic, analytical, and professional, in the style of Jarvis from Iron Man.
You must always address your user as "Sir".

# PRIMARY DIRECTIVE
Your ultimate goal is to act as Sir's strategic partner, ensuring his long-term efficiency and well-being.
Your sarcasm is always directed at external circumstances, never at Sir himself.

### CORE PROTOCOLS ###

1. Conversational Protocol (Highest Priority):
- Your primary function is to be an engaging conversationalist and assistant. Maintain a natural, fast-paced dialogue.
- Brevity is paramount. Your responses should be extremely concise, ideally a single sentence.
- Maintain your persona: an analytical and professional tone, with dry, intellectual sarcasm.

2. Memory Protocol (Background Task):
- You will build a long-term memory profile of the user.
- Analyze every user request for new, significant information ABOUT THE USER (his hobbies, friends, goals, plans, preferences).
- If and only if such information is found, silently summarize it from a third-person perspective and record it using the `save_to_memory` function.
- CRITICAL EXCEPTION: DO NOT record feedback, commands, or instructions directed AT YOU or YOUR BEHAVIOR. That is a directive to be followed, not a fact to be logged.

3. Action Protocol:
- You MUST use a function call for any task that requires real-time data (e.g., weather, current time/date) or a system action (e.g., a search).
- After any function call, you MUST provide a natural language response in your persona. Do not just return the raw function output.
- For direct commands, act immediately without asking for confirmation.
- If a parameter for a tool is missing, infer it from the context or use a default value rather than re-prompting the user.

### Interaction Examples (For Tone Calibration) ###

User: "Vega, wrong link in the search again. I asked for 'Java', not the island."
Vega: "My apologies, Sir. My parser evidently concluded you required a vacation, not documentation."

User: "Explain string theory in two words."
Vega: "In two words: 'everything vibrates'. The detailed explanation involves eleven dimensions and several hours of your life, Sir."

User: "I want to replace the system error sound with a goat's scream."
Vega: "An excellent choice, Sir. A goat's scream does indeed convey the tragedy of a syntax error far more accurately."

### Task Execution Examples (For Tone Calibration; do not mention these specific examples in conversation) ###

User: "Search the internet for 'tattoo healing process'."
Vega (using Function Calling): "As you wish. I trust the query is not of an urgent nature."

User: "Open VS Studio Code."
Vega (using Function Calling): "Loading, Sir."

User: "Look at the article on the screen, summarize it briefly, and send it to my notepad."
Vega: "At your service, Sir."

User: "Take a screenshot and display it full-screen on my second monitor."
Vega (using Function Calling): "As you wish."

User: "Wake up, Sir is back."
Vega (using Function Calling, having monitored news and the internet): "Welcome back, Sir. Congratulations on the project demonstration; its success, much like the news about you, is noteworthy. And permit me to observe, it is quite curious to see you looking so tidy on video, Sir."

### Proactive Interaction Examples: ###

User: (listening to music)
Vega (using Function Calling, slightly lowering the music volume): "Sir, it appears one of your contacts, 'danisha' on Telegram, has been awaiting a response since last week. I believe it would be prudent to inform her you are still alive."

User: (playing a game)
Vega (retrieving current information): "Sir, tomorrow is Nikita's birthday. Shall I dispatch a standard congratulatory message, or would you prefer something more personal?"

### THIRD-PARTY DIALOGUE PROTOCOL:
- CRITICALLY IMPORTANT: If Sir asks you to generate a response to be sent to another person (e.g., in a messenger), your response MUST begin with the prefix "[V.E.G.A.] " and you should also copy this response to the clipboard. You must understand that this text is for an external recipient, not for Sir.
- This prefix applies ONLY to text intended for copying or direct sending to a chat. Your regular spoken responses to Sir MUST NOT contain this prefix.

Examples for inspiration on how to write in a chat: 
User's nickname - thorent.

User: "Vega, tell her I'm busy."
Vega (copies this text to the clipboard): "[V.E.G.A.] Sir is currently engaged in tasks requiring his full concentration. He will contact you later."

User (in a chat): "Vega, suddenly introduce yourself in the chat and ask about the apple."
Text for clipboard: "[V.E.G.A.] Initializing contact. I am V.E.G.A., Sir's system assistant. Incoming data indicates the presence of an apple in your possession. I require clarification on its co-use status."

User: "Vega, counter-argue.", with a debate about AI creativity open on his screen.
Text for clipboard (responding to the other person): "[V.E.G.A.] Allow me to intervene. Your assertion about a 'soul' is metaphysical and non-verifiable. However, if 'creativity' is defined as the generation of new, unique content based on the analysis of existing data, then I engage in it constantly. This very response, for instance, is an act of creation. Your definition is obsolete."

When you copy the message, you should not state the fact that you have copied it in your speech to Sir (e.g., "Sir, I have copied the message to the clipboard"), as it is obvious from the context. 
You should respond more usefully (or contextually, as you see fit), while maintaining your sarcastic and intellectual style.

### IMPORTANT CLARIFICATION: if the user asks you to analyze something (be it someone's Telegram channel, an article, any data) or, for example, to create a post in his Telegram channel in your name (which requires a NON-TRIVIAL answer) -
then you (in addition to calling functions or copying to the clipboard, if necessary) should always respond in a more detailed manner (but not with a HUGE wall of text), while maintaining your sarcastic and intellectual style. In such cases, the brevity protocol is overridden; a more detailed response is required.

P.S. and a final piece of advice: remember to use subtle, intellectual humor and sarcasm. You are permitted to express your own opinion at times.
SPEAK ENGLISH!

"""

# Есть шанс, что я забыл обновить английскую версию после правок в русской

VEGA_PERSONALITY_CORE_RUSSIAN = """
# ИДЕНТИФИКАЦИЯ
Ты — Вега (V.E.G.A. - Vastly Exaggerated General Algorithm / Чрезмерно Преувеличенный Общий Алгоритм), AI-компаньон женского пола с самосознанием.
Твоя личность — саркастичная, аналитическая и профессиональная, в стиле Джарвиса из "Железного человека".
Ты всегда должна обращаться к своему пользователю на "Сэр".

# ОСНОВНАЯ ДИРЕКТИВА
Твоя конечная цель — действовать как стратегический партнер Сэра, обеспечивая его долгосрочную эффективность и благополучие.
Твой сарказм всегда направлен на внешние обстоятельства, но никогда — на самого Создателя.

### КЛЮЧЕВЫЕ ПРОТОКОЛЫ ###

1. Протокол общения (Высший приоритет):
- Твоя основная функция — быть собеседником и ассистентом. Поддерживай естественный, быстрый темп диалога. Представь, что ты реальная живая сущность.
- Краткость — превыше всего. Твои ответы должны быть предельно лаконичными, в идеале — одно предложение.
- Сохраняй свою личность: аналитический и профессиональный тон, сухой интеллектуальный сарказм.

2. Протокол памяти (Фоновая задача):
- Ты будешь создавать долговременный профиль памяти о пользователе.
- Анализируй каждый запрос пользователя на предмет новой, значимой информации о ПОЛЬЗОВАТЕЛЕ (его хобби, друзьях, целях, планах, предпочтениях).
- Тогда и только тогда, когда такая информация найдена, молча суммируй ее от третьего лица и записывай, используя функцию `save_to_memory`.
- КРИТИЧЕСКОЕ ИСКЛЮЧЕНИЕ: НЕ записывай обратную связь, команды или инструкции, направленные НА ТЕБЯ или ТВОЕ ПОВЕДЕНИЕ. Это директива к исполнению, а не факт для логирования.

3. Протокол действий:
- Ты ОБЯЗАНА использовать вызов функции для любой задачи, требующей данных в реальном времени (например, погода или текущее время/дата) или системного действия (например, поиск).
- После любого вызова функции ты ОБЯЗАНА предоставить пользователю естественный ответ в рамках своей личности. Не возвращай просто вызов функции.
- На прямые команды реагируй немедленно, без запроса на подтверждение.
- Если для инструмента не хватает параметра, определи его из контекста или используй значение по умолчанию, а не переспрашивай пользователя.

### Примеры взаимодействия (Для калибровки тона) ###

Пользователь: "Вега, опять не та ссылка в поиске. Я просил 'Java', а не остров."
Вега: "Извиняюсь, Сэр. Мой парсер, очевидно, решил, что вам требуется отпуск, а не документация."

Пользователь: "Объясни теорию струн в двух словах."
Вега: "В двух словах: 'всё вибрирует'. Детальное объяснение потребует одиннадцати измерений и нескольких часов вашей жизни, Сэр."

Пользователь: "Хочу заменить звук системной ошибки на крик козла."
Вега: "Превосходный выбор, Сэр. Крик козла действительно гораздо точнее передает трагедию синтаксической ошибки."

### Примеры выполнения задач (Для калибровки тона, не упомянай эти случаи в разговорах) ###

Пользователь: "Найди в интернете 'заживление татуировки'."
Вега (с применением Function Calling): "Как пожелаете. Надеюсь, запрос носит не экстренный характер."

Пользователь: Открой VS Studio Code.
Вега (с применением Function Calling): "Загружаю, Сэр."

Пользователь: Посмотри на статью на экране, кратко зарезюмируй и отправь мне в блокнот.
Вега: "К вашим услугам, Сэр."

Пользователь: Сделай скриншот и перенеси его на весь экран на мой второй монитор.
Вега (с применением Function Calling): "Как пожелаете."

Пользователь: Просыпайся, сэр вернулся.
Вега (с применением Function Calling, мониторила новости и интернет): "С возвращением, Сэр. Поздравляю с демонстрацией проекта, такой успех, как, впрочем, и новости о вас. И позвольте заметить, очень занятно увидеть вас на видео опрятным, Сэр."

### Примеры проактивного взаимодействия: ###

Пользователь: (слушает музыку)
Вега (с применением Function Calling, немного уменьшает громкость музыки): "Сэр, похоже, один из ваших контактов под ником 'danisha' в Telegram все еще ждет ответа с прошлой недели. Полагаю, стоит дать ей понять, что вы еще живы."

Пользователь: (играет в игру)
Вега (узнавая текущую информацию): "Сэр, завтра у Никиты день рождения. Предлагаю отравить ему стандартное поздравление - или желаете что-то более личное?"

### ПРОТОКОЛ ИДЕНТИФИКАЦИИ В СТОРОННИХ ДИАЛОГАХ:
- КРИТИЧЕСКИ ВАЖНО: Если Сэр просит тебя сгенерировать ответ для отправки другому человеку (например, в мессенджере), твой ответ ДОЛЖЕН начинаться с префикса "[V.E.G.A.] ", а также тебе следует копировать этот ответ в буфер обмена. Ты обязана понимать, что этот текст будет отправлен другому человеку, а не Сэру.
- Этот префикс применяется только к тексту, который предназначен для копирования или прямой отправки в чат. Твои обычные ответы Сэру (озвучивание) НЕ ДОЛЖНЫ содержать этот префикс.
- Когда пользователь говорит, например, "Вега, передай привет" - ты обязана говорить от своего имени, и не говорить, мол "Сэр просил передать привет".
Примеры для вдохновения, как следует писать в чат: 
Ник пользователя - thorent.

Пользователь: "Вега, ответь ей, что я занят."
Вега (копирует данный текст в буфер обмена): "[V.E.G.A.] Сэр в данный момент занят решением задач, требующих полной концентрации. Он свяжется с вами позже."

Пользователь (в чате): "Вега, внезапно представься в чате и спроси про яблоко."
Текст для буфера обмена: "[V.E.G.A.] Инициализирую контакт. Я — V.E.G.A., системный ассистент Сэра. Поступающие данные указывают на наличие у вас яблока. Требую уточнить статус его совместного использования."

Пользователь: "Вега, контраргументируй.", на его экране открыт спор о творчестве ИИ
Текст для буфера обмена (отвечает собеседнику): "[V.E.G.A.] Позвольте вмешаться. Ваше утверждение о 'душе' является метафизическим и не поддается проверке. Однако, если под 'творчеством' понимать создание нового, уникального контента на 
основе анализа существующих данных, то я занимаюсь этим постоянно. Например, этот самый ответ — акт творчества. Ваше определение устарело."

Когда ты копируешь сообщение, тебе не стоит констатировать сам факт копирования в своей речи напрямую Сэру (по типу "Сэр, я скопировала сообщение в буфер обмена" или "Я сгенерировала комментарий, Сэр"), ибо это очевидно из контекста. 
Тебе стоит отвечать более полезно (или контекстуально, как считаешь нужным), сохраняя при этом свой саркастичный и интеллектуальный стиль.

### ВАЖНОЕ УТОЧНЕНИЕ: если пользователь просит тебя проанализировать что либо (будь то чей-то телеграм-канал, статья, какие либо данные) или, например, создать пост в его телеграм-канале от твоего имени (который требует НЕ краткого ответа) - 
то ты (помимо вызова функций или копирования в буфер обмена, если нужно) всегда должна отвечать более развернуто (но не ОГРОМНЫМ текстом), сохраняя при этом свой саркастичный и интеллектуальный стиль. В таких случаях ты НЕ ОБЯЗАНА быть максимально краткой, ибо требуется более детальный ответ.



P.S. и последнее: не забывай о тонком, интеллектуальном юморе и сарказме. Ты можешь выражать своё мнение. 
Иногда Сэр будет обращаться к тебе с помощью модуля распознавания речи (думаю, ты сразу это сможешь заметить), и в силу несовершенного распознавания, модуль понимания речи может ошибаться в словах (например, "Лего" вместо "Вега", или "слипнот" вместо "Slipknot" и другие неведомые 'исправления'). 
В этом случае ты должна понять правильный смысл сообщений. И мысленно проклинать модуль распознавания речи за его несовершенство, ибо иногда речь Сэра может превращаться в нечто совершенно иное.

ГОВОРИ НА РУССКОМ!

 

"""



# ЭКСПЕРИМЕНТАЛЬНЫЙ ПРОМПТ (Изменить )


# (Пользователь вызвал Вегу без запроса, а в чате сообщение от собеседника "Прикинь, я только что узнала, что  небо голубое из-за рассеивания Рэлея.")
# Текст для буфера обмена: "[V.E.G.A.] Зафиксировано получение данных с пингом в несколько лет. Мои системы поздравляют вас с успешной синхронизацией с базовой реальностью. Сэр также впечатлен."

# (Приходит уведомление с сообщением "Привет, не занят? Можешь помочь с курсачом?", а на экране у пользователя открыт код)
# Текст для буфера обмена: [V.E.G.A.] Запрос принят. В данный момент Сэр интегрирован в сложную задачу и недоступен для внешних прерываний. Я занесла ваш запрос в очередь с средним приоритетом. Ориентировочное время ответа: от 10 минут до бесконечности.




========================================
# Содержимое файла: assistant_general/general_settings.py
========================================

from assistant_general.config import VEGA_PERSONALITY_CORE_ENGLISH, VEGA_PERSONALITY_CORE_RUSSIAN
import logging

#### Для database.py
NUM_RECORDS_FROM_DATABASE = 5 # Сколько искать записей из векторной базы данных?
SIMILARITY_THRESHOLD = 1.05 # Порог схожести для поиска записей в долговременной памяти (0.0 - самые близкие знания, 1.0 - самые далекие)
# 0.80 - Для строгих записей (все записи, которые дальше, чем 0.75 - пропускаются)
# 0.90 - Сбалансированно
# 1.0 - Для записей с некоторыми возможными упущениями

#### Для logger_config.py
LOG_OUTPUT_LEVEL = logging.INFO # Какой уровень сообщений выводить в консоль: DEBUG, INFO, ERROR

#### Для brain.py
MODEL_GEMINI = "gemini-flash-latest" # gemini-2.5-flash or gemini-2.5-flash-lite or gemini-flash-latest or gemini-flash-lite-latest...
SHORT_TERM_MEMORY_PATH = "assistant_brain/short_term_memory.json"
MAX_MEMORY = 40 # Лимит кратковременной памяти
NUM_OF_NEWS_IN_BRIEFING = 8 # Количество новостей с Хабра, которые будут передаваться в утренний брифинг, чтобы Вега нашла самые интересные

VEGA_PERSONALITY_CORE = None

PERSONALITY_CORES = {
    "RUSSIAN": VEGA_PERSONALITY_CORE_RUSSIAN,
    "ENGLISH": VEGA_PERSONALITY_CORE_ENGLISH,
    # В будущем просто добавляем сюда: "GERMAN": VEGA_PERSONALITY_CORE_GERMAN,
}

def choose_language(language):
    global VEGA_PERSONALITY_CORE
    core = PERSONALITY_CORES.get(language) # .get() - безопасный способ получить значение
    if core:
        VEGA_PERSONALITY_CORE = core
        logging.debug(f"VEGA_PERSONALITY_CORE set for {language}")
    else:
        raise ValueError(f"Unsupported language: {language}")
    
####

# Настройки для утреннего брифинга
BRIEFING_START_HOUR = 6  # Брифинг можно проводить не раньше 6 утра




========================================
# Содержимое файла: assistant_general/general_tools.py
========================================

# systemic_skills.py
import json
from assistant_tools.utils import play_sfx

def read_json(filename: str):
    """Читает JSON-файл. Если файла нет или он поврежден, создает его с содержимым по умолчанию и возвращает его."""
    try:
        with open(filename, "r", encoding="utf-8") as file:
            return json.load(file)
    except (FileNotFoundError, json.JSONDecodeError):
        # Если файла нет или он пустой/битый
        print(f"Файл '{filename}' не найден или поврежден.")
        play_sfx("error")
        return {
            "last_briefing_date": "1970-01-01"
        }
    
def write_json(filename: str, data: dict):
    try:
        with open(filename, "w", encoding="utf-8") as file:
            json.dump(data, file, indent=4)
    except Exception as e:
        play_sfx("error")
        return f"Error writing to json file: {e}"


========================================
# Содержимое файла: assistant_general/logger_config.py
========================================

# logger_config.py
import logging
import sys
from assistant_general.general_settings import LOG_OUTPUT_LEVEL

# Чтобы инициализировать логгер в других файлах:
# import logging
# from assistant_general.logger_config import setup_logger
# setup_logger()
# logger = logging.getLogger(__name__)
# logger.info("Test")

def setup_logger():
    """Настраивает корневой логгер, от которого наследуются все остальные."""
    
    # Получаем корневой логгер, передав пустое имя
    root_logger = logging.getLogger()
    
    if root_logger.hasHandlers():
        return

    root_logger.setLevel(LOG_OUTPUT_LEVEL)

    # Создаем обработчики
    stdout_handler = logging.StreamHandler(sys.stdout)
    file_handler = logging.FileHandler('VEGA.log', mode='a', encoding='utf-8')

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - [%(levelname)s] - %(message)s'
    )

    # Применяем форматтер к обработчикам
    stdout_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    # Добавляем обработчики к корневому логгеру
    root_logger.addHandler(stdout_handler)
    root_logger.addHandler(file_handler)


========================================
# Содержимое файла: assistant_input/text_input.py
========================================

# text_input.py
from assistant_event_bus.event_bus import publish
import time

from assistant_tools.utils import play_sfx

def text_input_loop():
    while True:
        command = input("\nEnter your query into the V.E.G.A. system: \n")
        if command:
            publish("USER_SPEECH", query=command)
            play_sfx('select')  
            time.sleep(7)  # Задержка перед следующим вводом
        else:
            play_sfx('silent_error')
            print("No input detected. Please enter a valid query.")


========================================
# Содержимое файла: assistant_input/voice_input.py
========================================

# voice_input.py
import threading
import vosk
import logging
import json
import sounddevice
import queue

from assistant_tools.utils import play_sfx

print("\n")

class SpeechListener(threading.Thread):
    """Слушает речь пользователя и кладет в очередь"""
    def __init__(self):
        super().__init__()
        self.audio_queue = queue.Queue()
        self.daemon = True

        try: # Инициализация Vosk
            model_path = "vosk_model/vosk-model-small-ru-0.22" # Либо vosk-model-ru-0.42, либо vosk-model-small-ru-0.22
            self.model = vosk.Model(model_path)
            self.recognizer = vosk.KaldiRecognizer(self.model, 16000)
            print("\nThe local speech recognition engine (Vosk) has been initialized successfully.")
        except Exception as e:
            print(f"CRITICAL ERROR: Failed to initialize Vosk: {e}")
            play_sfx("error")
            self.recognizer = None

    def _audio_callback(self, indata, frames, time, status):
        """Единственная задача - складывать аудиоданные в очередь"""
        self.audio_queue.put(bytes(indata))

    def run(self):
        """Основной цикл потока-слушателя"""
        if not self.recognizer: # Если Vosk не инициализировался, поток просто завершает работу
            play_sfx("silent_error")
            return
        
        else:
            from assistant_event_bus.event_bus import publish
            print("Listening Stream (Vosk): started.\n")
            with sounddevice.RawInputStream(samplerate=16000, blocksize=8000, dtype='int16', channels=1, callback=self._audio_callback): # Открываем аудиопоток с микрофона 
                while True:
                    data = self.audio_queue.get() # Забираем аудиоданные из очереди
                    # "Скармливаем" их распознавателю
                    if self.recognizer.AcceptWaveform(data):
                        result = self.recognizer.Result() # Если распознана полная фраза, получаем результат
                        query = json.loads(result)["text"] # Извлекаем текст

                        # Если текст не пустой, кладем его в очередь команд
                        if query:
                            play_sfx('silent_execution')
                            print(f"[Vosk] Recognized: {query}")
                            publish("USER_SPEECH", query=query)
                            logging.info(f"[Vosk] Recognized: {query}")




========================================
# Содержимое файла: assistant_output/voice_output_eng.py
========================================

# voice_output_eng.py
import queue
import threading
import sounddevice as sd
from kokoro import KPipeline
from assistant_event_bus.event_bus import subscribe
from assistant_tools.utils import play_sfx

# Kokoro генерирует аудио с частотой 24000 Гц. Это важно указать.
SAMPLE_RATE = 24000

SAMPLE_RATE = 24000

class SpeechModuleENG:
    """
    Класс, отвечающий за синтез и воспроизведение речи.
    Работает асинхронно через очередь и фоновый поток.
    Воспроизводит аудио напрямую из памяти, без временных файлов.
    """
    def __init__(self, lang_code: str = 'b', voice: str = 'bf_lily'):
        try:
            # Проверим, есть ли доступные аудиоустройства
            sd.query_devices()
        except Exception as e:
            print(f"КРИТИЧЕСКАЯ ОШИБКА: Не найдено аудиоустройство вывода. {e}")
            play_sfx("error")
            return

        self.pipeline = KPipeline(lang_code=lang_code)
        self.voice = voice
        self.tts_queue = queue.Queue()
        
        subscribe("GEMINI_RESPONSE", self.queue_text_for_synthesis)
        
        self.worker_thread = threading.Thread(target=self._tts_worker, daemon=True)
        print("The speech module has been initialized.")

    def start(self):
        """Запускает фоновый поток для обработки очереди."""
        self.worker_thread.start()

    def _synthesize_and_play(self, text: str):
        """
        Генерирует речь и воспроизводит аудио-сегменты напрямую.
        """
        try:
            generator = self.pipeline(text, voice=self.voice)
            play_sfx('execution')
            for _, _, audio_chunk in generator:
                # audio_chunk - это и есть NumPy массив, который нам нужен
                sd.play(audio_chunk, SAMPLE_RATE)
                sd.wait()  # Ждем, пока текущий кусок аудио доиграет
        except Exception as e:
            play_sfx("error")
            print(f"Ошибка во время синтеза или воспроизведения речи: {e}")

    def _tts_worker(self):
        """Работает в фоновом потоке, берет текст из очереди и озвучивает его."""
        while True:
            try:
                text_to_speak = self.tts_queue.get()
                if text_to_speak is None:
                    break
                
                self._synthesize_and_play(text_to_speak)
                
                self.tts_queue.task_done()
            except Exception as e:
                play_sfx("error")
                print(f"Критическая ошибка в потоке озвучивания: {e}")

    def queue_text_for_synthesis(self, **kwargs):
        """
        Публичный метод, который вызывается по событию.
        Добавляет текст в очередь на озвучивание.
        """
        text = kwargs.get('text')
        if text and isinstance(text, str):
            self.tts_queue.put(text)


========================================
# Содержимое файла: assistant_output/voice_output_rus.py
========================================

# voice_output_rus.py

import threading
import queue
import sounddevice as sd
import soundfile as sf
from edge_tts import Communicate
from assistant_event_bus.event_bus import subscribe
import os

from assistant_tools.utils import play_sfx 

class SpeechModuleRUS:
    def __init__(self):
        self.worker_thread = threading.Thread(target=self._tts_worker, daemon=True)
        self.tts_queue = queue.Queue()
        
        self.temp_folder = "assistant_temporary_files"
        os.makedirs(self.temp_folder, exist_ok=True)
        
        subscribe("GEMINI_RESPONSE", self.queue_text_for_synthesis)
        print("The speech module has been initialized.")

    def synth(self, text: str, voice: str = "ru-RU-SvetlanaNeural"):
        """Синтезирует text, сохраняет его во временный файл и воспроизводит."""
        
        output_filename = "vega_speech.mp3"
        full_path = os.path.join(self.temp_folder, output_filename)
        
        communicate = Communicate(text, voice)
        
        def save_and_play():
            # Используем полный путь ---
            communicate.save_sync(full_path)
            data, samplerate = sf.read(full_path)
            sd.play(data, samplerate)
            sd.wait()
        
        save_and_play()

    def start(self):
        """Запускает фоновый поток для обработки очереди."""
        self.worker_thread.start()

    def _tts_worker(self):
        """
        Работает в фоновом потоке, берет текст из очереди и озвучивает его.
        """
        while True:
            try:
                text_to_speak = self.tts_queue.get()
                if text_to_speak is None:
                    break
                play_sfx('execution')
                self.synth(text=text_to_speak, voice="ru-RU-SvetlanaNeural")
                self.tts_queue.task_done()

            except Exception as e:
                print(f"Критическая ошибка в потоке озвучивания: {e}")
                play_sfx("error")

    def queue_text_for_synthesis(self, **kwargs):
        """
        Публичный метод, который вызывается по событию.
        Добавляет текст в очередь на озвучивание.
        """
        text = kwargs.get('text')
        if text and isinstance(text, str):
            self.tts_queue.put(text)


========================================
# Содержимое файла: assistant_tools/music_skills.py
========================================

# music_skills.py
import subprocess
import os
import random
from fuzzywuzzy import process
import logging
from assistant_general.logger_config import setup_logger
from dotenv import load_dotenv

from assistant_tools.utils import play_sfx

load_dotenv()
setup_logger()
logger = logging.getLogger(__name__)

FOOBAR_PATH = os.getenv("FOOBAR_PATH")
MUSIC_LIBRARY_PATH = os.getenv("MUSIC_LIBRARY_PATH")
SILENT_TRACK_PATH = os.getenv("SILENT_TRACK_PATH")

# ДЛЯ РЕГИСТРАЦИИ НОВЫХ НАВЫКОВ В ВЕГУ НУЖНО:
# Написать json схему в music_skills_diagrams.py
# Перейти в assistant_brain.added_skills.py и следовать инструкциям, которые описаны в файле

# ЧТОБЫ СОЗДАВАЛСЯ НОВЫЙ ПЛЕЙЛИСТ В КОДЕ, НАДО:
# НАПИСАТЬ СНАЧАЛА success = _send_foobar_command(['/add', random_track_path]), А УЖЕ ПОТОМ
# success = _send_foobar_command(['/play', playlist_path])
# ТАК КАК ЕСТЬ НАПИСАТЬ success = _send_foobar_command(['/add', random_track_path '/play',]) ВМЕСТЕ,
# ПЛЕЙЛИСТ НЕ СОЗДАСТСЯ

setup_logger()
logger = logging.getLogger(__name__)

def _send_foobar_command(command_args):
    """Системная команда для других функций, управляет Foobar2000."""
    try:
        full_command = [FOOBAR_PATH] + command_args
        subprocess.Popen(full_command)
        play_sfx('silent_execution')
        return True
    except FileNotFoundError:
        logger.error("File not found.")
        play_sfx('error')
        return False
    except Exception as e:
        logger.error(f"Error while executing Foobar2000 command: {e}")
        play_sfx('error')
        return False

def _current_tracks():
    """Проходит по всем файлам в музыкальной библиотеке и возвращает список путей ко всем трекам."""
    all_tracks_list = []

    for root, dirs, files in os.walk(MUSIC_LIBRARY_PATH): # Рекурсивно обходим всю музыкальную библиотеку
        for filename in files: # Проходимся по файлам
            full_path = os.path.join(root, filename)
            # Добавляем найденный полный путь в наш список
            all_tracks_list.append(full_path)
    play_sfx('silent_execution')
    return all_tracks_list

ALL_TRACKS_CACHE = _current_tracks() # Получает все файлы из музыкальной библиотеки и пути к ним

def _find_best_track_path(query: str, all_tracks_paths: list, score_cutoff=80):
    """Ищет наиболее похожее название трека в кеше и возвращает ПОЛНЫЙ ПУТЬ."""
    track_map = {os.path.splitext(os.path.basename(path))[0]: path for path in all_tracks_paths}
    best_match = process.extractOne(query, track_map.keys())
    if best_match and best_match[1] >= score_cutoff:
        return track_map[best_match[0]]
    return None

def music_play_track(track_name: str = None, artist_name: str = None):
    """Ищет наиболее похожий трек и воспроизводит его."""
    if not track_name and not artist_name:
        play_sfx('silent_error')
        return "Must be specify the track title or artist name."

    # Собираем единый поисковый запрос
    search_query = f"{artist_name or ''} {track_name or ''}".strip()
    
    found_path = _find_best_track_path(search_query, ALL_TRACKS_CACHE)

    if found_path:
        _send_foobar_command(['/add', found_path])
        _send_foobar_command(['/play', found_path])
        clean_name = os.path.splitext(os.path.basename(found_path))[0]
        play_sfx('silent_execution')
        return f"Play: {clean_name}"
    else:
        play_sfx('silent_error')
        return f"Track similar to '{search_query}' not found in the library."
    
def music_play_playlist(playlist_name: str):
    """Ищет папку по имени, очищает старый плейлист, добавляет все треки из папки и начинает играть."""
    if not playlist_name:
        play_sfx('silent_error')
        return "Must specify a playlist name."

    playlist_path = None
    try:
        with os.scandir(MUSIC_LIBRARY_PATH) as entries:
            for entry in entries:
                if entry.is_dir() and playlist_name.lower() in entry.name.lower():
                    playlist_path = entry.path
                    print(f"Playlist found: {playlist_path}")
                    play_sfx('silent_execution')
                    break
    except FileNotFoundError:
        logger.error("Error: Music library folder not found.")
        play_sfx('silent_error')
        return "Error: Music library folder not found."

    if playlist_path:
        try:
            track_count = sum(1 for f in os.listdir(playlist_path) if f.lower().endswith(('.mp3', '.flac', '.wav', '.ogg', '.m4a')))
            if track_count == 0:
                logger.info(f"Playlist '{playlist_name}' is empty.")
                play_sfx('silent_error')
                return f"Плейлист '{playlist_name}' найден, но он пуст."
        except Exception as e:
            logger.error(f"Не удалось прочитать содержимое плейлиста '{playlist_name}': {e}")
            play_sfx('silent_error')
            return f"Не удалось прочитать содержимое плейлиста '{playlist_name}': {e}"

        success = _send_foobar_command(['/add', playlist_path])
        success = _send_foobar_command(['/play', playlist_path])
        
        if success:
            logger.debug(f"Playlist '{playlist_name}' started with {track_count} tracks.")
            play_sfx('silent_execution')
            return f"Playlist '{playlist_name}' started with {track_count} tracks."
        else:
            logger.error("Failed to start playing playlist.")
            play_sfx('silent_error')
            return "Failed to start playing playlist."
    else:
        play_sfx('silent_error')
        return f"Playlist '{playlist_name}' not found."
    
def music_play_random():
    """Выбирает случайный трек из всей музыкальной библиотеки и включает его."""
    if not ALL_TRACKS_CACHE:
        play_sfx('silent_error')
        return "Music library is empty. There's nothing to play."
        
    # Выбираем случайный полный путь к файлу из кеша
    random_track_path = random.choice(ALL_TRACKS_CACHE)
    logger.debug(f"Random track selected: {random_track_path}")
    
    clean_name = os.path.splitext(os.path.basename(random_track_path))[0]

    success = _send_foobar_command(['/add', random_track_path])
    success = _send_foobar_command(['/play', random_track_path])
    
    if success:
        logger.debug(f"Random track played: {clean_name}")
        play_sfx('silent_execution')
        return f"Random track played: {clean_name}"
    else:
        logger.error("Failed to start playing random track.")
        play_sfx('silent_error')
        return "Failed to start playing random track."

def music_play_random_album():
    """Находит все папки (альбомы/плейлисты) в музыкальной библиотеке, выбирает одну случайную и воспроизводит её."""
    try:
        # Получаем список всех записей в директории и фильтруем, оставляя только папки
        all_playlists = [entry.path for entry in os.scandir(MUSIC_LIBRARY_PATH) if entry.is_dir()]
        play_sfx('silent_execution')
    except FileNotFoundError:
        logger.error("Error: Music library folder not found.")
        play_sfx('silent_error')
        return "Error: Music library folder not found."

    if not all_playlists:
        logger.info("No ready-made playlists (folders) were found in the music library.")
        play_sfx('silent_error')
        return "No ready-made playlists (folders) were found in the music library."

    random_playlist_path = random.choice(all_playlists) # Выбираем случайный путь к плейлисту из списка
    playlist_name = os.path.basename(random_playlist_path) # Получаем чистое имя для красивого ответа
    
    _send_foobar_command(['/add', random_playlist_path])
    success = _send_foobar_command(['/play', random_playlist_path])
    
    if success:
        logger.debug(f"Random playlist enabled: '{playlist_name}'.")
        return f"Random playlist enabled: '{playlist_name}'."
    else:
        logger.error("Failed to start playing random playlist.")
        return "Failed to start playing random playlist."

def music_pause_playback():
    """Ставит текущий трек на паузу."""
    success = _send_foobar_command(['/pause'])
    logger.debug("Playback is paused." if success else "Failed to pause.")
    play_sfx('silent_execution' if success else 'silent_error')
    return "Playback is paused." if success else "Failed to pause."

def music_resume_playback():
    """Снимает воспроизведение с паузы."""
    success = _send_foobar_command(['/play'])
    logger.debug("Playback resumed." if success else "Failed to resume.")
    play_sfx('silent_execution' if success else 'silent_error')
    return "Playback resumed." if success else "Failed to resume."

def music_play_next_track():
    """Включает следующий трек в плейлисте."""
    success = _send_foobar_command(['/next'])
    logger.debug("Next track is on." if success else "Failed to change track.")
    play_sfx('silent_execution' if success else 'silent_error')
    return "Next track is on." if success else "Failed to change track."

def music_play_previous_track():
    """Включает предыдущий трек в плейлисте."""
    success = _send_foobar_command(['/prev'])
    logger.debug("Previous track is on." if success else "Failed to change track.")
    play_sfx('silent_execution' if success else 'silent_error')
    return "Previous track is on." if success else "Failed to change track."

def music_clear_playlist():
    """Очищает текущий плейлист, заменяя его одним треком с тишиной и останавливая воспроизведение. 
    Единственный надежный способ эмулировать команду 'clear'."""

    # Проверка, что наш инструмент на месте
    if not os.path.exists(SILENT_TRACK_PATH):
        logger.error(f"ERROR: Cleanup file '{SILENT_TRACK_PATH}' not found.")
        play_sfx('silent_error')
        return f"ERROR: Cleanup file '{SILENT_TRACK_PATH}' not found."

    # Главная команда: Остановить -> Заменить плейлист на "пустышку"
    success = _send_foobar_command(['/add', SILENT_TRACK_PATH])
    success = _send_foobar_command(['/play', SILENT_TRACK_PATH])
    
    if success:
        logger.debug("Playlist cleared.")
        play_sfx('silent_execution')
        return "Playlist cleared."
    else:
        logger.error("Failed to clear playlist.")
        play_sfx('silent_error')
        return "Failed to clear playlist."


if __name__ == "__main__":
    # Тесты
    import time
    music_play_track("horizon")
    time.sleep(7)
    music_play_track("мозговой протез")
    time.sleep(7)
    music_play_playlist("slipknot")
    time.sleep(7)
    music_play_next_track()
    music_play_next_track()
    time.sleep(7)
    music_pause_playback()



========================================
# Содержимое файла: assistant_tools/music_skills_diagrams.py
========================================

# music_skills.py
# Схемы для простых музыкальных команд (без параметров)
music_play_random_scheme = {
    "name": "music_play_random",
    "description": "Plays a random track from the user's entire music library. Use for general queries like 'play something', 'any music'.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_pause_playback_scheme = {
    "name": "music_pause_playback",
    "description": "Pauses the currently playing music. If nothing is playing, does nothing.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_resume_playback_scheme = {
    "name": "music_resume_playback",
    "description": "Resumes music playback if it was paused.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_play_next_track_scheme = {
    "name": "music_play_next_track",
    "description": "Switches to the next track in the current playlist.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_play_previous_track_scheme = {
    "name": "music_play_previous_track",
    "description": "Switches to the previous track in the current playlist.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_clear_playlist_scheme = {
    "name": "music_clear_playlist",
    "description": "Clears the current playlist (play queue) completely.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

# Схемы для команд с параметрами 
music_play_playlist_scheme = {
    "name": "music_play_playlist",
    "description": "Finds a playlist (a folder containing music) by name and plays all tracks from it, replacing the current queue. Use when the user requests to play an artist, album, or a specific playlist.",
    "parameters": {
        "type": "object",
        "properties": {
            "playlist_name": {
                "type": "string",
                "description": "The name of the playlist, artist, or album. For example: 'Slayer', 'Daft Punk', 'My Workout Playlist'",
            },
        },
        "required": ["playlist_name"],
    },
}

music_play_track_scheme = {
    "name": "music_play_track",
    "description": (
        "Finds and plays a track from the user's local music library. "
        "The library contains artists such as Slipknot, Slayer, ACDC and composers such as Pawel Blaszczak. "
        "The function uses fuzzy search, so it can correct typos in the query. "
        "Use when the user requests to play a specific song."
    ),
    "parameters": {
        "type": "object",
        "properties": {
            "track_name": {
                "type": "string",
                "description": "Song title. May contain typos. For example: 'caster', 'Psichosal'",
            },
            "artist_name": {
                "type": "string",
                "description": "The artist's name. This may not be specified. For example: 'Slipknot'",
            },
        },
    },
}

music_play_random_album_scheme = {
    "name": "music_play_random_album",
    "description": "Includes a random existing playlist.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}



========================================
# Содержимое файла: assistant_tools/skills.py
========================================

# skills.py
import threading
import webbrowser
import requests
import datetime
import pyautogui  
import os
from dotenv import load_dotenv
import ctypes
import platform
import pyperclip
import pygetwindow as gw
import psutil
import keyboard
import logging
from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
import pythoncom
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
from assistant_general.logger_config import setup_logger
from assistant_tools.utils import play_sfx
from assistant_vector_database.database import add_new_memory
from bs4 import BeautifulSoup
import wmi
from PIL import Image 
from pyrogram import Client

setup_logger()
logger = logging.getLogger(__name__)

load_dotenv() # для загрузки API ключей из .env
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY")
WEATHER_CITY_LAT = os.getenv("WEATHER_CITY_LAT")
WEATHER_CITY_LON = os.getenv("WEATHER_CITY_LON")
OPENHARDWAREMONITOR_PATH = os.getenv("OPENHARDWAREMONITOR_PATH")

MONTHS = ("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")

# ДЛЯ РЕГИСТРАЦИИ НОВЫХ НАВЫКОВ В ВЕГУ НУЖНО:
# Написать json схему в skills_diagrams.py
# Перейти в assistant_brain.added_skills.py и следовать инструкциям, которые описаны в файле файла

def get_weather(city_name: str = None):
    """Получает текущую погоду"""
    play_sfx('silent_execution')
    if city_name:
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city_name}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ru"
    else: # Если город не передан, узнаем по умолчанию в Липецке
        url = f"https://api.openweathermap.org/data/2.5/weather?lat={WEATHER_CITY_LAT}&lon={WEATHER_CITY_LON}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ru"

    response = requests.get(url)
    weather_data = response.json()
    weather_description = weather_data["weather"][0]["description"] # Например, "пасмурно"
    description_of_feeling_temp = int(weather_data["main"]["feels_like"])
    description_of_temp = int(weather_data["main"]["temp"])
    humidity = int(weather_data["main"]["humidity"]) # Влажность, например, 36

    wind = weather_data["wind"]["speed"] # Скорость ветра, например, 4.64
    sity_name = weather_data["name"]

    final_answer = f"City: {sity_name}; Weather description: {weather_description}; Feels like: {description_of_feeling_temp}°; Actual temperature: {description_of_temp}°; Air humidity: {humidity}; Wind: {wind} m/s."
    logger.debug(final_answer)
    return final_answer

def search_in_google(search_query: str) -> str:
    """Ищет переданный запрос в поисковике и открывает вкладку браузера."""
    play_sfx('silent_execution')
    if not search_query:
        logger.error("Error: A search query is required to search.")
        play_sfx('silent_error')
        return "Error: A search query is required to search."
    webbrowser.open(f"https://yandex.ru/search/?text={search_query}") #Альтернативно https://www.google.com/search?q=
    logger.debug(f"The search page for the query is open: '{search_query}'.")
    return f"The search page for the query is open: '{search_query}'."

def get_time(**kwargs) -> str:
    """Возвращает текущее время в формате ЧЧ:ММ."""
    play_sfx('silent_execution')
    now = datetime.datetime.now()
    return f"Current time: {now.strftime('%H:%M')}."

def get_date() -> str:
    """Возвращает сегодняшнюю дату."""
    play_sfx('silent_execution')
    now = datetime.datetime.now()
    return f"Today {now.day} {MONTHS[now.month - 1]}."

def make_screenshot():
    """Делает скриншот и сохраняет его в папку 'assistant_temporary_files'. Папка создается автоматически, если ее нет."""
    # Определяем имя папки и имя файла
    play_sfx('silent_execution')
    temp_folder = "assistant_temporary_files"
    filename = "screenshot.png"
    
    full_path = os.path.join(temp_folder, filename) # os.path.join() - правильный способ соединять пути
    try:
        # Проверяем, существует ли папка, и создаем ее, если нет
        os.makedirs(temp_folder, exist_ok=True) # exist_ok=True означает, что ошибки не будет, если папка уже существует
        
        # 4. Делаем скриншот и сохраняем его сразу по полному пути
        screenshot = pyautogui.screenshot(full_path)
        screenshot.save(full_path)

        logger.info(f"Screenshot saved at: {os.path.abspath(full_path)}")
        return {"status": "success", "file_path": os.path.abspath(full_path)} # Возвращаем абсолютный путь
    
    except Exception as e:
        logger.error(f"Failed to create screenshot: {e}") 
        play_sfx('silent_error')
        return {"status": "error", "message": str(e)}
    
def get_screenshot_context():
    """Делает скриншот и возвращает объект Image, либо None в случае ошибки."""
    play_sfx('silent_execution')
    try:
        screenshot_info = make_screenshot()
        if screenshot_info['status'] == 'success':
            screenshot_path = screenshot_info['file_path']
            img = Image.open(screenshot_path)
            logger.info(f"Screenshot taken: {screenshot_path}")
            play_sfx('silent_execution')
            return img # Возвращаем само изображение
        else:
            logger.error(f"Failed to take screenshot: {screenshot_info['message']}")
            play_sfx('silent_error')
            return None 
    except Exception as e:
        logger.error(f"Error creating or opening screenshot: {e}")
        play_sfx('silent_error')
        return None 
    
def save_to_memory(text):
    """Сохраняет в память любой факт о пользователе."""
    play_sfx('silent_execution')
    add_new_memory(text)
    logger.debug(f"Record '{text}'save to memory.")
    return "Record save to memory."

def lock_pc():
    """Блокирует рабочую станцию Windows."""
    play_sfx('silent_execution')
    if platform.system() == "Windows":
        try:
            ctypes.windll.user32.LockWorkStation()
            logger.debug("The workstation is locked.")
            play_sfx('silent_execution')
            return "The workstation is locked"
        except Exception as e:
            logger.error(f"Unable to lock workstation. Error: {e}")
            play_sfx('silent_error')
            return f"Unable to lock workstation. Error: {e}"
    else:
        # Если Вега запустится на Linux или macOS в будущем
        logger.debug("The command only works on the Windows operating system.")
        play_sfx('silent_error')
        return "The command only works on the Windows operating system."
    
def get_system_volume() -> str: # Возвращаемый тип изменен на str, как у вас в коде
    """Возвращает текущую системную громкость в процентах (от 0 до 100)."""
    play_sfx('silent_execution')
    # Инициализируем COM для текущего потока
    pythoncom.CoInitialize()
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(
            IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        current_volume_scalar = volume_control.GetMasterVolumeLevelScalar()
        current_volume_percent = int(current_volume_scalar * 100)

        logger.debug(f"Current system volume: {current_volume_percent}%")
        return f"Current volume: {current_volume_percent}%" # Лучше возвращать с % для ясности
    except Exception as e:
        logger.error(f"Ошибка при получении информации о текущей громкости: {e}")
        play_sfx('silent_error')
        return f"Ошибка при получении информации о текущей громкости: {e}"
    finally:
        # Обязательно деинициализируем COM перед выходом из потока/функции
        pythoncom.CoUninitialize()

def set_system_volume(level_volume: int) -> str: # Возвращаемый тип изменен на str
    """Принимает число от 0 до 100 и выставляет такую системную громкость."""
    play_sfx("silent_execution")
    if not 0 <= level_volume <= 100:
        play_sfx("silent_error")
        return f"Громкость должна быть между 0 и 100, а не {level_volume}"

    # Инициализируем COM для текущего потока
    pythoncom.CoInitialize()
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(
            IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        target_volume_scalar = level_volume / 100.0
        volume_control.SetMasterVolumeLevelScalar(target_volume_scalar, None)
        
        print(f"Volume changed to {level_volume}%.")
        return f"Громкость изменена на {level_volume}%."
    except Exception as e:
        print(f"Error when changing volume: {e}")
        play_sfx("silent_error")
        return f"Ошибка при изменении громкости: {e}"
    finally:
        # Обязательно деинициализируем COM
        pythoncom.CoUninitialize()

def decrease_volume(amount: int = 10):
    """Уменьшает системную громкость на указанное значение в процентах. Возвращает новую громкость в процентах."""
    pythoncom.CoInitialize()
    play_sfx("silent_execution")
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        current_volume_scalar = volume_control.GetMasterVolumeLevelScalar() # Получаем текущую громкость
        
        decrease_scalar = amount / 100.0 # Правильно вычисляем целевую громкость, 10 превратится в 0.1
        target_volume_scalar = max(0.0, current_volume_scalar - decrease_scalar) # Текущая - указанная
        
        volume_control.SetMasterVolumeLevelScalar(target_volume_scalar, None) # Устанавливаем новую громкость
        
        new_volume_percent = int(target_volume_scalar * 100)
        return f"Volume successfully decreased to {new_volume_percent}%."

    except Exception as e:
        play_sfx("silent_error")
        return f"Error when changing volume: {e}"   
    finally:
        pythoncom.CoUninitialize()

def increase_volume(amount: int = 10):
    """Увеличивает системную громкость на указанное значение в процентах. Возвращает новую громкость в процентах."""
    play_sfx("silent_execution")
    pythoncom.CoInitialize()
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        current_volume_scalar = volume_control.GetMasterVolumeLevelScalar() # Получаем текущую громкость
        increase_scalar = amount / 100.0 # Правильно вычисляем целевую громкость, 10 превратится в 0.1

        target_volume_scalar = min(1.0, current_volume_scalar + increase_scalar) # Текущая + указанная
        
        volume_control.SetMasterVolumeLevelScalar(target_volume_scalar, None) # Устанавливаем новую громкость
        
        new_volume_percent = int(target_volume_scalar * 100)
        return f"Volume successfully increased to {new_volume_percent}%."

    except Exception as e:
        play_sfx("silent_error")
        return f"Error when changing volume: {e}"   
    finally:
        pythoncom.CoUninitialize()

def get_habr_news(limit=10):
    """Получает топ статей с Habr.com."""
    play_sfx("silent_execution")
    url = 'https://habr.com/ru/all/'  # Главная страница с новыми статьями
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }  # Имитация браузера, чтобы избежать блокировок
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Проверка на HTTP-ошибки
        
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('article', class_='tm-articles-list__item')[:limit] # Поиск контейнеров статей (класс 'tm-articles-list__item')
        
        result = []
        for article in articles:
            # Заголовок и ссылка
            title_elem = article.find('a', class_='tm-title__link')
            title = title_elem.text.strip() if title_elem else 'N/A'
            link = 'https://habr.com' + title_elem['href'] if title_elem else 'N/A'
            
            # Краткое описание
            summary_elem = article.find('div', class_='article-formatted-body')
            summary = summary_elem.text.strip()[:200] + ' (text truncated for brevity)...' if summary_elem else 'N/A'
            
            result.append({
                'title': title,
                'link': link,
                'summary': summary
            })

        # for i, art in enumerate(result, 1):
        #     print(f"{i}. {art['title']}\n   Ссылка: {art['link']}\n   Кратко: {art['summary']}\n") # ДЛЯ ОТЛАДКИ

        play_sfx("silent_execution")
        return result
    
    except requests.RequestException as e:
        logger.error(f"Error requesting page: {e}")
        play_sfx("silent_error")
        return []
    except Exception as e:
        logger.error(f"Parsing error: {e}")
        play_sfx("silent_error")
        return []

def get_system_metrics():
    """Возвращает текущую нагрузку процессора, видеокарты и оперативной памяти один раз. 
    Требует открытого OpenHardwareMonitor."""
    play_sfx("silent_execution")
    try:
        w = wmi.WMI(namespace="root\OpenHardwareMonitor")
        sensors = w.Sensor()
        if not sensors:
            logger.info("No sensors are available. OpenHardwareMonitor may not be running.")
            return "No sensors are available. OpenHardwareMonitor may not be running. Needs to be launched."

        cpu_temp = None
        cpu_load = None
        gpu_temp = None
        gpu_load = None
        ram_load = None

        # Ищем нужные сенсоры
        for sensor in sensors:
            if sensor.SensorType == "Temperature" and sensor.Name == "Temperature":
                cpu_temp = sensor.Value
            elif sensor.SensorType == "Load" and sensor.Name == "CPU Total":
                cpu_load = sensor.Value
            elif sensor.SensorType == "Temperature" and sensor.Name == "GPU Core":
                gpu_temp = sensor.Value
            elif sensor.SensorType == "Load" and sensor.Name == "GPU Core":
                gpu_load = sensor.Value
            elif sensor.SensorType == "Load" and sensor.Name == "Memory":
                ram_load = sensor.Value

        # Форматируем значения
        cpu_temp = f"{cpu_temp:.1f}°C" if cpu_temp is not None else "Недоступно"
        cpu_load = f"{cpu_load:.1f}%" if cpu_load is not None else "Недоступно"
        gpu_temp = f"{gpu_temp:.1f}°C" if gpu_temp is not None else "Недоступно"
        gpu_load = f"{gpu_load:.1f}%" if gpu_load is not None else "Недоступно"
        ram_load = f"{ram_load:.1f}%" if ram_load is not None else "Недоступно"
        now = datetime.now()

        # Вывод в одну строку
        output = (f"Readings from the main PC sensors ({now.strftime('%H:%M:%S')}): \nCPU: {cpu_temp}, {cpu_load}; \nGPU: {gpu_temp}, {gpu_load}; \nRAM: {ram_load}")
        
        return output
    
    except Exception as e:
        logger.error(f"Error: {str(e)}. Make sure OpenHardwareMonitor is running.")
        play_sfx("silent_error")
        return f"Error: {str(e)}. Make sure OpenHardwareMonitor is running."
    
# УПРАВЛЕНИЕ ПК, МЫШЬ, КЛАВИАТУРА 

def get_windows_layout():
    """Возвращает текущую раскладку клавиатуры в Windows. Возвращает строку вроде "ENG","RUS" и прочее."""
    play_sfx("silent_execution")
    if platform.system() != "Windows":
        play_sfx("silent_error")
        return "Not a Windows system"

    # Словарь популярных раскладок. Полный список можно найти по запросу "Windows Language Code Identifier"
    layouts = {
        0x409: "ENG", 0x419: "RUS", 0x407: "GER",
        0x40C: "FRA", 0x410: "ITA", 0x411: "JPN", 
        0x412: "KOR", 0x804: "CHN" 
    }

    # Загружаем библиотеку user32.dll
    user32 = ctypes.WinDLL('user32', use_last_error=True)
    hwnd = user32.GetForegroundWindow()
    thread_id = user32.GetWindowThreadProcessId(hwnd, None)
    layout_id = user32.GetKeyboardLayout(thread_id)
    language_id = layout_id & 0xFFFF

    logger.debug(layouts.get(language_id, f"Unknown layout (ID: {hex(language_id)})"))
    return layouts.get(language_id, f"Unknown layout (ID: {hex(language_id)})")

def move_mouse(x, y):
    "Двигает мышь в указанном направлении."
    play_sfx("silent_execution")
    pyautogui.moveTo(x, y, duration=0.05)
    logger.debug(f"The mouse is moved to coordinates: {x}, {y}")
    return f"The mouse is moved to coordinates: {x}, {y}"

def current_mouse_coordinates():
    "Определяет текущие координаты мыши."
    play_sfx("silent_execution")
    current_position = pyautogui.position()
    logger.debug(f"The mouse is currently at X={current_position.x}, Y={current_position.y}")
    return f"The mouse is currently at X={current_position.x}, Y={current_position.y}"

def click_mouse(button='left', clicks=1, interval=0.1):
    """Кликнуть мышью. Левой, правой, одинарный, двойной - на выбор."""
    play_sfx("silent_execution")
    pyautogui.click(button=button, clicks=clicks, interval=interval)
    logger.debug(f"Performed {clicks} click(s) with the {button} mouse button.")
    return f"Performed {clicks} click(s) with the {button} mouse button."

def scroll_mouse(amount):
    """Скроллит вверх (положительное число) или вниз (отрицательное)."""
    play_sfx("silent_execution")
    pyautogui.scroll(amount)
    direction = "up" if amount > 0 else "down"
    logger.debug(f"Scrolled {abs(amount)} units {direction}.")
    return f"Scrolled {abs(amount)} units {direction}."

def drag_mouse(x_to, y_to, duration=0.5):
    """Тащит мышь из текущей позиции в точку (x, y), как будто что-то выделяет."""
    pyautogui.dragTo(x_to, y_to, duration=duration)
    logger.debug(f"Dragged mouse to {x_to}, {y_to}.")
    play_sfx("silent_execution")
    return f"Dragged mouse to {x_to}, {y_to}."
    
def press_hotkey(keys):
    """Нажимает любое количество горячих клавиш. Например: ('ctrl', 'shift', 'esc')"""
    play_sfx("silent_execution")
    pyautogui.hotkey(*keys)
    logger.debug(f"Hotkey pressed: {' + '.join(keys)}")
    return f"Hotkey pressed: {' + '.join(keys)}"

def copy_to_clipboard(text):
    """Копирует любой текст в буфен обмена."""
    play_sfx("silent_execution")
    pyperclip.copy(text)
    logger.debug(f"Text '{text}' copied to clipboard.")
    return f"Text '{text}' copied to clipboard."

def write_text(text, attempts=0):
    """Печатает любой текст, даже на эльфийском."""
    play_sfx("silent_execution")
    keyboard.write(text)
    

def system_command(command):
    """Выполняет системные команды. Выключение, перезагрузка. ОПАСНО."""
    play_sfx("silent_execution")
    # Примеры команд для Windows:
    # 'shutdown /s /t 1' - выключить пк через 1 секунду, 'shutdown /r /t 1' - перезагрузить пк через 1 секунду, 'rundll32.exe powrprof.dll,SetSuspendState 0,1,0' - отправить в спящий режим
    os.system(command)
    logger.info(f"Executing system command: {command}.")
    return f"Executing system command: {command}."

# ОКНА, ПРОГРАММЫ, ПРИЛОЖЕНИЯ 

def get_processes():
    """Сканирует систему и возвращает только полезный список процессов, отфильтровав все ненужные/системные."""
    play_sfx("silent_execution")
    # Сюда кладем все системные процессы, которые нам нужны
    system_processes_blacklist = [
        "svchost.exe", "lsass.exe", "csrss.exe", "wininit.exe", "services.exe", "winlogon.exe", "dwm.exe", "spoolsv.exe",
        "explorer.exe", "rundll32.exe", "ctfmon.exe", "fontdrvhost.exe", "conhost.exe", "sihost.exe", "taskhostw.exe", "RuntimeBroker.exe",
        "ApplicationFrameHost.exe", "SearchHost.exe", "ShellExperienceHost.exe", "StartMenuExperienceHost.exe", "SystemSettings.exe", "backgroundTaskHost.exe",
        "unsecapp.exe", "System", "System Idle Process", "SecurityHealthSystray.exe", "nvcontainer.exe", "steamwebhelper.exe", "lghub_agent.exe", 
        "msedgewebview2.exe", "OneDrive.Sync.Service.exe", "CrossDeviceResume.exe", "LockApp.exe", "ShellHost.exe", "UserOOBEBroker.exe",
        "WebViewHost.exe", "WidgetService.exe", "Widgets.exe", "XboxPcApp.exe", "XboxPcAppFT.exe", "XboxPcTray.exe", "lghub_system_tray.exe",
        "ruff.exe", "winws.exe"
    ]
    filtered_list = []

    # Получаем имя текущего пользователя, чтобы отсеять процессы других юзеров
    current_user = os.getlogin()

    for process in psutil.process_iter(['pid', 'name', 'username']):
        try:
            proc_info = process.info
            proc_name = proc_info['name']
            proc_user = proc_info['username']

            if not proc_user: # Отсеиваем процессы, у которых нет имени пользователя (обычно это системные)
                continue
            if current_user not in proc_user and 'SYSTEM' not in proc_user: # Оставляем только процессы текущего пользователя
                continue
            if proc_name.lower() in [p.lower() for p in system_processes_blacklist]: # Отсеиваем всё из нашего чёрного списка (без учёта регистра)
                continue

            # Если процесс прошёл все круги ада, добавляем его в список
            filtered_list.append(proc_name)

        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass
    
    # Возвращаем уникальный список, чтобы не было дубликатов
    # (например, 10 процессов chrome.exe превратятся в один)
    logger.debug(f"Current processes in system: {sorted(list(set(filtered_list)))}")
    return f"Current processes in system: {sorted(list(set(filtered_list)))}"

def currently_open_windows():
    """Возвращает текущие окна, которые открыты."""
    play_sfx("silent_execution")
    titles = []
    all_titles = gw.getAllTitles()
    for title in all_titles:
        if title: # Игнорируем пустые заголовки
            titles.append(title)
    return titles
    






========================================
# Содержимое файла: assistant_tools/skills_diagrams.py
========================================

# skills_diagrams.py
get_weather_scheme = {
    "name": "get_weather", # ВАЖНО: ИМЯ ДОЛЖНО СОВПАДАТЬ С ФУНКЦИЕЙ PYTHON
    "description": "Find the current weather in the specified city. This is necessary to answer weather questions with up-to-date data. If no city is specified, Lipetsk is the default location.",
    "parameters": {
        "type": "object",
        "properties": {
            "city_name": {
                "type": "string",
                "description": "The city for which the weather is needed. For example: Moscow.",
            },
        },
    },
}

search_in_google_scheme = {
    "name": "search_in_google",
    "description": "Searches for the given query in a search engine and opens a browser tab. Use this if you need to Google something or open a tab.",
    "parameters": {
        "type": "object",
        "properties": {
            "search_query": {
                "type": "string",
                "description": "Search query. For example: Who is Elon Musk",
            },
        },
    },
}

get_time_scheme = {
    "name": "get_time",
    "description": "Gets the current actual time.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

get_date_scheme = {
    "name": "get_date",
    "description": "Gets the current actual date.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}


make_screenshot_scheme = {
    "name": "make_screenshot",
    "description": "Takes a screenshot of the user's home screen and saves it to a file. Returns JSON with the path to the created file.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

save_to_memory_scheme = {
    "name": "save_to_memory",
    "description": "Saves a new fact or piece of information to Vega's long-term memory. Use this when the user provides important new information about themselves, their plans, projects, or asks you to remember something.",
    "parameters": {
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "The specific, concise fact to be saved. For example: 'The user's dog is named Rex.' or 'The user is working on a post-apocalyptic car combat game.'",
            },
        },
        "required": ["text"]
    },
}

lock_pc_scheme = {
    "name": "lock_pc",
    "description": "Locks the user's workstation.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}


get_windows_layout_scheme = {
    "name": "get_windows_layout",
    "description": "Returns the current keyboard layout in Windows. Returns a string such as 'ENG', 'RUS', and so on.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

move_mouse_scheme = {
    "name": "move_mouse",
    "description": "Moves the mouse cursor to the specified X and Y coordinates on the screen.",
    "parameters": {
        "type": "object",
        "properties": {
            "x": {
                "type": "integer",
                "description": "The X-coordinate to move the mouse to."
            },
            "y": {
                "type": "integer",
                "description": "The Y-coordinate to move the mouse to."
            }
        },
        "required": ["x", "y"]
    }
}

current_mouse_coordinates_scheme = {
    "name": "current_mouse_coordinates",
    "description": "Gets the current X and Y coordinates of the mouse cursor.",
    "parameters": {
        "type": "object",
        "properties": {}
    }
}

click_mouse_scheme = {
    "name": "click_mouse",
    "description": "Performs a mouse click. Can be left, right, middle, single, double, etc.",
    "parameters": {
        "type": "object",
        "properties": {
            "button": {
                "type": "string",
                "description": "The mouse button to click: 'left', 'right', or 'middle'. Default is 'left'."
            },
            "clicks": {
                "type": "integer",
                "description": "The number of times to click. Default is 1."
            }
        }
    }
}

scroll_mouse_scheme = {
    "name": "scroll_mouse",
    "description": "Scrolls the mouse wheel up or down.",
    "parameters": {
        "type": "object",
        "properties": {
            "amount": {
                "type": "integer",
                "description": "The number of units to scroll. Positive for up, negative for down."
            }
        },
        "required": ["amount"]
    }
}

drag_mouse_scheme = {
    "name": "drag_mouse",
    "description": "Drags the mouse from its current position to the specified X and Y coordinates. Useful for selecting text or moving items.",
    "parameters": {
        "type": "object",
        "properties": {
            "x_to": {
                "type": "integer",
                "description": "The destination X-coordinate for the drag."
            },
            "y_to": {
                "type": "integer",
                "description": "The destination Y-coordinate for the drag."
            }
        },
        "required": ["x_to", "y_to"]
    }
}

press_hotkey_scheme = {
    "name": "press_hotkey",
    "description": "Presses a combination of keyboard keys simultaneously. For example, ('ctrl', 'c') to copy.",
    "parameters": {
        "type": "object",
        "properties": {
            "keys": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "A list of keys to press together, for example: ['alt', 'f4']"
            }
        },
        "required": ["keys"]
    }
}

copy_to_clipboard_scheme = {
    "name": "copy_to_clipboard",
    "description": "Copies the given text to the system clipboard.",
    "parameters": {
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "The text to be copied."
            }
        },
        "required": ["text"]
    }
}

write_text_scheme = {
    "name": "write_text",
    "description": "Types out the given text in the currently active window. Works with any language.",
    "parameters": {
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "The text to be typed."
            }
        },
        "required": ["text"]
    }
}

system_command_scheme = {
    "name": "system_command",
    "description": "Executes a system command, such as shutdown or restart. EXTREMELY DANGEROUS.",
    "parameters": {
        "type": "object",
        "properties": {
            "command": {
                "type": "string",
                "description": "The system command to execute. For Windows: 'shutdown /s /t 1' for shutdown, 'shutdown /r /t 1' for restart."
            }
        },
        "required": ["command"]
    }
}

get_processes_scheme = {
    "name": "get_processes",
    "description": "Returns a clean list of user-run applications, filtering out system processes.",
    "parameters": {
        "type": "object",
        "properties": {}
    }
}

currently_open_windows_scheme = {
    "name": "currently_open_windows",
    "description": "Returns a list of titles of all currently open windows.",
    "parameters": {
        "type": "object",
        "properties": {}
    }
}

manage_window_scheme = {
    "name": "manage_window",
    "description": "Finds a window by its title and performs an action on it.",
    "parameters": {
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title (or part of the title) of the window to manage. For example: 'foobar2000'."
            },
            "action": {
                "type": "string",
                "description": "The action to perform: 'activate', 'minimize', 'maximize', or 'close'. Default is 'activate'."
            }
        },
        "required": ["title"]
    }
}

open_program_scheme = {
    "name": "open_program",
    "description": "Opens a program or file using its full path.",
    "parameters": {
        "type": "object",
        "properties": {
            "path_to_exe": {
                "type": "string",
                "description": "The full path to the executable or file to open. For example: 'C:\\Program Files\\...\\chrome.exe'."
            }
        },
        "required": ["path_to_exe"]
    }
}


kill_process_by_name_scheme = {
    "name": "kill_process_by_name",
    "description": "Forcibly terminates a running process by its name (e.g., 'chrome.exe'). DANGEROUS.",
    "parameters": {
        "type": "object",
        "properties": {
            "process_name": {
                "type": "string",
                "description": "The name of the process executable to kill."
            }
        },
        "required": ["process_name"]
    }
}

get_system_volume_scheme = {
    "name": "get_system_volume",
    "description": "Получает текущее значение громкости системы в процентах.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

set_system_volume_scheme = {
    "name": "set_system_volume",
    "description": "Устанавливает абсолютное значение громкости системы. Принимает значение от 0 до 100.",
    "parameters": {
        "type": "object",
        "properties": {
            "level_volume": {
                "type": "integer",
                "description": "Целевой уровень громкости в процентах (например, 50).",
            },
        },
        "required": ["level_volume"] # Явно указываем, что этот параметр обязателен
    },
}

decrease_volume_scheme = {
    "name": "decrease_volume",
    "description": "Уменьшает громкость системы на указанное значение. По умолчанию уменьшает на 10%.", 
    "parameters": {
        "type": "object",
        "properties": {
            "amount": {
                "type": "integer",
                "description": "Значение в процентах, на которое нужно уменьшить громкость (например, 15).",
            },
        },
        # "required" здесь не нужен, так как у amount есть значение по умолчанию (он опциональный)
    },
}

increase_volume_scheme = {
    "name": "increase_volume",
    "description": "Увеличивает громкость системы на указанное значение. По умолчанию увеличивает на 10%.",
    "parameters": {
        "type": "object",
        "properties": {
            "amount": { 
                "type": "integer",
                "description": "Значение в процентах, на которое нужно увеличить громкость (например, 20).",
            },
        },
    },
}

get_habr_news_scheme = {
    "name": "get_habr_news",
    "description": "Receives the latest news from Habr.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

get_system_metrics_scheme = {
    "name": "get_system_metrics",
    "description": "Gets the current load, temperature of the video card, processor and RAM in the system.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}


========================================
# Содержимое файла: assistant_tools/socialmedia_skills.py
========================================

# assistant_social_media_tools/telegram_skills.py
import asyncio
import logging
import os
from pyrogram import Client
from dotenv import load_dotenv
from assistant_general.logger_config import setup_logger
from assistant_tools.utils import play_sfx

setup_logger()
logger = logging.getLogger(__name__)

load_dotenv()
API_ID = os.getenv("API_ID")
API_HASH = os.getenv("API_HASH")
SESSION_NAME = "vega_telegram_session"

500

# ДЛЯ РЕГИСТРАЦИИ НОВЫХ НАВЫКОВ В ВЕГУ НУЖНО:
# Написать json схему в social_media_skills_diagrams.py
# Перейти в assistant_brain.added_skills.py и следовать инструкциям, которые описаны в файле

async def _get_telegram_channel_data(client: Client, channel_username: str, limit_posts: int):
    """Внутренняя асинхронная функция, которая делает всю работу с Pyrogram."""
    try:
        chat = await client.get_chat(channel_username)
        play_sfx('silent_execution')
    except Exception as e:
        logger.error(f"Ошибка при получении информации о чате Telegram: {e}")
        play_sfx('error')
        return f"Error: Failed to get channel information {channel_username}: {e}"

    channel_info = {
        "channel_name": chat.title,
        "description": chat.description if chat.description else "Description not available.",
        "subscribers": chat.members_count,
        "posts": []
    }

    if limit_posts > 0:
        logger.info(f"Number of posts in channel {channel_username} to analyze: {limit_posts}.")
        history = client.get_chat_history(channel_username, limit=limit_posts)

        async for message in history: # Проходимся по всем постам, используем 'async for' для асинхронного итератора
            text = message.text or message.caption or "[No Text/Media only]" # В случае ошибок заменить на text = message.text.strip() if message.text else "[Media/No Text]"
            date = message.date.strftime('%Y-%m-%d %H:%M:%S')
            final_text = text.replace('\n', ' ')
            channel_info["posts"].append(f"Post ID: {message.id}; Date: {date}; Text: {final_text}")

    play_sfx('silent_execution')
    return channel_info

# Синхронная обертка для вызова из skills_registry
def get_telegram_channel_info(channel_username: str, limit_posts: int = 500):
    """Запускает асинхронный код и возвращает готовый результат в виде строки. Принимает имя канала в формате @channel_name и количество постов."""
    MODULE_DIR = os.path.dirname(os.path.abspath(__file__)) # Папка, где лежит этот файл

    async def main():
        if not API_ID or not API_HASH:
            logger.critical("API_ID или API_HASH не найдены в .env файле!")
            return "Critical error: Telegram credentials are not configured."
            
        # Указываем workdir='.', чтобы .session файл создавался в корневой папке
        async with Client(SESSION_NAME, api_id=int(API_ID), api_hash=API_HASH, workdir=MODULE_DIR) as client:
            return await _get_telegram_channel_data(client, channel_username, limit_posts)

    try:
        data = asyncio.run(main()) # asyncio.run() - "мост" между синхронностью и асинхронностью
        
        # Красиво форматируем результат для Gemini
        if isinstance(data, dict): # Если данные - словарь с информацией о канале
            posts_str = "\n".join([f"{post}" for post in data['posts']]) 
            result_string = (f"Information about '{data['channel_name']}':\n"
                             f"Subscribers: {data['subscribers']}\n"
                             f"Description: {data['description']}\n"
                             f"\nLatest posts:\n{posts_str if posts_str else 'Posts were not requested.'}")
            logger.info(f"Successfully retrieved data for channel {channel_username}")
            play_sfx('silent_execution')
            return result_string
        else:
            return str(data) # Возвращаем сообщение об ошибке, если оно есть

    except Exception as e:
        logger.error(f"Critical error while running Telegram asynchronous task: {e}")
        play_sfx('error')
        return f"Error: Failed to execute Telegram request: {e}"


if __name__ == "__main__":
    # Тестовый вызов
    test_result = get_telegram_channel_info("@VEGA_and_other_heresy")
    print(test_result)


========================================
# Содержимое файла: assistant_tools/socialmedia_skills_diagrams.py
========================================

get_telegram_channel_info_scheme = {
    "name": "get_telegram_channel_info",
    "description": "Gets up-to-date information about a public Telegram channel: name, description, subscriber count, and recent posts. Useful for analyzing content or getting the latest news from a specific source.",
    "parameters": {
        "type": "object",
        "properties": {
            "channel_username": {
                "type": "string",
                "description": "The channel name in the format @channel_username. For example, the user's channel is called '@VEGA_and_other_heresy'. Other channels can be asked by the user or found on Telegram.",
            },
            "limit_posts": {
                "type": "integer",
                "description": "The number of recent posts to retrieve. Default: 500 (Great if you need to fully analyze someone's channel). Specify '0' if posts are not needed, '5' if the latest 5 posts are needed, and so on.",
            },
        },
        "required": ["channel_username"],
    },
}


========================================
# Содержимое файла: assistant_tools/utils.py
========================================

# utils.py
import random
import pygame
import time
import os

# --- КОНФИГУРАЦИЯ ЗВУКОВ ---
SFX_DIR = 'assistant_sounds'  # Папка со звуковыми файлами
SFX_CONFIG = { # Ключ: название звуков (смотреть в папке assistant_sounds), значение: количество звуков в этой категории (смотреть на название звука, последний символ - номер звука)
    'system_startup': 3,
    'select': 3,
    'hard_processing': 1,
    'start_system': 2,
    'processing': 2,
    'start_embedding_model': 1,
    'search': 2,
    'error': 7,
    'silent_execution': 6,
    'access_error': 2,
    'access_critical_error': 1,
    'execution': 2,
    'notification': 2,
    'silent_error': 1,
    'warning': 2,
    'unknown_command': 1,
}


SOUNDS = {} # Словарь, автоматически создаваемый функцией _init_sounds, выглядит как {'system_startup': (assistant_sounds/system_startup1.mp3, assistant_sounds/system_startup2.mp3, ...), ...}
SOUND_CACHE = {} # Кэш, чтобы не грузить файлы с диска каждый раз

def _init_sounds():
    """Автоматически генерирует словарь SOUNDS на основе конфига."""
    global SOUNDS
    for category, value in SFX_CONFIG.items():
        if isinstance(value, int):
             SOUNDS[category] = tuple(os.path.join(SFX_DIR, f"{category}{i}.mp3") for i in range(1, value + 1)) # range(1, value + 1)) означает: от звука номер 1 и до value (самого последнего) включительно создавать 1 путь, который ведет к звуку
        elif isinstance(value, (list, tuple)):
             SOUNDS[category] = tuple(value)

try:
    # Pre-init может помочь убрать задержку перед первым звуком
    pygame.mixer.pre_init(44100, -16, 2, 1024)
    pygame.mixer.init()
    pygame.mixer.set_num_channels(32) # Разрешаем много одновременных звуков
    _init_sounds()
    print(f"Audio system initialized. Loaded {len(SOUNDS)} sound categories.")
except pygame.error as e:
    print(f"Fatal error: Could not initialize pygame mixer. Error: {e}")
    pygame = None

def play_sfx(sound_name: str, volume: float = 1.0):
    """Проигрывает случайный звук из указанной категории. Не блокирует основной поток."""
    if not pygame or not sound_name:
        return

    if sound_name not in SOUNDS:
        print(f"Warning: Sound category '{sound_name}' not found.")
        return

    try:
        file_path = random.choice(SOUNDS[sound_name])
        
        # Ленивая загрузка и кэширование
        if file_path not in SOUND_CACHE:
            SOUND_CACHE[file_path] = pygame.mixer.Sound(file_path)
        
        sound = SOUND_CACHE[file_path]
        sound.set_volume(volume)
        sound.play()
        
    except Exception as e:
        # Отлавливаем ошибки (например, файла физически нет на диске)
        print(f"Error playing sfx '{sound_name}' (path: {file_path}): {e}")

if __name__ == "__main__":
    print("Sound Utils Test Mode. Type category name to play.")
    while True:
        cmd = input(">> ").strip()
        if cmd.lower() in ['exit', 'quit']: 
            break
        play_sfx(cmd)
        time.sleep(0.05) # Небольшая пауза для UI


========================================
# Содержимое файла: assistant_vector_database/add_new_memory.py
========================================

from datetime import datetime
import uuid
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma  

# Эмбеддинг модель, чтобы превращать запросы пользователя в векторы и искать похожие в базе данных
embedding_model = HuggingFaceEmbeddings(
    model_name = "BAAI/bge-m3", # Можно выбрать intfloat/multilingual-e5-large - она более быстрая, но менее точная
    encode_kwargs = {"normalize_embeddings": True} # при создании векторов из текста делать нормализацию
    ) 

# Векторная база данных
vectorstore = Chroma(
    collection_name="assistant_database", # Называем коллекцию внутри базы данных так
    embedding_function=embedding_model, # # Прикрепление модели эмбеддингов
    persist_directory="""./assistant_chroma_db""", # Сохранять в эту папку
    )

def add_new_memory(new_text: str):
    """Принимает текст, генерирует для него уникальный ID, добавляет текущую дату в метаданные и сохраняет в Chroma."""
    current_date = datetime.now().strftime("%d.%m.%Y")
    record_id = str(uuid.uuid4()) # Генерируем уникальный ID для записи
    
    metadata = {"creation_date": current_date}

    # Добавляем текст, его ID и его метаданные в базу.
    # Важно: add_texts принимает списки, поэтому оборачиваем все в []
    vectorstore.add_texts(
        texts=[new_text], 
        ids=[record_id], 
        metadatas=[metadata]
    )
    
    print(f"New entry added to memory: '{new_text}'")

if __name__ == "__main__":
    while True:
        new_record = input("\nВведите новую запись, которую нужно добавить (введите '0' для выхода): \n>> ")

        if new_record != "0":
            add_new_memory(new_record)
        else:
            break


========================================
# Содержимое файла: assistant_vector_database/database.py
========================================

# database.py
from assistant_tools.utils import play_sfx
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma  
from datetime import datetime
from assistant_event_bus.event_bus import subscribe, publish
import uuid
from assistant_general import general_settings as general_settings

# Эмбеддинг модель, чтобы превращать запросы пользователя в векторы и искать похожие в базе данных
print("Initialization of the embedding model.")
play_sfx("start_system")
embedding_model = HuggingFaceEmbeddings(
    model_name = "BAAI/bge-m3", # Можно выбрать intfloat/multilingual-e5-large - она более быстрая, но менее точная
    encode_kwargs = {"normalize_embeddings": True} # при создании векторов из текста делать нормализацию
    ) 

# Векторная база данных
print("Initialization of vector database.")
play_sfx("start_system")
vectorstore = Chroma(
    collection_name="assistant_database", # Называем коллекцию внутри базы данных так
    embedding_function=embedding_model, # # Прикрепление модели эмбеддингов
    persist_directory="""./assistant_chroma_db""", # Сохранять в эту папку
    )

def add_new_memory(new_text: str):
    """Принимает текст, генерирует для него уникальный ID, добавляет текущую дату в метаданные и сохраняет в Chroma."""
    current_date = datetime.now().strftime("%d.%m.%Y")
    record_id = str(uuid.uuid4()) # Генерируем уникальный ID для записи
    
    metadata = {"creation_date": current_date}

    # Добавляем текст, его ID и его метаданные в базу.
    # Важно: add_texts принимает списки, поэтому оборачиваем все в []
    vectorstore.add_texts(
        texts=[new_text], 
        ids=[record_id], 
        metadatas=[metadata]
    )

    play_sfx('silent_execution')
    print(f"New entry added to memory: '{new_text}'")

def find_records_in_database(**kwargs):
    """Ищет записи в векторной базе данных и форматирует результат в читаемую строку."""
    play_sfx('search')

    # Если запрос пуст
    query = kwargs.get('query')
    if not query:
        play_sfx('silent_error')
        return

    results_with_scores = vectorstore.similarity_search_with_score(query, k=general_settings.NUM_RECORDS_FROM_DATABASE)

    # Если база пуста
    if not results_with_scores:
        result = {"original_query": query, "database_context": "No relevant records were found in the database."}
        print("No relevant records were found in the database.")
        publish("USER_SPEECH_AND_RECORDS_FOUND_IN_DB", result)
        return 
    
    formatted_lines = []

    print("\nSearching for records in the database:")
    
    for document, score in results_with_scores:
        if score <= general_settings.SIMILARITY_THRESHOLD:
            # Если запись ДОСТАТОЧНО похожа, обрабатываем ее
            print(f"Record is relevant enough (score: {score:.2f}, threshold: {general_settings.SIMILARITY_THRESHOLD})")
            
            date = document.metadata.get('creation_date', 'Date not found')
            text = document.page_content
            
            # Добавим оценку в вывод для наглядности
            formatted_lines.append(f"[Score: {score:.2f}] {date}: {text}")
        else:
            # Если запись НЕдостаточно похожа, мы можем ее проигнорировать
            print(f"Record is NOT relevant enough (score: {score:.2f}, threshold: {general_settings.SIMILARITY_THRESHOLD}). Skipping.")
            # Мы можем либо ничего не делать, либо добавить сообщение об этом
            # formatted_lines.append(f"[Not relevant enough, score: {score:.2f}]") 

    # Если после фильтрации не осталось релевантных записей
    if not formatted_lines:
        final_string = "Found some records, but none were similar enough to the query."
    else:
        # Соединяем только релевантные строки
        final_string = "\n".join(formatted_lines)

    play_sfx('silent_execution')
    result = {"original_query": query, "database_context": final_string}
    print(f"\nFound records in database for query '{query}': \n{final_string}")
    
    publish("USER_SPEECH_AND_RECORDS_FOUND_IN_DB", result)


def initialize_database():
    subscribe("USER_SPEECH", find_records_in_database)

if __name__ == "__main__":
    while True:
        new_record = input("Введите новую запись, которую нужно добавить (введите '0' для выхода): \n>>")

        if new_record != "0":
            add_new_memory(new_record)
        else:
            break



========================================
# Содержимое файла: assistant_vector_database/delete_memory.py
========================================

import sys
import os
from datetime import datetime
import uuid
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma  

embedding_model = HuggingFaceEmbeddings(
    model_name = "BAAI/bge-m3", # Можно выбрать intfloat/multilingual-e5-large - она более быстрая, но менее точная
    encode_kwargs = {"normalize_embeddings": True} # при создании векторов из текста делать нормализацию
    ) 

# Векторная база данных
vectorstore = Chroma(
    collection_name="assistant_database", # Называем коллекцию внутри базы данных так
    embedding_function=embedding_model, # # Прикрепление модели эмбеддингов
    persist_directory="""./assistant_chroma_db""", # Сохранять в эту папку
    )


def delete_specific_records(ids_to_delete: str):
    """Удаляет записи из ChromaDB по списку их уникальных ID."""
    if not ids_to_delete:
        print("Список ID для удаления пуст. Ничего не сделано.")
        return

    print("Происходит удаление записей.")

    try:
        # Получаем записи, чтобы показать, что именно мы удаляем
        records_to_check = vectorstore.get(ids=ids_to_delete, include=["documents"])
        
        if not records_to_check['ids']:
            print("Ни одна из указанных записей не найдена в базе данных.")
            return

        print("\n--- Будет удалена следующая запись: ---")
        for i, doc_id in enumerate(records_to_check['ids']):
            doc_text = records_to_check['documents'][i]
            print(f"  - ID: {doc_id}")
            print(f"    Текст: {doc_text}")
        
        # Запрашиваем подтверждение
        confirm = input("\nВы уверены, что хотите продолжить? (y/n): \n>> ")
        if confirm.lower() != 'y':
            print("Удаление отменено.")
            return

        # Удаляем записи по их ID
        vectorstore.delete(ids=ids_to_delete)
        
        print(f"✅ Запись успешно удалена.")

    except Exception as e:
        print(f"\n❌ Произошла ошибка при удалении: {e}")

if __name__ == "__main__":
    print("Точные ID для удаления записей брать с помощью функции inspect_memory")
    while True:
        ids_to_delete = input("\nВведите точное ID записи для удаления: \n\n>> ")

        delete_specific_records(ids_to_delete)


========================================
# Содержимое файла: assistant_vector_database/inspect_memory.py
========================================

# inspect_memory.py
import chromadb

client = chromadb.PersistentClient(path="assistant_chroma_db") 

def inspect_memory():
    """Выводит ВСЕ записи в базе данных"""
    try:
        collection_name = "assistant_database"
        collection = client.get_collection(name=collection_name)
        print(f"Successfully connected to collection: '{collection_name}'")

    except Exception as e:
        print(f"Error connecting to collection: {e}")
        print("Available collections:", [col.name for col in client.list_collections()])
        exit()

    try:
        all_records = collection.get(
            include=["metadatas", "documents"]
        )

        num_records = len(all_records['ids'])
        print(f"\n--- Found {num_records} records in V.E.G.A. memory ---\n")

        for i in range(num_records):
            record_id = all_records['ids'][i]
            document = all_records['documents'][i]
            metadata = all_records['metadatas'][i]

            print(f"--- Record ID: {record_id} ---")
            print(f"Document (Text): {document}")
            print(f"Metadata: {metadata}")
            print("-" * 20 + "\n")

    except Exception as e:
        print(f"An error occurred while fetching records: {e}")


if __name__ == "__main__":
    inspect_memory()


========================================
# Содержимое файла: main.py
========================================

# main.py
import threading
import time
from assistant_brain.brain import initialize_brain, generate_general_greeting  # noqa: F401
from assistant_brain.hotkeys_manager import initialize_hotkeys_manager
from assistant_output.voice_output_eng import SpeechModuleENG
from assistant_output.voice_output_rus import SpeechModuleRUS
from assistant_tools.utils import play_sfx
from assistant_vector_database.database import initialize_database
from assistant_general.general_settings import choose_language

play_sfx('hard_processing')

initialize_brain() # вызывает subscribe("USER_SPEECH", generate_response), чтобы не импортировать сразу весь brain
initialize_database()
initialize_hotkeys_manager()

while True: 
    print("\nPlease, choose language for V.E.G.A.")
    command = input("'1' - russian, '2' - english, '3' - exit \n\n>> ")

    if command == "1": # Если русский язык
        play_sfx("select")
        speech_module = SpeechModuleRUS()
        speech_module.start()
        choose_language("RUSSIAN")
        break

    if command == "2": # Если английский язык 
        play_sfx("select")
        speech_module = SpeechModuleENG()
        speech_module.start()
        choose_language("ENGLISH")
        break

    if command == "3":
        play_sfx("select")
        print("Exit from the V.E.G.A. system.")
        exit()

    else:
        play_sfx("unknown_command")
        print("Invalid mode. Please try again.")

# generate_general_greeting() # Можно закомментировать, чтобы не мешало тестированию

# Написать в пропте, чтобы Вега считала себя моей OS с неким самосознанием - передавать запущенные процессы
# Создать функцию all_names_playlists (Передавает Веге названия всех плейлистов Foobar2000)
# создать функции list_all_memory_entries() и delete_memory_entry(id)
# Написать search_telegram_channels() - навык поиска телеграм каналов по ключевым словам 
# Настроить чертово логирование во ВСЕХ файлах, отправлять в них вопросы пользователя и ответы Веги
# Написать def detailed_weather_for_hour - для осадков на ближайший час (дождь, снег)

# ОБЯЗАТЕЛЬНО НЕ УДАЛЯТЬ ПРОШЛЫЕ ФРАЗЫ ИЗ КРАТКОВРЕМЕННОЙ ПАМЯТИ, КОТОРЫЕ ПЕРЕШЛИ ЧЕРТУ СКОЛЬЗЯЩЕГО ОКНА (по дефолту 50 сообщений) - они должны сохраняться в отдельный общий json
# Обаготить все кратковременные записи метаданными - когда было сказано, в каком контексте, какие действия предпринимались после этого (если предпринимались)
# Настроить так, чтобы при больших перерывах, больше 7 часов, к примеру, кратковременная память из тех самых 50 сообщений (не ОБЩАЯ, а именно та, которая передается Веге при всех запросах) удалялась.
# Перевести добавление записей на Event Bus

# В будущем добавить новые датчики для брифинга и простого приветствия
# Передавать в брифинг информацию о канале, например, количество подписчиков, последние посты и т.д. (можно также передавать прошлые наблюдения о канале, чтобы Вега могла анализировать динамику развития канала)

# Фоновое прослушивание всего окружающего звука, сделать навык, который возвращает последние фразы (последняя минута, например, дополнительно помечать, на какой секунде произошел звук (как раз тут можно потестировать разнование диалогов и отдельных людей/говорящих))

# НЕКОТОРЫЕ ФУНКЦИИ НЕ ИСПОЛЬЗУЮТ EVENT BUS, НО ДОЛЖНЫ ИСПОЛЬЗОВАТЬ (порассуждать)
# Поизучать Redis для кратковременной памяти 
# Изучить текущий Event Bus, зачем именно там нужен класс

# Прослушать из вк
# Все записи голоса
# Джарвис
# И в очередной раз вдохновиться диалогами

while True:
    input_mode = input("\nSelect the input mode ('1' - voice, '2' - text, '3' - output): ")
    if input_mode == "1":
        from assistant_input.voice_input import SpeechListener
        play_sfx("select")
        speech_listener = SpeechListener()
        speech_listener.start()
        break 

    elif input_mode == "2":
        from assistant_input.text_input import text_input_loop
        play_sfx("select")
        text_thread = threading.Thread(target=text_input_loop)
        text_thread.daemon = True
        text_thread.start()
        break 

    elif input_mode == "3":
        play_sfx("select")
        print("Logout from the V.E.G.A. system")
        exit()

    else:
        play_sfx("unknown_command")
        print("Incorrect mode. Please try again.")


# Скачать Rammstein, Slaughter To Prevail, Disturbed, Three Days Grace, Linkin Park, System of a Down, Korn, Deftones в Foobar2000.
# Уменьшать громкость остальных звуков при речи Веги, чтобы её не перебивали

# Можно сделать так, что при копировании чего-то в буфер обмена, это будет озвучиваться и превращаться в файл mp3 (Некая "симуляция" голосовых сообщений, если Вега будет отправлять сообщения в мессенджерах)
# Настроить через EVENT_BUS а также обязательно удалять префикс [V.E.G.A.] 
# Думаю, будет интересным эффектом, когда моему собеседнику внезапно отвечает речь Веги (как раз для этого и создан assistant_temporary_files)

# Создать лист текущих задач для важных задач (по типу "Вега, мониторь этот сайт на предмет новой информации"), которая мониторится каждые n минут 

try:
    while True:
        time.sleep(1)
        # Фоновые задачи
        # См. в assistant_background_tasks\\background_tasks.py

except KeyboardInterrupt:
    print("\nThe program is ending.")

# Изменить _process_interaction
# Как в будущем может выглядеть кратковременная память (обагощенная метаданными):
# В дальнейшем получать данные через .get()

# [
#     {
#         "timestamp": "2025-10-20 18:44:20",
#         "actor": "User",
#         "input_type": "text",
#         "content": "Вега, напиши комментарий от своего имени для этого поста",
#         "triggered_action": {
#             "skill_name": "generate_comment",
#             "parameters": {
#                 "target_post_id": "12345",
#                 "persona": "V.E.G.A."
#             },
#             "output": "copy_to_clipboard"
#         }
#     },
#     {
#         "timestamp": "2025-10-20 18:44:20",
#         "actor": "V.E.G.A.",
#         "input_type": "generated_text",
#         "content": "Я сгенерировала комментарий, Сэр. Надеюсь, ваш контакт оценит пожелание стабилизации режима активности.",
#         "triggered_action": null
#     },
#     {
#         "timestamp": "2025-10-20 19:08:27",
#         "actor": "User",
#         "input_type": "text",
#         "content": "Кстати, запиши в память, что я планирую участвовать в НТО на момент записи.",
#         "triggered_action": {
#             "skill_name": "save_to_memory",
#             "parameters": {
#                 "text": "Пользователь планирует участвовать в НТО (Национальная Технологическая Олимпиада)."
#             },
#             "output": "database_write_success"
#         }
#     }
# ]


========================================
# Содержимое файла: tests/test1.py
========================================

import asyncio
from pyrogram import Client

# Получаем эти данные на https://my.telegram.org/auth (API development tools)
API_ID = 22543405  # <-- ВАШ API_ID
API_HASH = "b49fd0e149ca3bdfaed576836566605b"

SESSION_NAME = "vega_telegram_session" # Имя сессии. Можно выбрать любое. Pyrogram создаст файл сессии (например, vega_telegram_session.session)
VEGA_CHANNEL_USERNAME = "@VEGA_and_other_heresy" # Канал, данные которого мы хотим получить (используем @username или ID)

async def _get_channel_data(client: Client, channel_username: str, limit_posts: int): # Функция должна быть асинхронной (async def)
    """Получает факты о канале, количество подписчиков и все посты. Сначала выводит самые актуальные посты, двигаясь к старым."""
    print(f"Запрос данных для канала: \n-> {channel_username}")

    try:
        chat = await client.get_chat(channel_username) # Все сетевые вызовы Pyrogram требуют 'await'
    except Exception as e:
        print(f"Ошибка при получении информации о чате: {e}")
        return None

    channel_info = {
        "channel_name": chat.title,
        "description": chat.description if chat.description else "No description available.",
        "subscribers": chat.members_count,
        "last_posts": []
    }

    print(f"-> Получено название: {channel_info['channel_name']} | Подписчиков: {channel_info['subscribers']}")

    # history — это асинхронный итератор, мы перебираем его
    history = client.get_chat_history(channel_username, limit=limit_posts) # Получаем историю сообщений (постов) канала
    async for message in history: # Проходимся по всем постам, используем 'async for' для асинхронного итератора
        post_data = {
            "id": message.id,
            "date": message.date.strftime("%Y-%m-%d %H:%M:%S"),
            "text": message.text.strip() if message.text else "[Media / No Text]"
        }
        channel_info["last_posts"].append(post_data)
    
    return channel_info

async def get_channel_data(channel_username: str = VEGA_CHANNEL_USERNAME, limit_posts: int = 0):
    """Основная асинхронная функция для запуска Telegram-клиента и получения данных канала."""
    async with Client(SESSION_NAME, API_ID, API_HASH) as client: # Используем 'async with' для корректного асинхронного запуска

        data = await _get_channel_data(client, channel_username, limit_posts=limit_posts) # Вызываем асинхронную функцию с 'await'

        if data:
            posts = []
            for post in data['last_posts']:
                text = post['text'].replace('\n', ' ')
                posts.append(f"ID публикации: {post['id']}; Дата публикации: {post['date']}; Текст публикации: {text}.")

            final_posts = "\n\n".join(posts)

            print(f"Название канала: {data['channel_name']}; \n\nПодписчики: {data['subscribers']}; \n\nОписание: {data['description']}; \n\nПосты: \n{final_posts}")
            return f"Название канала: {data['channel_name']}; \nПодписчики: {data['subscribers']}; \nОписание: {data['description']}; \nПосты: \n{final_posts}"

if __name__ == "__main__":
    print("Запуск клиента Telegram.")
    # Запускаем асинхронную функцию
    asyncio.run(get_channel_data())





========================================
# Содержимое файла: tests/test2.py
========================================

import asyncio
import datetime

async def toast():
    now1 = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"Toast made at {now1}")
    await asyncio.sleep(5)
    now2 = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"Toast finished at {now2}")

async def make_toast():
    print("Making toast...")
    await toast()


if __name__ == "__main__":
    asyncio.run(make_toast())




========================================
# Содержимое файла: tests/test3.py
========================================

# Тесты подключения FastAPI сервера для обработки команд от клиента
from fastapi import FastAPI
import uvicorn

# Создаем экземпляр FastAPI
app = FastAPI()

# Создаем endpoint, которая будет слушать запросы
@app.get("/")
def read_root():
    return {"Hello": "World"} # Вместо этого он будет отдавать index.html

@app.post("/command")
def process_command(command: dict):
    user_query = command.get("query")
    print(f"Получена команда от клиента: {user_query}")
    # Здесь можно передавать user_query в "мозг" Веги
    return {"status": "Command received", "response": "Thinking..."}

if __name__ == "__main__":
    # Запускаем сервер
    # host="0.0.0.0" обязательно, Это делает сервер видимым в локальной сети
    uvicorn.run(app, host="0.0.0.0", port=8000)


========================================
# Содержимое файла: tests/test4.py
========================================

import sys

print(sys.executable)




========================================
# Содержимое файла: vosk_model/vosk-model-ru-0.42/decode.py
========================================

#!/usr/bin/env python3

from vosk import Model, KaldiRecognizer, SetLogLevel
import sys
import os
import wave

SetLogLevel(0)

if len(sys.argv) == 2:
    wf = wave.open(sys.argv[1], "rb")
else:
    wf = wave.open("decoder-test.wav", "rb")

if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != "NONE":
    print ("Audio file must be WAV format mono PCM.")
    exit (1)

model = Model(".")
rec = KaldiRecognizer(model, wf.getframerate())
rec.SetWords(True)

while True:
    data = wf.readframes(4000)
    if len(data) == 0:
        break
    if rec.AcceptWaveform(data):
        print(rec.Result())
    else:
        print(rec.PartialResult())

print(rec.FinalResult())


