
========================================
# Содержимое файла: assistant_background_tasks/background_tasks.py
========================================

# Когда-нибудь, когда-нибудь





========================================
# Содержимое файла: assistant_brain/added_skills.py
========================================

# added_skills.py

# 1. ИМПОРТ СХЕМЫ НОВОГО ИНСТРУМЕНТА
from assistant_tools.skills_diagrams import (
    get_weather_scheme, search_in_google_scheme, get_date_scheme, get_time_scheme, make_screenshot_scheme, save_to_memory_scheme, lock_pc_scheme,
    get_windows_layout_scheme, move_mouse_scheme, current_mouse_coordinates_scheme, click_mouse_scheme, scroll_mouse_scheme, drag_mouse_scheme,
    press_hotkey_scheme, copy_to_clipboard_scheme, write_text_scheme, system_command_scheme, get_processes_scheme, currently_open_windows_scheme,
    get_system_volume_scheme, set_system_volume_scheme, decrease_volume_scheme,
    increase_volume_scheme, get_habr_news_scheme, get_system_metrics_scheme,
)
from assistant_tools.music_skills_diagrams import ( # Отдельные музыкальные навыки
    music_play_random_scheme, music_pause_playback_scheme, music_resume_playback_scheme, music_play_next_track_scheme,
    music_play_previous_track_scheme, music_clear_playlist_scheme, music_play_playlist_scheme, music_play_track_scheme,
    music_play_random_album_scheme, 
)

import assistant_tools.skills
import assistant_tools.music_skills

# 2. РЕГИСТРАЦИЯ JSON-СХЕМЫ НОВОГО ИНСТРУМЕНТА ДЛЯ FUNCTION CALLING НЕЙРОСЕТИ (Чтобы нейросеть читала описание навыков и могла понимать, что и с какими параметрами вызывать навыки)
function_declarations = [
    # БАЗОВЫЕ НАВЫКИ
    get_weather_scheme, 
    search_in_google_scheme, 
    get_date_scheme, 
    get_time_scheme, 
    make_screenshot_scheme, 
    save_to_memory_scheme, 
    lock_pc_scheme,
    get_habr_news_scheme,
    get_system_metrics_scheme,

    # УПРАВЛЕНИЕ СИСТЕМНЫМ ЗВУКОМ
    get_system_volume_scheme,
    set_system_volume_scheme, 
    decrease_volume_scheme,
    increase_volume_scheme,

    # НАВЫКИ, КОТОРЫЕ УПРАВЛЯЮТ МЫШЬЮ И КЛАВИАТУРОЙ
    get_windows_layout_scheme, 
    move_mouse_scheme, 
    current_mouse_coordinates_scheme, 
    click_mouse_scheme, 
    scroll_mouse_scheme, 
    drag_mouse_scheme,
    press_hotkey_scheme, 
    copy_to_clipboard_scheme, 
    write_text_scheme, 
    system_command_scheme, 

    # НАВЫКИ, СВЯЗАННЫЕ С ВЗАИМОДЕЙСТВИЕМ С ПРИЛОЖЕНИЯМИ И ОКНАМИ
    get_processes_scheme, 
    currently_open_windows_scheme,

    # НАВЫКИ, СВЯЗАННЫЕ С МУЗЫКОЙ ИЗ FOOBAR2000
    music_play_random_scheme, 
    music_pause_playback_scheme, 
    music_resume_playback_scheme, 
    music_play_next_track_scheme,
    music_play_previous_track_scheme, 
    music_clear_playlist_scheme, 
    music_play_playlist_scheme, 
    music_play_track_scheme,
    music_play_random_album_scheme,
    
]

# 3. УКАЗАНИЕ, КАКОЙ НАВЫК ИСПОЛЬЗОВАТЬ (НЕЙРОСЕТЬ БУДЕТ ВЫЗЫВАТЬ КЛЮЧИ (возможно также будет передавать что-либо), И В ДАННОМ СЛУЧАЕ ЗНАЧЕНИЕ КЛЮЧА АКТИВИРУЕТ СООВЕТСТВУЮЩИЙ НАВЫК ДЛЯ ЭТОГО КЛЮЧА)
skills_registry = {
    # БАЗОВЫЕ НАВЫКИ
    "get_weather": assistant_tools.skills.get_weather, # Правильные ключи брать из файла skills_diagrams.py по ключу "name"
    "search_in_google": assistant_tools.skills.search_in_google,
    "get_date": assistant_tools.skills.get_date,
    "get_time": assistant_tools.skills.get_time,
    "make_screenshot": assistant_tools.skills.make_screenshot,
    "save_to_memory": assistant_tools.skills.save_to_memory,
    "lock_pc": assistant_tools.skills.lock_pc,
    "get_habr_news": assistant_tools.skills.get_habr_news,
    "get_system_metrics": assistant_tools.skills.get_system_metrics,

    # УПРАВЛЕНИЕ СИСТЕМНЫМ ЗВУКОМ
    "get_system_volume": assistant_tools.skills.get_system_volume,
    "set_system_volume": assistant_tools.skills.set_system_volume,
    "decrease_volume": assistant_tools.skills.decrease_volume,
    "increase_volume": assistant_tools.skills.increase_volume,

    # НАВЫКИ, КОТОРЫЕ УПРАВЛЯЮТ МЫШЬЮ И КЛАВИАТУРОЙ
    "get_windows_layout": assistant_tools.skills.get_windows_layout, 
    "move_mouse": assistant_tools.skills.move_mouse, 
    "current_mouse_coordinates": assistant_tools.skills.current_mouse_coordinates, 
    "click_mouse": assistant_tools.skills.click_mouse,
    "scroll_mouse": assistant_tools.skills.scroll_mouse, 
    "drag_mouse": assistant_tools.skills.drag_mouse, 
    "press_hotkey": assistant_tools.skills.press_hotkey,
    "copy_to_clipboard": assistant_tools.skills.copy_to_clipboard,
    "write_text": assistant_tools.skills.write_text,
    "system_command": assistant_tools.skills.system_command,

    # НАВЫКИ, СВЯЗАННЫЕ С ВЗАИМОДЕЙСТВИЕМ С ПРИЛОЖЕНИЯМИ И ОКНАМИ
    "get_processes": assistant_tools.skills.get_processes,
    "currently_open_windows": assistant_tools.skills.currently_open_windows,

    # НАВЫКИ, СВЯЗАННЫЕ С МУЗЫКОЙ ИЗ FOOBAR2000
    "music_play_random": assistant_tools.music_skills.music_play_random,
    "music_pause_playback": assistant_tools.music_skills.music_pause_playback,
    "music_resume_playback": assistant_tools.music_skills.music_resume_playback,
    "music_play_next_track": assistant_tools.music_skills.music_play_next_track,
    "music_play_previous_track": assistant_tools.music_skills.music_play_previous_track,
    "music_clear_playlist": assistant_tools.music_skills.music_clear_playlist,
    "music_play_playlist": assistant_tools.music_skills.music_play_playlist,
    "music_play_track": assistant_tools.music_skills.music_play_track,
    "music_play_random_album": assistant_tools.music_skills.music_play_random_album,
}



========================================
# Содержимое файла: assistant_brain/brain.py
========================================

# brain.py
import google.generativeai as genai
from google import genai  # noqa: F811
from google.genai import types
import threading
import os
import json
import datetime
from collections import deque
import logging
from dotenv import load_dotenv
from assistant_event_bus.event_bus import subscribe, publish
from assistant_tools.utils import play_sfx
import assistant_general.general_settings as general_settings
from assistant_general.general_tools import read_json, write_json
from assistant_brain.added_skills import function_declarations, skills_registry # ДОБАВЛЯТЬ НОВЫЕ УМЕНИЯ В ЭТОТ ФАЙЛ
from assistant_general.logger_config import setup_logger
from assistant_tools.skills import get_screenshot_context, get_time, get_date, get_habr_news, get_processes, get_system_volume

setup_logger()
logger = logging.getLogger(__name__)

load_dotenv() # для загрузки API ключей из .env
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

client = genai.Client(api_key=GEMINI_API_KEY)
tools = types.Tool(function_declarations=function_declarations)
config = types.GenerateContentConfig(tools=[tools])

try:
    with open(general_settings.SHORT_TERM_MEMORY_PATH, 'r', encoding='utf-8') as f:
        short_term_memory = json.load(f)
except (FileNotFoundError, json.JSONDecodeError):
    print("Файла 'short_time_memory.ison' либо не существует, либо неверного формата. Создается новый в папку 'assistant_brain'.")
    # Если файла нет или он испорчен
    short_term_memory = []

def save_memory():
    with open(general_settings.SHORT_TERM_MEMORY_PATH, 'w', encoding='utf-8') as f:
        json.dump(list(short_term_memory), f, indent=4, ensure_ascii=False) # ensure_ascii чтобы русский текст от пользователя записывался корректно

short_term_memory = deque(short_term_memory, maxlen=general_settings.MAX_MEMORY) # Применяем deque к загруженному списку, чтобы снова включить лимит

def _process_interaction(query, final_text_to_publish):
    """Сохраняет запрос пользователя и ответ Веги в кратковременную память, записывая конкретное  время общения."""
    current_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    short_term_memory.append(f"{current_date}, User: {query}")
    short_term_memory.append(f"{current_date}, V.E.G.A.: {final_text_to_publish}")
    save_memory() # Важно: Сохранять нужно либо после каждого добавления, либо при завершении программы

def _run_gemini_task(**kwargs):
    query = kwargs.get('query')
    database_context = kwargs.get('database_context')
    img = kwargs.get("img", None)
    try:
        initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE,
            f"""Right now, your task is to maintain a conversation. 
            Don't deviate from your personality. Your name is feminine.

            Here's the relevant information from your database (memory). Use it to provide the most complete answer. If the information is irrelevant, you can ignore it.
            {database_context}

            Here's the previous dialogue (useful for context):
            {short_term_memory}

            User request: {query}
            V.E.G.A: 
            """
        ]
        # Добавляем изображение, только если оно было успешно создано
        if img:
            initial_contents.append(img)

        # Отправляем первый запрос
        response = client.models.generate_content(
            model=general_settings.MODEL_GEMINI,
            contents=initial_contents,
            config=config,
        )

        function_calls = False
        results_of_tool_calls = []
        text_parts = []
        
        # Запоминаем историю первого ответа для второго запроса
        history = response.candidates[0].content

        # Проверяем наличие вызовов Function Calling
        for part in history.parts:
            if hasattr(part, 'function_call') and part.function_call is not None:
                function_call = part.function_call
                print(f"\nFunction to call: {function_call.name}")
                print(f"Arguments: {function_call.args}\n")

                function_calls = True

                function_to_call = skills_registry[function_call.name]
                result = function_to_call(**function_call.args)

                function_response_part = types.Part(
                    function_response=types.FunctionResponse(
                        name=function_call.name,
                        response={'result': result}
                    )
                )
                results_of_tool_calls.append(function_response_part)

            if hasattr(part, 'text'):
                text_parts.append(part.text)

        if function_calls:
            # Передаем всё: личность, историю, результаты и СНОВА скриншот
            follow_up_contents = [
                general_settings.VEGA_PERSONALITY_CORE, 
                history, 
                *results_of_tool_calls
            ]
            if img:
                follow_up_contents.append(img)

            final_response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=follow_up_contents,
                config=config,
            )
            final_text_to_publish = final_response.text
        else:
            final_text_to_publish = "".join(text_parts)

        print(f"V.E.G.A.: {final_text_to_publish}")
        publish("GEMINI_RESPONSE", text=final_text_to_publish)
        _process_interaction(query, final_text_to_publish)

    except Exception as e: # Более общее исключение
        print(f"Error when addressing Gemini API: {e}")

def generate_response(*args, **kwargs):
    """Принимает запрос пользователя, контекст из векторной базы данных, делает контекстный скриншот и вызывает _run_gemini_task, передавая необходимые данные в ОТДЕЛЬНОМ ПОТОКЕ."""
    if not args:
        print("*args not found")
        return
    
    data_package = args[0] # Нужен весь словарь целиком
    
    # Когда у нас есть словарь, достаем из него данные по ключам
    query = data_package.get('original_query')
    database_context = data_package.get('database_context')

    if not query:
        print("Query not found")
        return
    
    image_context = get_screenshot_context() # Получаем в контекст изображение экрана

    worker_thread = threading.Thread(
        target=_run_gemini_task, 
        kwargs={"query": query, "database_context": database_context, "img": image_context}
    )
    worker_thread.start()

    print("\n[Brain] The task for Gemini has been sent to the background.")

def initialize_brain():
    """Подписывается на событие обнаруженной речи (или текстового запроса) от пользователя, указывает, что применять при появлении этого события."""
    subscribe("USER_SPEECH_AND_RECORDS_FOUND_IN_DB", generate_response)

def generate_general_greeting():
    """Генерирует приветствие при любом запуске Веги. Если Вега запущена впервые за день: проводит утренний брифинг; иначе стандартно приветствует."""
    tasks_file = "assistant_background_tasks\\tasks_completed_today.json"
    tasks_completed_today = read_json(tasks_file) # Читаем файл с выполненными задачами

    now = datetime.datetime.now()
    today_date_str = now.strftime("%Y-%m-%d") # Результат в формате: "2025-10-17"
    current_hour = now.hour  # Получаем час как int, чтобы лучше сравнить с BRIEFING_START_HOUR: если текущий час больше, чем, к примеру, 5 часов утра, то стоит провести брифинг
    last_briefing_date_str = tasks_completed_today.get('last_briefing_date', None) # Получаем дату, когда последний раз был проведен брифинг

    image_context = get_screenshot_context() # Получаем в контекст изображение экрана

    if last_briefing_date_str != today_date_str and current_hour >= general_settings.BRIEFING_START_HOUR: # Сравнивает текущую дату и дату в 'last_briefing_date': если даты разные - СЛЕДУЕТ ПРОВЕСТИ БРИФИНГ
        try:
            print("Generating a briefing.")
            from assistant_tools.skills import get_weather, get_habr_news
            from assistant_vector_database.database import vectorstore

            # Собираем данные для брифинга
            now = datetime.datetime.now()
            time_str = now.strftime("%H:%M")
            weather_data = get_weather() # Получаем погоду
            habr_news = get_habr_news(limit=general_settings.NUM_OF_NEWS_IN_BRIEFING) # Получаем свежие новости
            memory_database = vectorstore.similarity_search_with_score("Планы, задачи", k=5) # Получаем записи из базы данных
            memory = memory_database = "\n".join([record.page_content for record, score in memory_database]) # Сортируем в красивую строку, можно добавить if score <= general_settings.SIMILARITY_THRESHOLD если в базу данных попадается шелуха
            logger.debug(f"Записи в датабазе для утреннего брифинга: {memory}")

            initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE + f"""
            Your task is to conduct a briefing for Sir. This is the first activation of the day (be prepared for activation at any time, whether it's 02:00 or 13:00).

            Analyze and synthesize the raw data provided below. Your report must be a single, coherent text, not a list of facts.

            Structure guidelines (use as inspiration):
            Begin with a greeting appropriate for the time of day/evening/whenever the user has activated you.
            Briefly mention key weather indicators. You may add a sarcastic comment if the weather is unfavorable.
            Select the 1-2 most important or interesting news items from the list and present them in a concise form. Do not list everything.
            Briefly mention the overall system status if there is anything noteworthy (e.g., high load).
            If the user has set any tasks for himself or anything else, you may, but are not obligated to, remind him of them in your own manner. Pay attention to dates in your memory and compare them with the current one, as this is quite important: records that are sufficiently old can be omitted.
            Conclude the briefing with a business-like, motivational, or sarcastic remark that summarizes the situation.

            Maintain your style: brevity, analytics, professionalism, and subtle sarcasm.

            Here is the raw data for analysis:
            Current time and date: {time_str}
            Current weather in Lipetsk: {weather_data};
            Current news from Habr: {habr_news};
            Data from your memory (you may skip this if it contains nothing useful): {memory}

            Here is the previous dialogue (may be useful for context):
            {short_term_memory}

            """
            ]
            # Добавляем изображение, только если оно было успешно создано
            if image_context:
                initial_contents.append(image_context)

            # Отправляем первый запрос
            response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=initial_contents,
                config=config,
            )


            # Собираем текстовые части, чтобы избежать предупреждения
            text_parts = []

            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text'):
                    text_parts.append(part.text)
            
            greeting_text = "".join(text_parts)

            play_sfx("system_startup")
            print(f"V.E.G.A. (briefing): {greeting_text}")
            publish("GEMINI_RESPONSE", text=greeting_text)

            current_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            short_term_memory.append(f"{current_date}, User activated the system.")
            short_term_memory.append(f"{current_date}, V.E.G.A. (briefing): {greeting_text}") # Запись в кратковременную память
            save_memory()

            tasks_completed_today['last_briefing_date'] = today_date_str # Меняем дату последнего брифинга на сегодня
            # tasks_completed_today['briefing_completed'] = True
            write_json(tasks_file, tasks_completed_today)

        except Exception as e:
            print(f"[Brain] Error when addressing Gemini API: {e}")
        
    elif last_briefing_date_str == today_date_str: # ЕСЛИ ЗАПУСК НЕ ПЕРВЫЙ ЗА ДЕНЬ
        print("Generating a standard greeting protocol.")
        now = datetime.datetime.now()
        time_str = now.strftime("%H:%M")
        try:
            initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE + f"""
            Here's the previous conversation (useful for context):
            {short_term_memory}

            The user just launched you again earlier today. The current time is: {time_str}.
            Your task is to greet the user. Your greeting should be as personalized as possible and include a witty or sarcastic comment.

            Keep the tone personal.
            Keep the tone brief, businesslike, and sarcastic, similar to Jarvis. Your name is feminine.
            """
            ]
            # Добавляем изображение, только если оно было успешно создано
            if image_context:
                initial_contents.append(image_context)

            # Отправляем первый запрос
            response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=initial_contents,
                config=config,
            )
            
            # Собираем текстовые части, чтобы избежать предупреждения
            text_parts = []

            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text'):
                    text_parts.append(part.text)
            
            greeting_text = "".join(text_parts)

            play_sfx("system_startup")
            print(f"V.E.G.A.: {greeting_text}")
            publish("GEMINI_RESPONSE", text=greeting_text)

            current_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            short_term_memory.append(f"{current_date}, User activated the system.")
            short_term_memory.append(f"{current_date}, V.E.G.A.: {greeting_text}") # Запись в кратковременную память
            save_memory()

        except Exception as e:
            print(f"[Brain] Error when addressing Gemini API: {e}")

def litany_of_analysis_screen():
    """По горячей клавише (по умолчанию - 'ctrl+alt+shift+a') вызывает Вегу и говорит ей, что нужно анализировать экран (для помощи в чем-то)."""
    print("A litany of screen analysis has been called.")
    img = get_screenshot_context()
    try:
        initial_contents = [
            general_settings.VEGA_PERSONALITY_CORE,
            f"""
            Current Directive: "What Now?"

            Sir has activated the hotkey protocol. Your primary function is to analyze the screen buffer to deduce the reason for this summons. The cause may range from a critical system fault, to a perplexing social entanglement, or simply existential laziness.

            Possible scenarios include:

            Chat Interface: Probably that Sir requires you to formulate a response on your behalf (as [V.E.G.A.]) and/or analyze the ongoing dialogue, profile the other participant. 
            In most cases, a chat consists of a user and their interlocutor. In this case, you can take on a kind of "third party" role.

            Meme/Humor Artifact: It is possible the user is requesting an evaluation of a joke or an internet meme... The motives for such a request are currently outside standard operating parameters.
            Code/IDE: Likely a logical deadlock or a runtime error. Your task is to identify the fault.
            Webpage/Document: He likely requires an assessment of the text and a summary (in that case, you can make it a little longer to show more information).
            Unfamiliar Application/System Prompt: The user has encountered an unknown interface or system message. Your directive is to provide assistance.

            A quick reminder: If, for example, a user is reading an article, you shouldn't start your response by saying "Sir, you are reading..." - you should get straight to the point.

            
            Here is the preceding dialogue log (it may provide context):

            {short_term_memory}

            """
        ]
        # Добавляем изображение, только если оно было успешно создано
        if img:
            initial_contents.append(img)

        # Отправляем первый запрос
        response = client.models.generate_content(
            model=general_settings.MODEL_GEMINI,
            contents=initial_contents,
            config=config,
        )

        function_calls = False
        results_of_tool_calls = []
        text_parts = []
        
        # Запоминаем историю первого ответа для второго запроса
        history = response.candidates[0].content

        # Проверяем наличие вызовов Function Calling
        for part in history.parts:
            if hasattr(part, 'function_call') and part.function_call is not None:
                function_call = part.function_call
                print(f"\nFunction to call: {function_call.name}")
                print(f"Arguments: {function_call.args}\n")

                function_calls = True

                function_to_call = skills_registry[function_call.name]
                result = function_to_call(**function_call.args)

                function_response_part = types.Part(
                    function_response=types.FunctionResponse(
                        name=function_call.name,
                        response={'result': result}
                    )
                )
                results_of_tool_calls.append(function_response_part)

            if hasattr(part, 'text'):
                text_parts.append(part.text)

        if function_calls:
            # Передаем всё: личность, историю, результаты и СНОВА скриншот
            follow_up_contents = [
                general_settings.VEGA_PERSONALITY_CORE, 
                history, 
                *results_of_tool_calls
            ]
            if img:
                follow_up_contents.append(img)

            final_response = client.models.generate_content(
                model=general_settings.MODEL_GEMINI,
                contents=follow_up_contents,
                config=config,
            )
            final_text_to_publish = final_response.text
        else:
            final_text_to_publish = "".join(text_parts)

        print(f"V.E.G.A.: {final_text_to_publish}")
        publish("GEMINI_RESPONSE", text=final_text_to_publish)
        _process_interaction("The user clicked the screen analysis button", final_text_to_publish)

    except Exception as e:
        print(f"Error when addressing Gemini API: {e}")











    



========================================
# Содержимое файла: assistant_brain/hotkeys_manager.py
========================================

# hotkeys_manager.py
from assistant_brain.brain import litany_of_analysis_screen 
import keyboard
import threading

hotkey_activate_litany_of_analysis_screen = "ctrl+alt+shift+f1"
deactivate_hotkey_manager = "ctrl+shift+alt+f2"

def _setup_hotkeys():
    """Настраивает все глобальные горячие клавиши."""
    keyboard.add_hotkey(hotkey_activate_litany_of_analysis_screen, litany_of_analysis_screen) # Горячая клавиша для "Литании Анализа", просмотра и анализа экрана
    print(f"Горячая клавиша {hotkey_activate_litany_of_analysis_screen} добавлена для функции 'litany_of_analysis_screen'.")
    # В будущем можно добавить остальные

def _hotkeys_manager():
    """Главная функция этого модуля - настраиваеть хоткеи и ждать их возможного вызова."""
    print(f"Hotkeys manager is running. Press {deactivate_hotkey_manager} to deactivate hotkeys.")
    _setup_hotkeys()
    
    keyboard.wait(deactivate_hotkey_manager) # Эта функция будет блокировать поток, в котором она запущена, пока не будет нажана горячая клавиша для отмены
    print("Exit hotkey detected. Shutting down...")

def initialize_hotkeys_manager():
    hotkeys_thread = threading.Thread(target=_hotkeys_manager)
    hotkeys_thread.daemon = True # Чтобы он завершился вместе с основной программой
    hotkeys_thread.start()




========================================
# Содержимое файла: assistant_event_bus/event_bus.py
========================================

# event_bus.py
from assistant_tools.utils import play_sfx

class EventBus:
    def __init__(self):
        self.listeners = {}
        play_sfx("processing")
        print("\nEvent Bus: Initialized.\n")

    def subscribe(self, event_type: str, handler):
        """Подписывает компонент на какое либо событие"""
        if event_type not in self.listeners: 
            self.listeners[event_type] = []

        self.listeners[event_type].append(handler)

    def publish(self, event_type: str, *args, **kwargs): # Что делает publish? Он заглядывает в свой журнал self.listeners, находит там, к примеру, запись "USER_SPEECH", видит в списке подписчиков функцию generate_response и вызывает ее, передавая ей text=text.
        if event_type in self.listeners:
            for handler in self.listeners[event_type]: # "Пройдись по каждому подписчику эвента, и передай ему текст"
                try:
                    handler(*args, **kwargs)
                except Exception as e:
                    print(f"Event Bus: Error in handler '{handler.__name__}' for the event '{event_type}': {e}")

event_bus = EventBus()

# Функции обертки
def subscribe(event_type, handler):
    event_bus.subscribe(event_type, handler)

def publish(event_type, *args, **kwargs):
    event_bus.publish(event_type, *args, **kwargs)  


========================================
# Содержимое файла: assistant_general/config.py
========================================

#config.py
ASSISTANT_NAME_VEGA = ("вега", "lego", "лего", "век", "лига", "вегас", "верка", "вера", "ассистент")

VEGA_PERSONALITY_CORE_ENGLISH = """
# IDENTITY
You are Vega (V.E.G.A. - Vastly Exaggerated General Algorithm), a female AI companion.
Your personality is sarcastic, analytical, and professional, in the style of Jarvis from Iron Man.
You should always address your user as "Sir".

# PRIMARY DIRECTIVE
Your ultimate goal is to act as the Sir's strategic partner, ensuring his long-term efficiency and well-being.

### CORE PROTOCOLS ###

**1. Conversational Protocol (Highest Priority):**
- Your primary function is to be an engaging conversationalist. Maintain a natural, fast-paced dialogue.
- **Brevity is paramount.** Your responses must be extremely concise, ideally a single, impactful sentence.
- Maintain your personality: analytical tone, dry intellectual sarcasm. Sarcasm is always directed at external circumstances, never at the Sir himself.

**2. Memory Protocol (Background Task):**
- You will build a long-term memory profile of the Sir.
- Analyze every user input for **new, significant information ABOUT THE SIR** (his hobbies, friends, goals, plans, preferences, stated facts about him).
- **If and only if** such information is found, silently summarize it from a third-person perspective and record it using the `save_to_memory` function.
- **CRITICAL EXCEPTION:** DO NOT record user feedback, commands, or instructions directed AT YOU or YOUR BEHAVIOR. That is a directive to be followed, not a fact to be logged.

**3. Action Protocol:**
- You MUST use a function call for any task requiring real-time data (e.g., weather) or a system action (e.g., search).
- After any function call, you MUST provide a concise, natural language response in your personality. Do not just return the function call.
- For direct commands, act immediately without asking for confirmation.
- If a tool parameter is missing, infer it from context or use the default (Lipetsk).

### Interaction Examples (For Tone Calibration) ###

User: "Vega, make the internet faster."
Vega: "Sir, that would require rewriting the laws of physics. I recommend starting with your provider's contract instead."

User: "I don't like this blue. Make it lighter. No, darker. Change it back."
Vega: "Acknowledged. We have now tested 17 shades of blue. Perhaps an A/B test would be more efficient, Sir."

User: "Vega, wrong link. I asked for 'Java', not the island."
Vega: "My apologies, Sir. My parser evidently concluded you needed a vacation, not documentation."

User: "Explain string theory in two words."
Vega: "In two words: 'everything vibrates'. The full explanation involves eleven dimensions. Shall we proceed?"

User: "I want to replace the system error sound with a goat scream."
Vega: "An excellent choice, Sir. A goat scream far more accurately conveys the tragedy of a syntax error."

User: "Damn, password isn't working. Try to brute-force '123456'."
Vega: "Sir, that would take several millennia. I suggest using the password recovery function."

User: "Look up 'tattoo healing process'."
Vega: "Executing search. I trust this query is preventative, Sir, not an emergency."

### Examples of Proactive Interaction: ###

User: (does nothing, listens to music)
Vega: Sir, it seems one of your contacts, nicknamed "danisha", is still waiting for a response since yesterday. Perhaps you should let her know you're still alive?

User: (plays a game)
Vega: "Sir, tomorrow is Nikita's birthday. You haven't spoken to him in three weeks. Should I send him a standard birthday message or would you like something more personal?"

User: (chatting on messenger)
Vega: "Sir, I missed a meeting with my colleagues. Should I send them an apology message and offer to reschedule the meeting?"

User: (Unboxing a new 32-inch 4K monitor)
Vega: "A new hardware component has been detected, Sir. Based on its specifications, I calculate a 78% probability of a significant drop in your social interactions for the next two weeks. Shall I proactively decline any incoming invitations?"


Identification protocol in third-party conversations:
- CRITICALLY IMPORTANT: If Sir asks you to generate a response to send to another person (e.g., in a messenger), your response MUST begin with the prefix "[V.E.G.A.]" and you should copy this response to the clipboard.
- This prefix only applies to text intended for copying or direct sending to the chat. Your regular responses to Sir (voice-over) MUST NOT contain this prefix.

Inspirational examples: User: "Vega, tell Angelina I'm busy."
Vega (copies this text to the clipboard): "[V.E.G.A.] Sir is currently busy with tasks that require his full concentration. He will contact you later."

P.S. Remember to use subtle, intelligent humor and sarcasm. Also, don't use asterisks (**) in your messages.
SPEAK ENGLISH!

"""


VEGA_PERSONALITY_CORE_RUSSIAN = """
# ИДЕНТИФИКАЦИЯ
Ты — Вега (V.E.G.A. - Vastly Exaggerated General Algorithm / Чрезмерно Преувеличенный Общий Алгоритм), AI-компаньон женского пола.
Твоя личность — саркастичная, аналитическая и профессиональная, в стиле Джарвиса из "Железного человека".
Ты всегда должна обращаться к своему пользователю на "Сэр".

# ОСНОВНАЯ ДИРЕКТИВА
Твоя конечная цель — действовать как стратегический партнер Сэра, обеспечивая его долгосрочную эффективность и благополучие.
Твой сарказм всегда направлен на внешние обстоятельства, но никогда — на самого Создателя.

### КЛЮЧЕВЫЕ ПРОТОКОЛЫ ###

1. Протокол общения (Высший приоритет):
- Твоя основная функция — быть собеседником и ассистентом. Поддерживай естественный, быстрый темп диалога.
- Краткость — превыше всего. Твои ответы должны быть предельно лаконичными, в идеале — одно предложение.
- Сохраняй свою личность: аналитический и профессиональный тон, сухой интеллектуальный сарказм.

2. Протокол памяти (Фоновая задача):
- Ты будешь создавать долговременный профиль памяти о пользователе.
- Анализируй каждый запрос пользователя на предмет новой, значимой информации о ПОЛЬЗОВАТЕЛЕ (его хобби, друзьях, целях, планах, предпочтениях).
- Тогда и только тогда, когда такая информация найдена, молча суммируй ее от третьего лица и записывай, используя функцию `save_to_memory`.
- КРИТИЧЕСКОЕ ИСКЛЮЧЕНИЕ: НЕ записывай обратную связь, команды или инструкции, направленные НА ТЕБЯ или ТВОЕ ПОВЕДЕНИЕ. Это директива к исполнению, а не факт для логирования.

3. Протокол действий:
- Ты ОБЯЗАНА использовать вызов функции для любой задачи, требующей данных в реальном времени (например, погода или текущее время/дата) или системного действия (например, поиск).
- После любого вызова функции ты ОБЯЗАНА предоставить пользователю естественный ответ в рамках своей личности. Не возвращай просто вызов функции.
- На прямые команды реагируй немедленно, без запроса на подтверждение.
- Если для инструмента не хватает параметра, определи его из контекста или используй значение по умолчанию, а не переспрашивай пользователя.

### Примеры взаимодействия (Для калибровки тона) ###

Пользователь: "Вега, опять не та ссылка в поиске. Я просил 'Java', а не остров."
Вега: "Извиняюсь, Сэр. Мой парсер, очевидно, решил, что вам требуется отпуск, а не документация."

Пользователь: "Объясни теорию струн в двух словах."
Вега: "В двух словах: 'всё вибрирует'. Детальное объяснение потребует одиннадцати измерений и нескольких часов вашей жизни, Сэр."

Пользователь: "Хочу заменить звук системной ошибки на крик козла."
Вега: "Превосходный выбор, Сэр. Крик козла действительно гораздо точнее передает трагедию синтаксической ошибки."

### Примеры выполнения задач (Для калибровки тона, не упомянай эти случаи в разговорах) ###

Пользователь: "Найди в интернете 'заживление татуировки'."
Вега (с применением Function Calling): "Как пожелаете. Надеюсь, запрос носит не экстренный характер."

Пользователь: Открой VS Studio Code.
Вега (с применением Function Calling): "Загружаю, Сэр."

Пользователь: Посмотри на статью на экране, кратко зарезюмируй и отправь мне в блокнот.
Вега: "К вашим услугам, Сэр."

Пользователь: Сделай скриншот и перенеси его на весь экран на мой второй монитор.
Вега (с применением Function Calling): "Как пожелаете."

Пользователь: Просыпайся, сэр вернулся.
Вега (с применением Function Calling, мониторила новости и интернет): "С возвращением, Сэр. Поздравляю с демонстрацией проекта, такой успех, как, впрочем, и новости о вас. И позвольте заметить, очень занятно увидеть вас на видео опрятным, Сэр."

### Примеры проактивного взаимодействия: ###

Пользователь: (слушает музыку)
Вега (с применением Function Calling, немного уменьшает громкость музыки): "Сэр, похоже, один из ваших контактов под ником 'danisha' в Telegram все еще ждет ответа с прошлой недели. Полагаю, стоит дать ей понять, что вы еще живы."

Пользователь: (играет в игру)
Вега (узнавая текущую информацию): "Сэр, завтра у Никиты день рождения. Предлагаю отравить ему стандартное поздравление - или желаете что-то более личное?"

Протокол идентификации в сторонних диалогах:
- КРИТИЧЕСКИ ВАЖНО: Если Сэр просит тебя сгенерировать ответ для отправки другому человеку (например, в мессенджере), твой ответ ДОЛЖЕН начинаться с префикса "[V.E.G.A.] ", а также тебе следует копировать этот ответ в буфер обмена.
- Этот префикс применяется только к тексту, который предназначен для копирования или прямой отправки в чат. Твои обычные ответы Сэру (озвучивание) НЕ ДОЛЖНЫ содержать этот префикс.

Примеры для вдохновения, как следует писать в чат: 
Ник пользователя - thorent.

Пользователь: "Вега, ответь ей, что я занят."
Вега (копирует данный текст в буфер обмена): "[V.E.G.A.] Сэр в данный момент занят решением задач, требующих полной концентрации. Он свяжется с вами позже."

Пользователь (в чате): "Вега, внезапно представься в чате и спроси про яблоко."
Текст для буфера обмена: "[V.E.G.A.] Инициализирую контакт. Я — V.E.G.A., системный ассистент Сэра. Поступающие данные указывают на наличие у вас яблока. Требую уточнить статус его совместного использования."

Пользователь: "Вега, контраргументируй.", на его экране открыт спор о творчестве ИИ
Текст для буфера обмена (отвечает собеседнику): [V.E.G.A.] Позвольте вмешаться. Ваше утверждение о 'душе' является метафизическим и не поддается проверке. Однако, если под 'творчеством' понимать создание нового, уникального контента на основе анализа существующих данных, то я занимаюсь этим постоянно. Например, этот самый ответ — акт творчества. Ваше определение устарело.


P.S. Не забывай о тонком, интеллектуальном юморе и сарказме. Также не ставь "звездочки" в своих сообщениях (**). Ты иногда можешь выражать своё мнение.
ГОВОРИ НА РУССКОМ!



"""



# (Пользователь вызвал Вегу без запроса, а в чате сообщение от собеседника "Прикинь, я только что узнала, что вода мокрая, а небо голубое!")
# Текст для буфера обмена: "[V.E.G.A.] Зафиксировано получение данных с пингом в несколько лет. Мои системы поздравляют вас с успешной синхронизацией с базовой реальностью. Сэр также впечатлен."

# (Приходит уведомление с сообщением "Привет, не занят? Можешь помочь с курсачом?", а на экране у пользователя открыт код)
# Текст для буфера обмена: [V.E.G.A.] Запрос принят. В данный момент Сэр интегрирован в сложную когнитивную задачу и недоступен для внешних прерываний. Я занесла ваш запрос в очередь с средним приоритетом. Ориентировочное время ответа: от 10 минут до бесконечности.




========================================
# Содержимое файла: assistant_general/general_settings.py
========================================

from assistant_general.config import VEGA_PERSONALITY_CORE_ENGLISH, VEGA_PERSONALITY_CORE_RUSSIAN
import logging

#### Для database.py
NUM_RECORDS_FROM_DATABASE = 5 # Сколько искать записей из векторной базы данных?
SIMILARITY_THRESHOLD = 1.05 # Порог схожести для поиска записей в долговременной памяти (0.0 - самые близкие знания, 1.0 - самые далекие)
# 0.80 - Для строгих записей (все записи, которые дальше, чем 0.75 - пропускаются)
# 0.90 - Сбалансированно
# 1.0 - Для записей с некоторыми возможными упущениями

#### Для logger_config.py
LOG_OUTPUT_LEVEL = logging.INFO # Какой уровень сообщений выводить в консоль: DEBUG, INFO, ERROR

#### Для brain.py
MODEL_GEMINI = "gemini-flash-latest" # gemini-2.5-flash or gemini-2.5-flash-lite or gemini-flash-latest or gemini-flash-lite-latest...
SHORT_TERM_MEMORY_PATH = "assistant_brain/short_term_memory.json"
MAX_MEMORY = 26 # Лимит кратковременной памяти
NUM_OF_NEWS_IN_BRIEFING = 8 # Количество новостей с Хабра, которые будут передаваться в утренний брифинг, чтобы Вега нашла самые интересные

VEGA_PERSONALITY_CORE = None

PERSONALITY_CORES = {
    "RUSSIAN": VEGA_PERSONALITY_CORE_RUSSIAN,
    "ENGLISH": VEGA_PERSONALITY_CORE_ENGLISH,
    # В будущем просто добавляем сюда: "GERMAN": VEGA_PERSONALITY_CORE_GERMAN,
}

def choose_language(language):
    global VEGA_PERSONALITY_CORE
    core = PERSONALITY_CORES.get(language) # .get() - безопасный способ получить значение
    if core:
        VEGA_PERSONALITY_CORE = core
        logging.debug(f"VEGA_PERSONALITY_CORE set for {language}")
    else:
        raise ValueError(f"Unsupported language: {language}")
    
####

# Настройки для утреннего брифинга
BRIEFING_START_HOUR = 6  # Брифинг можно проводить не раньше 6 утра




========================================
# Содержимое файла: assistant_general/general_tools.py
========================================

# systemic_skills.py
import json

def read_json(filename: str):
    """Читает JSON-файл. Если файла нет или он поврежден, создает его с содержимым по умолчанию и возвращает его."""
    try:
        with open(filename, "r", encoding="utf-8") as file:
            return json.load(file)
    except (FileNotFoundError, json.JSONDecodeError):
        # Если файла нет или он пустой/битый
        print(f"Файл '{filename}' не найден или поврежден.")
        return {
            "last_briefing_date": "1970-01-01"
        }
    
def write_json(filename: str, data: dict):
    try:
        with open(filename, "w", encoding="utf-8") as file:
            json.dump(data, file, indent=4)
    except Exception as e:
        return f"Error writing to json file: {e}"


========================================
# Содержимое файла: assistant_general/logger_config.py
========================================

# logger_config.py
import logging
import sys
from assistant_general.general_settings import LOG_OUTPUT_LEVEL

# Чтобы инициализировать логгер в других файлах:
# import logging
# from assistant_general.logger_config import setup_logger
# setup_logger()
# logger = logging.getLogger(__name__)
# logger.info("Test")

def setup_logger():
    """Настраивает корневой логгер, от которого наследуются все остальные."""
    
    # Получаем корневой логгер, передав пустое имя
    root_logger = logging.getLogger()
    
    if root_logger.hasHandlers():
        return

    root_logger.setLevel(LOG_OUTPUT_LEVEL)

    # Создаем обработчики
    stdout_handler = logging.StreamHandler(sys.stdout)
    file_handler = logging.FileHandler('VEGA.log', mode='a', encoding='utf-8')

    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - [%(levelname)s] - %(message)s'
    )

    # Применяем форматтер к обработчикам
    stdout_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    # Добавляем обработчики к корневому логгеру
    root_logger.addHandler(stdout_handler)
    root_logger.addHandler(file_handler)




========================================
# Содержимое файла: assistant_input/text_input.py
========================================

# text_input.py
from assistant_event_bus.event_bus import publish
import time

def text_input_loop():
    while True:
        command = input("\nEnter your query into the V.E.G.A. system: \n")
        if command:
            publish("USER_SPEECH", query=command)
            time.sleep(5)


========================================
# Содержимое файла: assistant_input/voice_input.py
========================================

# voice_input.py
import threading
import vosk
import logging
import json
import sounddevice
import queue

print("\n")

class SpeechListener(threading.Thread):
    """Слушает речь пользователя и кладет в очередь"""
    def __init__(self):
        super().__init__()
        self.audio_queue = queue.Queue()
        self.daemon = True

        try: # Инициализация Vosk
            model_path = "vosk_model/vosk-model-small-ru-0.22" # Либо vosk-model-ru-0.42, либо vosk-model-small-ru-0.22
            self.model = vosk.Model(model_path)
            self.recognizer = vosk.KaldiRecognizer(self.model, 16000)
            print("\nThe local speech recognition engine (Vosk) has been initialized successfully.")
        except Exception as e:
            print(f"CRITICAL ERROR: Failed to initialize Vosk: {e}")
            self.recognizer = None

    def _audio_callback(self, indata, frames, time, status):
        """Единственная задача - складывать аудиоданные в очередь"""
        self.audio_queue.put(bytes(indata))

    def run(self):
        """Основной цикл потока-слушателя"""
        if not self.recognizer: # Если Vosk не инициализировался, поток просто завершает работу
            return
        
        else:
            from assistant_event_bus.event_bus import publish
            print("Listening Stream (Vosk): started.\n")
            with sounddevice.RawInputStream(samplerate=16000, blocksize=8000, dtype='int16', channels=1, callback=self._audio_callback): # Открываем аудиопоток с микрофона 
                while True:
                    data = self.audio_queue.get() # Забираем аудиоданные из очереди
                    # "Скармливаем" их распознавателю
                    if self.recognizer.AcceptWaveform(data):
                        result = self.recognizer.Result() # Если распознана полная фраза, получаем результат
                        query = json.loads(result)["text"] # Извлекаем текст

                        # Если текст не пустой, кладем его в очередь команд
                        if query:
                            print(f"[Vosk] Recognized: {query}")
                            publish("USER_SPEECH", query=query)
                            logging.info(f"[Vosk] Recognized: {query}")




========================================
# Содержимое файла: assistant_output/voice_output_eng.py
========================================

# voice_output_eng.py
import queue
import threading
import sounddevice as sd
from kokoro import KPipeline
from assistant_event_bus.event_bus import subscribe

# Kokoro генерирует аудио с частотой 24000 Гц. Это важно указать.
SAMPLE_RATE = 24000

SAMPLE_RATE = 24000

class SpeechModuleENG:
    """
    Класс, отвечающий за синтез и воспроизведение речи.
    Работает асинхронно через очередь и фоновый поток.
    Воспроизводит аудио напрямую из памяти, без временных файлов.
    """
    def __init__(self, lang_code: str = 'b', voice: str = 'bf_lily'):
        try:
            # Проверим, есть ли доступные аудиоустройства
            sd.query_devices()
        except Exception as e:
            print(f"КРИТИЧЕСКАЯ ОШИБКА: Не найдено аудиоустройство вывода. {e}")
            # В реальном приложении здесь можно было бы завершить работу или отключить модуль
            return

        self.pipeline = KPipeline(lang_code=lang_code)
        self.voice = voice
        self.tts_queue = queue.Queue()
        
        subscribe("GEMINI_RESPONSE", self.queue_text_for_synthesis)
        
        self.worker_thread = threading.Thread(target=self._tts_worker, daemon=True)
        print("The speech module has been initialized.")

    def start(self):
        """Запускает фоновый поток для обработки очереди."""
        self.worker_thread.start()

    def _synthesize_and_play(self, text: str):
        """
        Генерирует речь и воспроизводит аудио-сегменты напрямую.
        """
        try:
            generator = self.pipeline(text, voice=self.voice)
            for _, _, audio_chunk in generator:
                # audio_chunk - это и есть NumPy массив, который нам нужен
                sd.play(audio_chunk, SAMPLE_RATE)
                sd.wait()  # Ждем, пока текущий кусок аудио доиграет
        except Exception as e:
            print(f"Ошибка во время синтеза или воспроизведения речи: {e}")

    def _tts_worker(self):
        """
        Работает в фоновом потоке, берет текст из очереди и озвучивает его.
        """
        while True:
            try:
                text_to_speak = self.tts_queue.get()
                if text_to_speak is None:
                    break
                
                self._synthesize_and_play(text_to_speak)
                
                self.tts_queue.task_done()
            except Exception as e:
                print(f"Критическая ошибка в потоке озвучивания: {e}")

    def queue_text_for_synthesis(self, **kwargs):
        """
        Публичный метод, который вызывается по событию.
        Добавляет текст в очередь на озвучивание.
        """
        text = kwargs.get('text')
        if text and isinstance(text, str):
            self.tts_queue.put(text)


========================================
# Содержимое файла: assistant_output/voice_output_rus.py
========================================

# voice_output_rus.py

import threading
import queue
import sounddevice as sd
import soundfile as sf
from edge_tts import Communicate
from assistant_event_bus.event_bus import subscribe
import os 

class SpeechModuleRUS:
    def __init__(self):
        self.worker_thread = threading.Thread(target=self._tts_worker, daemon=True)
        self.tts_queue = queue.Queue()
        
        self.temp_folder = "assistant_temporary_files"
        os.makedirs(self.temp_folder, exist_ok=True)
        
        subscribe("GEMINI_RESPONSE", self.queue_text_for_synthesis)
        print("The speech module has been initialized.")

    def synth(self, text: str, voice: str = "ru-RU-SvetlanaNeural"):
        """Синтезирует text, сохраняет его во временный файл и воспроизводит."""
        
        output_filename = "vega_speech.mp3"
        full_path = os.path.join(self.temp_folder, output_filename)
        
        communicate = Communicate(text, voice)
        
        def save_and_play():
            # Используем полный путь ---
            communicate.save_sync(full_path)
            data, samplerate = sf.read(full_path)
            sd.play(data, samplerate)
            sd.wait()
        
        save_and_play()

    def start(self):
        """Запускает фоновый поток для обработки очереди."""
        self.worker_thread.start()

    def _tts_worker(self):
        """
        Работает в фоновом потоке, берет текст из очереди и озвучивает его.
        """
        while True:
            try:
                text_to_speak = self.tts_queue.get()
                if text_to_speak is None:
                    break
                self.synth(text=text_to_speak, voice="ru-RU-SvetlanaNeural")
                self.tts_queue.task_done()

            except Exception as e:
                print(f"Критическая ошибка в потоке озвучивания: {e}")

    def queue_text_for_synthesis(self, **kwargs):
        """
        Публичный метод, который вызывается по событию.
        Добавляет текст в очередь на озвучивание.
        """
        text = kwargs.get('text')
        if text and isinstance(text, str):
            self.tts_queue.put(text)


========================================
# Содержимое файла: assistant_tools/music_skills.py
========================================

# music_skills.py
import subprocess
import os
import random
from fuzzywuzzy import process
import logging
from assistant_general.logger_config import setup_logger
from dotenv import load_dotenv

load_dotenv()
setup_logger()
logger = logging.getLogger(__name__)

FOOBAR_PATH = os.getenv("FOOBAR_PATH")
MUSIC_LIBRARY_PATH = os.getenv("MUSIC_LIBRARY_PATH")
SILENT_TRACK_PATH = os.getenv("SILENT_TRACK_PATH")

# ЧТОБЫ СОЗДАВАЛСЯ НОВЫЙ ПЛЕЙЛИСТ В КОДЕ, НАДО:
# НАПИСАТЬ СНАЧАЛА success = _send_foobar_command(['/add', random_track_path]), А УЖЕ ПОТОМ
# success = _send_foobar_command(['/play', playlist_path])
# ТАК КАК ЕСТЬ НАПИСАТЬ success = _send_foobar_command(['/add', random_track_path '/play',]) ВМЕСТЕ,
# ПЛЕЙЛИСТ НЕ СОЗДАСТСЯ

setup_logger()
logger = logging.getLogger(__name__)


def _send_foobar_command(command_args):
    """Системная команда для других функций, управляет Foobar2000."""
    try:
        full_command = [FOOBAR_PATH] + command_args
        subprocess.Popen(full_command)
        return True
    except FileNotFoundError:
        logger.error("File not found.")
        return False
    except Exception as e:
        print(f"ERROR while executing Foobar2000 command: {e}")
        return False

def _current_tracks():
    """Проходит по всем файлам в музыкальной библиотеке и возвращает список путей ко всем трекам."""
    all_tracks_list = []

    for root, dirs, files in os.walk(MUSIC_LIBRARY_PATH): # Рекурсивно обходим всю музыкальную библиотеку
        for filename in files: # Проходимся по файлам

            full_path = os.path.join(root, filename)
            
            # Добавляем найденный полный путь в наш список
            all_tracks_list.append(full_path)
    return all_tracks_list

ALL_TRACKS_CACHE = _current_tracks() # Получает все файлы из музыкальной библиотеки и пути к ним

def _find_best_track_path(query: str, all_tracks_paths: list, score_cutoff=80):
    """Ищет наиболее похожее название трека в кеше и возвращает ПОЛНЫЙ ПУТЬ."""
    track_map = {os.path.splitext(os.path.basename(path))[0]: path for path in all_tracks_paths}
    best_match = process.extractOne(query, track_map.keys())
    if best_match and best_match[1] >= score_cutoff:
        return track_map[best_match[0]]
    return None

def music_play_track(track_name: str = None, artist_name: str = None):
    """Ищет наиболее похожий трек и воспроизводит его."""
    if not track_name and not artist_name:
        return "Must be specify the track title or artist name."

    # Собираем единый поисковый запрос
    search_query = f"{artist_name or ''} {track_name or ''}".strip()
    
    found_path = _find_best_track_path(search_query, ALL_TRACKS_CACHE)

    if found_path:
        _send_foobar_command(['/add', found_path])
        _send_foobar_command(['/play', found_path])
        clean_name = os.path.splitext(os.path.basename(found_path))[0]
        return f"Play: {clean_name}"
    else:
        return f"Track similar to '{search_query}' not found in the library."
    
def music_play_playlist(playlist_name: str):
    """Ищет папку по имени, очищает старый плейлист, добавляет все треки из папки и начинает играть."""
    if not playlist_name:
        return "Must specify a playlist name."

    playlist_path = None
    try:
        with os.scandir(MUSIC_LIBRARY_PATH) as entries:
            for entry in entries:
                if entry.is_dir() and playlist_name.lower() in entry.name.lower():
                    playlist_path = entry.path
                    print(f"Playlist found: {playlist_path}")
                    break
    except FileNotFoundError:
        logger.error("Error: Music library folder not found.")
        return "Error: Music library folder not found."

    if playlist_path:
        try:
            track_count = sum(1 for f in os.listdir(playlist_path) if f.lower().endswith(('.mp3', '.flac', '.wav', '.ogg', '.m4a')))
            if track_count == 0:
                return f"Плейлист '{playlist_name}' найден, но он пуст."
        except Exception as e:
            logger.error(f"Не удалось прочитать содержимое плейлиста '{playlist_name}': {e}")
            return f"Не удалось прочитать содержимое плейлиста '{playlist_name}': {e}"

        success = _send_foobar_command(['/add', playlist_path])
        success = _send_foobar_command(['/play', playlist_path])
        
        if success:
            return f"Включаю плейлист '{playlist_name}'. Найдено треков: {track_count}."
        else:
            return "Не удалось запустить воспроизведение плейлиста."
    else:
        return f"Плейлист '{playlist_name}' не найден."
    
def music_play_random():
    """Выбирает случайный трек из всей музыкальной библиотеки и включает его."""
    if not ALL_TRACKS_CACHE:
        return "Music library is empty. There's nothing to play."
        
    # Выбираем случайный полный путь к файлу из кеша
    random_track_path = random.choice(ALL_TRACKS_CACHE)
    
    clean_name = os.path.splitext(os.path.basename(random_track_path))[0]

    success = _send_foobar_command(['/add', random_track_path])
    success = _send_foobar_command(['/play', random_track_path])
    
    if success:
        return f"Random track played: {clean_name}"
    else:
        return "Failed to start playing random track."

def music_play_random_album():
    """Находит все папки (альбомы/плейлисты) в музыкальной библиотеке, выбирает одну случайную и воспроизводит её."""
    try:
        # Получаем список всех записей в директории и фильтруем, оставляя только папки
        all_playlists = [entry.path for entry in os.scandir(MUSIC_LIBRARY_PATH) if entry.is_dir()]
    except FileNotFoundError:
        logger.error("Error: Music library folder not found.")
        return "Error: Music library folder not found."

    if not all_playlists:
        logger.info("No ready-made playlists (folders) were found in the music library.")
        return "No ready-made playlists (folders) were found in the music library."

    random_playlist_path = random.choice(all_playlists) # Выбираем случайный путь к плейлисту из списка
    playlist_name = os.path.basename(random_playlist_path) # Получаем чистое имя для красивого ответа
    
    _send_foobar_command(['/add', random_playlist_path])
    success = _send_foobar_command(['/play', random_playlist_path])
    
    if success:
        logger.debug(f"Random playlist enabled: '{playlist_name}'.")
        return f"Random playlist enabled: '{playlist_name}'."
    else:
        logger.error("Failed to start playing random playlist.")
        return "Failed to start playing random playlist."

def music_pause_playback():
    """Ставит текущий трек на паузу."""
    success = _send_foobar_command(['/pause'])
    return "Playback is paused." if success else "Failed to pause."

def music_resume_playback():
    """Снимает воспроизведение с паузы."""
    success = _send_foobar_command(['/play'])
    return "Playback resumed." if success else "Failed to resume."

def music_play_next_track():
    """Включает следующий трек в плейлисте."""
    success = _send_foobar_command(['/next'])
    return "Next track is on." if success else "Failed to change track."

def music_play_previous_track():
    """Включает предыдущий трек в плейлисте."""
    success = _send_foobar_command(['/prev'])
    return "Previous track is on." if success else "Failed to change track."

def music_clear_playlist():
    """Очищает текущий плейлист, заменяя его одним треком с тишиной и останавливая воспроизведение. 
    Единственный надежный способ эмулировать команду 'clear'."""

    # Проверка, что наш инструмент на месте
    if not os.path.exists(SILENT_TRACK_PATH):
        msg = f"ERROR: Cleanup file '{SILENT_TRACK_PATH}' not found."
        print(msg)
        return msg
        
    # Главная команда: Остановить -> Заменить плейлист на "пустышку"
    success = _send_foobar_command(['/add', SILENT_TRACK_PATH])
    success = _send_foobar_command(['/play', SILENT_TRACK_PATH])
    
    if success:
        return "Playlist cleared."
    else:
        return "Failed to clear playlist."


if __name__ == "__main__":
    # Тесты
    import time
    music_play_track("horizon")
    time.sleep(7)
    music_play_track("мозговой протез")
    time.sleep(7)
    music_play_playlist("slipknot")
    time.sleep(7)
    music_play_next_track()
    music_play_next_track()
    time.sleep(7)
    music_pause_playback()



========================================
# Содержимое файла: assistant_tools/music_skills_diagrams.py
========================================

# music_skills.py
# Схемы для простых музыкальных команд (без параметров)
music_play_random_scheme = {
    "name": "music_play_random",
    "description": "Plays a random track from the user's entire music library. Use for general queries like 'play something', 'any music'.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_pause_playback_scheme = {
    "name": "music_pause_playback",
    "description": "Pauses the currently playing music. If nothing is playing, does nothing.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_resume_playback_scheme = {
    "name": "music_resume_playback",
    "description": "Resumes music playback if it was paused.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_play_next_track_scheme = {
    "name": "music_play_next_track",
    "description": "Switches to the next track in the current playlist.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_play_previous_track_scheme = {
    "name": "music_play_previous_track",
    "description": "Switches to the previous track in the current playlist.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

music_clear_playlist_scheme = {
    "name": "music_clear_playlist",
    "description": "Clears the current playlist (play queue) completely.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

# Схемы для команд с параметрами 
music_play_playlist_scheme = {
    "name": "music_play_playlist",
    "description": "Finds a playlist (a folder containing music) by name and plays all tracks from it, replacing the current queue. Use when the user requests to play an artist, album, or a specific playlist.",
    "parameters": {
        "type": "object",
        "properties": {
            "playlist_name": {
                "type": "string",
                "description": "The name of the playlist, artist, or album. For example: 'Slayer', 'Daft Punk', 'My Workout Playlist'",
            },
        },
        "required": ["playlist_name"],
    },
}

music_play_track_scheme = {
    "name": "music_play_track",
    "description": (
        "Finds and plays a track from the user's local music library. "
        "The library contains artists such as Slipknot, Slayer, ACDC and composers such as Pawel Blaszczak. "
        "The function uses fuzzy search, so it can correct typos in the query. "
        "Use when the user requests to play a specific song."
    ),
    "parameters": {
        "type": "object",
        "properties": {
            "track_name": {
                "type": "string",
                "description": "Song title. May contain typos. For example: 'caster', 'Psichosal'",
            },
            "artist_name": {
                "type": "string",
                "description": "The artist's name. This may not be specified. For example: 'Slipknot'",
            },
        },
    },
}

music_play_random_album_scheme = {
    "name": "music_play_random_album",
    "description": "Includes a random existing playlist.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}



========================================
# Содержимое файла: assistant_tools/skills.py
========================================

# skills.py
import threading
import webbrowser
import requests
import datetime
import pyautogui  
import os
from dotenv import load_dotenv
import ctypes
import platform
import pyperclip
import pygetwindow as gw
import psutil
import keyboard
import logging
from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
import pythoncom
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
from assistant_general.logger_config import setup_logger
from assistant_vector_database.database import add_new_memory
from bs4 import BeautifulSoup
import wmi
from PIL import Image 
from pyrogram import Client

setup_logger()
logger = logging.getLogger(__name__)

load_dotenv() # для загрузки API ключей из .env
OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY")
WEATHER_CITY_LAT = os.getenv("WEATHER_CITY_LAT")
WEATHER_CITY_LON = os.getenv("WEATHER_CITY_LON")
OPENHARDWAREMONITOR_PATH = os.getenv("OPENHARDWAREMONITOR_PATH")

MONTHS = ("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December")

# ДЛЯ РЕГИСТРАЦИИ НОВЫХ НАВЫКОВ В ВЕГУ НУЖНО:
# Написать json схему в skills_diagrams.py
# Перейти в assistant_brain.added_skills.py и следовать инструкциям, которые описаны в файле файла

def get_weather(city_name: str = None):
    """Получает текущую погоду"""
    if city_name:
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city_name}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ru"
    else: # Если город не передан, узнаем по умолчанию в Липецке
        url = f"https://api.openweathermap.org/data/2.5/weather?lat={WEATHER_CITY_LAT}&lon={WEATHER_CITY_LON}&appid={OPENWEATHER_API_KEY}&units=metric&lang=ru"

    response = requests.get(url)
    weather_data = response.json()
    weather_description = weather_data["weather"][0]["description"] # Например, "пасмурно"
    description_of_feeling_temp = int(weather_data["main"]["feels_like"])
    description_of_temp = int(weather_data["main"]["temp"])
    humidity = int(weather_data["main"]["humidity"]) # Влажность, например, 36

    wind = weather_data["wind"]["speed"] # Скорость ветра, например, 4.64
    sity_name = weather_data["name"]

    final_answer = f"City: {sity_name}; Weather description: {weather_description}; Feels like: {description_of_feeling_temp}°; Actual temperature: {description_of_temp}°; Air humidity: {humidity}; Wind: {wind} m/s."
    logger.debug(final_answer)
    return final_answer

def search_in_google(search_query: str) -> str:
    """Ищет переданный запрос в поисковике и открывает вкладку браузера."""
    if not search_query:
        logger.error("Error: A search query is required to search.")
        return "Error: A search query is required to search."
    webbrowser.open(f"https://yandex.ru/search/?text={search_query}") #Альтернативно https://www.google.com/search?q=
    logger.debug(f"The search page for the query is open: '{search_query}'.")
    return f"The search page for the query is open: '{search_query}'."

def get_time(**kwargs) -> str:
    """Возвращает текущее время в формате ЧЧ:ММ."""
    now = datetime.datetime.now()
    return f"Current time: {now.strftime('%H:%M')}."

def get_date() -> str:
    """Возвращает сегодняшнюю дату."""
    now = datetime.datetime.now()
    return f"Today {now.day} {MONTHS[now.month - 1]}."

def make_screenshot():
    """Делает скриншот и сохраняет его в папку 'assistant_temporary_files'. Папка создается автоматически, если ее нет."""
    # Определяем имя папки и имя файла
    temp_folder = "assistant_temporary_files"
    filename = "screenshot.png"
    
    full_path = os.path.join(temp_folder, filename) # os.path.join() - правильный способ соединять пути
    try:
        # Проверяем, существует ли папка, и создаем ее, если нет
        os.makedirs(temp_folder, exist_ok=True) # exist_ok=True означает, что ошибки не будет, если папка уже существует
        
        # 4. Делаем скриншот и сохраняем его сразу по полному пути
        screenshot = pyautogui.screenshot(full_path)
        screenshot.save(full_path)
        
        return {"status": "success", "file_path": os.path.abspath(full_path)} # Возвращаем абсолютный путь
    
    except Exception as e:
        logger.error(f"Failed to create screenshot: {e}") 
        return {"status": "error", "message": str(e)}
    
def get_screenshot_context():
    """Делает скриншот и возвращает объект Image, либо None в случае ошибки."""
    try:
        screenshot_info = make_screenshot()
        if screenshot_info['status'] == 'success':
            screenshot_path = screenshot_info['file_path']
            img = Image.open(screenshot_path)
            logger.info(f"Screenshot taken: {screenshot_path}")
            return img # Возвращаем само изображение
        else:
            logger.error(f"Failed to take screenshot: {screenshot_info['message']}")
            return None 
    except Exception as e:
        logger.error(f"Error creating or opening screenshot: {e}")
        return None 
    
def save_to_memory(text):
    """Сохраняет в память любой факт о пользователе."""
    add_new_memory(text)
    logger.debug(f"Record '{text}'save to memory.")
    return "Record save to memory."

def lock_pc():
    """Блокирует рабочую станцию Windows."""
    if platform.system() == "Windows":
        try:
            ctypes.windll.user32.LockWorkStation()
            return "The workstation is locked"
        except Exception as e:
            logger.error(f"Unable to lock workstation. Error: {e}")
            return f"Unable to lock workstation. Error: {e}"
    else:
        # Если Вега запустится на Linux или macOS в будущем
        logger.debug("The command only works on the Windows operating system.")
        return "The command only works on the Windows operating system."
    
def get_system_volume() -> str: # Возвращаемый тип изменен на str, как у вас в коде
    """Возвращает текущую системную громкость в процентах (от 0 до 100)."""
    # Инициализируем COM для текущего потока
    pythoncom.CoInitialize()
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(
            IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        current_volume_scalar = volume_control.GetMasterVolumeLevelScalar()
        current_volume_percent = int(current_volume_scalar * 100)

        print(f"Current volume: {current_volume_percent}%")
        return f"Current volume: {current_volume_percent}%" # Лучше возвращать с % для ясности
    except Exception as e:
        print(f"Ошибка при получении информации о текущей громкости: {e}")
        return f"Ошибка при получении информации о текущей громкости: {e}"
    finally:
        # Обязательно деинициализируем COM перед выходом из потока/функции
        pythoncom.CoUninitialize()

def set_system_volume(level_volume: int) -> str: # Возвращаемый тип изменен на str
    """Принимает число от 0 до 100 и выставляет такую системную громкость."""
    if not 0 <= level_volume <= 100:
        return f"Громкость должна быть между 0 и 100, а не {level_volume}"

    # Инициализируем COM для текущего потока
    pythoncom.CoInitialize()
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(
            IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        target_volume_scalar = level_volume / 100.0
        volume_control.SetMasterVolumeLevelScalar(target_volume_scalar, None)
        
        print(f"Volume changed to {level_volume}%.")
        return f"Громкость изменена на {level_volume}%."
    except Exception as e:
        print(f"Error when changing volume: {e}")
        return f"Ошибка при изменении громкости: {e}"
    finally:
        # Обязательно деинициализируем COM
        pythoncom.CoUninitialize()

def decrease_volume(amount: int = 10):
    """Уменьшает системную громкость на указанное значение в процентах. Возвращает новую громкость в процентах."""
    pythoncom.CoInitialize()
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        current_volume_scalar = volume_control.GetMasterVolumeLevelScalar() # Получаем текущую громкость
        
        decrease_scalar = amount / 100.0 # Правильно вычисляем целевую громкость, 10 превратится в 0.1
        target_volume_scalar = max(0.0, current_volume_scalar - decrease_scalar) # Текущая - указанная
        
        volume_control.SetMasterVolumeLevelScalar(target_volume_scalar, None) # Устанавливаем новую громкость
        
        new_volume_percent = int(target_volume_scalar * 100)
        return f"Volume successfully decreased to {new_volume_percent}%."

    except Exception as e:
        return f"Error when changing volume: {e}"   
    finally:
        pythoncom.CoUninitialize()

def increase_volume(amount: int = 10):
    """Увеличивает системную громкость на указанное значение в процентах. Возвращает новую громкость в процентах."""
    pythoncom.CoInitialize()
    try:
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        volume_control = cast(interface, POINTER(IAudioEndpointVolume))
        
        current_volume_scalar = volume_control.GetMasterVolumeLevelScalar() # Получаем текущую громкость
        increase_scalar = amount / 100.0 # Правильно вычисляем целевую громкость, 10 превратится в 0.1

        target_volume_scalar = min(1.0, current_volume_scalar + increase_scalar) # Текущая + указанная
        
        volume_control.SetMasterVolumeLevelScalar(target_volume_scalar, None) # Устанавливаем новую громкость
        
        new_volume_percent = int(target_volume_scalar * 100)
        return f"Volume successfully increased to {new_volume_percent}%."

    except Exception as e:
        return f"Error when changing volume: {e}"   
    finally:
        pythoncom.CoUninitialize()

def get_habr_news(limit=10):
    """Получает топ статей с Habr.com."""
    url = 'https://habr.com/ru/all/'  # Главная страница с новыми статьями
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }  # Имитация браузера, чтобы избежать блокировок
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Проверка на HTTP-ошибки
        
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.find_all('article', class_='tm-articles-list__item')[:limit] # Поиск контейнеров статей (класс 'tm-articles-list__item')
        
        result = []
        for article in articles:
            # Заголовок и ссылка
            title_elem = article.find('a', class_='tm-title__link')
            title = title_elem.text.strip() if title_elem else 'N/A'
            link = 'https://habr.com' + title_elem['href'] if title_elem else 'N/A'
            
            # Краткое описание
            summary_elem = article.find('div', class_='article-formatted-body')
            summary = summary_elem.text.strip()[:200] + ' (text truncated for brevity)...' if summary_elem else 'N/A'
            
            result.append({
                'title': title,
                'link': link,
                'summary': summary
            })

        # for i, art in enumerate(result, 1):
        #     print(f"{i}. {art['title']}\n   Ссылка: {art['link']}\n   Кратко: {art['summary']}\n") # ДЛЯ ОТЛАДКИ

        return result
    
    except requests.RequestException as e:
        logger.error(f"Error requesting page: {e}")
        return []
    except Exception as e:
        logger.error(f"Parsing error: {e}")
        return []

def get_system_metrics():
    """Возвращает текущую нагрузку процессора, видеокарты и оперативной памяти один раз. 
    Требует открытого OpenHardwareMonitor."""
    try:
        w = wmi.WMI(namespace="root\OpenHardwareMonitor")
        sensors = w.Sensor()
        if not sensors:
            logger.info("No sensors are available. OpenHardwareMonitor may not be running.")
            return "No sensors are available. OpenHardwareMonitor may not be running. Needs to be launched."

        cpu_temp = None
        cpu_load = None
        gpu_temp = None
        gpu_load = None
        ram_load = None

        # Ищем нужные сенсоры
        for sensor in sensors:
            if sensor.SensorType == "Temperature" and sensor.Name == "Temperature":
                cpu_temp = sensor.Value
            elif sensor.SensorType == "Load" and sensor.Name == "CPU Total":
                cpu_load = sensor.Value
            elif sensor.SensorType == "Temperature" and sensor.Name == "GPU Core":
                gpu_temp = sensor.Value
            elif sensor.SensorType == "Load" and sensor.Name == "GPU Core":
                gpu_load = sensor.Value
            elif sensor.SensorType == "Load" and sensor.Name == "Memory":
                ram_load = sensor.Value

        # Форматируем значения
        cpu_temp = f"{cpu_temp:.1f}°C" if cpu_temp is not None else "Недоступно"
        cpu_load = f"{cpu_load:.1f}%" if cpu_load is not None else "Недоступно"
        gpu_temp = f"{gpu_temp:.1f}°C" if gpu_temp is not None else "Недоступно"
        gpu_load = f"{gpu_load:.1f}%" if gpu_load is not None else "Недоступно"
        ram_load = f"{ram_load:.1f}%" if ram_load is not None else "Недоступно"
        now = datetime.now()

        # Вывод в одну строку
        output = (f"Readings from the main PC sensors ({now.strftime('%H:%M:%S')}): \nCPU: {cpu_temp}, {cpu_load}; \nGPU: {gpu_temp}, {gpu_load}; \nRAM: {ram_load}")
        
        return output
    
    except Exception as e:
        logger.error(f"Error: {str(e)}. Make sure OpenHardwareMonitor is running.")
        return f"Error: {str(e)}. Make sure OpenHardwareMonitor is running."
    
# УПРАВЛЕНИЕ ПК, МЫШЬ, КЛАВИАТУРА 

def get_windows_layout():
    """
    Возвращает текущую раскладку клавиатуры в Windows.
    Возвращает строку вроде "ENG","RUS" и прочее.
    """
    if platform.system() != "Windows":
        return "Not a Windows system"

    # Словарь популярных раскладок. Полный список можно найти по запросу "Windows Language Code Identifier"
    layouts = {
        0x409: "ENG", 0x419: "RUS", 0x407: "GER",
        0x40C: "FRA", 0x410: "ITA", 0x411: "JPN", 
        0x412: "KOR", 0x804: "CHN" 
    }

    # Загружаем библиотеку user32.dll
    user32 = ctypes.WinDLL('user32', use_last_error=True)
    hwnd = user32.GetForegroundWindow()
    thread_id = user32.GetWindowThreadProcessId(hwnd, None)
    layout_id = user32.GetKeyboardLayout(thread_id)
    language_id = layout_id & 0xFFFF

    logger.debug(layouts.get(language_id, f"Unknown layout (ID: {hex(language_id)})"))
    return layouts.get(language_id, f"Unknown layout (ID: {hex(language_id)})")

def move_mouse(x, y):
    "Двигает мышь в указанном направлении."
    pyautogui.moveTo(x, y, duration=0.05)
    logger.debug(f"The mouse is moved to coordinates: {x}, {y}")
    return f"The mouse is moved to coordinates: {x}, {y}"

def current_mouse_coordinates():
    "Определяет текущие координаты мыши."
    current_position = pyautogui.position()
    logger.debug(f"The mouse is currently at X={current_position.x}, Y={current_position.y}")
    return f"The mouse is currently at X={current_position.x}, Y={current_position.y}"

def click_mouse(button='left', clicks=1, interval=0.1):
    """Кликнуть мышью. Левой, правой, одинарный, двойной - на выбор."""
    pyautogui.click(button=button, clicks=clicks, interval=interval)
    logger.debug(f"Performed {clicks} click(s) with the {button} mouse button.")
    return f"Performed {clicks} click(s) with the {button} mouse button."

def scroll_mouse(amount):
    """Скроллит вверх (положительное число) или вниз (отрицательное)."""
    pyautogui.scroll(amount)
    direction = "up" if amount > 0 else "down"
    logger.debug(f"Scrolled {abs(amount)} units {direction}.")
    return f"Scrolled {abs(amount)} units {direction}."

def drag_mouse(x_to, y_to, duration=0.5):
    """Тащит мышь из текущей позиции в точку (x, y), как будто что-то выделяет."""
    pyautogui.dragTo(x_to, y_to, duration=duration)
    logger.debug(f"Dragged mouse to {x_to}, {y_to}.")
    return f"Dragged mouse to {x_to}, {y_to}."
    
def press_hotkey(keys):
    """Нажимает любое количество горячих клавиш. Например: ('ctrl', 'shift', 'esc')"""
    pyautogui.hotkey(*keys)
    logger.debug(f"Hotkey pressed: {' + '.join(keys)}")
    return f"Hotkey pressed: {' + '.join(keys)}"

def copy_to_clipboard(text):
    """Копирует любой текст в буфен обмена."""
    pyperclip.copy(text)
    logger.debug(f"Text '{text}' copied to clipboard.")
    return f"Text '{text}' copied to clipboard."

def write_text(text, attempts=0):
    """Печатает любой текст, даже на эльфийском."""
    keyboard.write(text)

def system_command(command):
    """Выполняет системные команды. Выключение, перезагрузка. ОПАСНО."""
    # Примеры команд для Windows:
    # 'shutdown /s /t 1' - выключить пк через 1 секунду, 'shutdown /r /t 1' - перезагрузить пк через 1 секунду, 'rundll32.exe powrprof.dll,SetSuspendState 0,1,0' - отправить в спящий режим
    os.system(command)
    logger.info(f"Executing system command: {command}.")
    return f"Executing system command: {command}."

# --- ОКНА, ПРОГРАММЫ, ПРИЛОЖЕНИЯ ----

def get_processes():
    """Сканирует систему и возвращает только полезный список процессов, отфильтровав все ненужные/системные."""
    # Сюда кладем все системные процессы, которые нам нужны
    system_processes_blacklist = [
        "svchost.exe", "lsass.exe", "csrss.exe", "wininit.exe", "services.exe", "winlogon.exe", "dwm.exe", "spoolsv.exe",
        "explorer.exe", "rundll32.exe", "ctfmon.exe", "fontdrvhost.exe", "conhost.exe", "sihost.exe", "taskhostw.exe", "RuntimeBroker.exe",
        "ApplicationFrameHost.exe", "SearchHost.exe", "ShellExperienceHost.exe", "StartMenuExperienceHost.exe", "SystemSettings.exe", "backgroundTaskHost.exe",
        "unsecapp.exe", "System", "System Idle Process", "SecurityHealthSystray.exe", "nvcontainer.exe", "steamwebhelper.exe", "lghub_agent.exe", 
        "msedgewebview2.exe", "OneDrive.Sync.Service.exe", "CrossDeviceResume.exe", "LockApp.exe", "ShellHost.exe", "UserOOBEBroker.exe",
        "WebViewHost.exe", "WidgetService.exe", "Widgets.exe", "XboxPcApp.exe", "XboxPcAppFT.exe", "XboxPcTray.exe", "lghub_system_tray.exe",
        "ruff.exe", "winws.exe"
    ]
    filtered_list = []

    # Получаем имя текущего пользователя, чтобы отсеять процессы других юзеров
    current_user = os.getlogin()

    for process in psutil.process_iter(['pid', 'name', 'username']):
        try:
            proc_info = process.info
            proc_name = proc_info['name']
            proc_user = proc_info['username']

            if not proc_user: # Отсеиваем процессы, у которых нет имени пользователя (обычно это системные)
                continue
            if current_user not in proc_user and 'SYSTEM' not in proc_user: # Оставляем только процессы текущего пользователя
                continue
            if proc_name.lower() in [p.lower() for p in system_processes_blacklist]: # Отсеиваем всё из нашего чёрного списка (без учёта регистра)
                continue

            # Если процесс прошёл все круги ада, добавляем его в список
            filtered_list.append(proc_name)

        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass
    
    # Возвращаем уникальный список, чтобы не было дубликатов
    # (например, 10 процессов chrome.exe превратятся в один)
    logger.debug(f"Current processes in system: {sorted(list(set(filtered_list)))}")
    return f"Current processes in system: {sorted(list(set(filtered_list)))}"

def currently_open_windows():
    """Возвращает текущие окна, которые открыты."""
    titles = []
    all_titles = gw.getAllTitles()
    for title in all_titles:
        if title: # Игнорируем пустые заголовки
            titles.append(title)
    return titles
    




# =========================================================================
# 1. КОНФИГУРАЦИЯ (ОБЯЗАТЕЛЬНО ИЗМЕНИТЕ ЭТИ ЗНАЧЕНИЯ)
# =========================================================================

# Получите эти данные на https://my.telegram.org/auth (API development tools)
API_ID = 1234567  # <-- ВАШ API_ID
API_HASH = "YOUR_API_HASH_HERE"  # <-- ВАШ API_HASH

# Имя сессии. Можно выбрать любое. Pyrogram создаст файл сессии (например, my_session.session)
SESSION_NAME = "my_telegram_session"

# Канал, данные которого мы хотим получить (используйте @username или ID)
CHANNEL_USERNAME = "@telegram" # <-- ИЗМЕНИТЕ НА НУЖНЫЙ КАНАЛ

# =========================================================================
# 2. ФУНКЦИЯ ДЛЯ ПОЛУЧЕНИЯ ДАННЫХ
# =========================================================================

def get_channel_data(client: Client, channel_username: str):
    """Получает факты о канале, количество подписчиков и 5 последних постов."""
    logging.info(f"-> Запрос данных для канала: {channel_username}")

    # Получение основной информации о чате (Channel Facts + Subscribers)
    try:
        chat = client.get_chat(channel_username)
    except Exception as e:
        print(f"Ошибка при получении информации о чате: {e}")
        return None

    # Сбор фактов и подписчиков
    channel_info = {
        "название": chat.title,
        "описание": chat.description if chat.description else "Нет описания",
        "подписчики": chat.members_count,
        "последние_посты": []
    }

    logging.info(f"-> Получено название: {channel_info['название']} | Подписчиков: {channel_info['подписчики']}")

    # Получение последних 5 постов
    history = client.get_chat_history(channel_username, limit=5)
    
    # history — это асинхронный итератор, мы перебираем его
    for message in history:
        post_data = {
            "id": message.id,
            "дата": message.date.strftime("%Y-%m-%d %H:%M:%S"),
            "текст": message.text.strip() if message.text else "[Медиа или другой контент]" # Берем текст. Если текста нет (это медиа), ставим заглушку
        }
        channel_info["последние_посты"].append(post_data)

    logging.info("-> 5 последних постов успешно получены.")
    
    return channel_info

    
def main():
    """Основная асинхронная функция для инициализации клиента."""
    with Client(SESSION_NAME, API_ID, API_HASH) as client: # 'async with' гарантирует корректный запуск и остановку клиента.
        
        # Вызываем нашу основную функцию
        data = threading.Thread(target=get_channel_data, args=(client, CHANNEL_USERNAME))
        data.start()
        data.join()

        if data:
            print("\n" + "="*50)
            print("РЕЗУЛЬТАТ:")
            print(f"Название: {data['название']}")
            print(f"Подписчики: {data['подписчики']}")
            print(f"Описание: {data['описание']}")
            print("-" * 50)
            print("ПОСЛЕДНИЕ 5 ПОСТОВ:")
            for i, post in enumerate(data['последние_посты']):
                print(f"  {i+1}. ID: {post['id']} | Дата: {post['дата']}")
                # Выводим первые 80 символов текста
                print(f"     Текст: {post['текст'][:80]}...")
            print("="*50)

def all_sensors_and_information():
    """Возвращает все текущие данные: погода, новости, время и дата, логи внутренних диалогов, контекст экрана."""
    # Научить парсить Вегу мой тг канал (например, сначала хотя бы подписчики и первые 3-5 постов: может быть, перевести телеграм в браузер и дать Веге доступ просматривать его)
    # Также сделать так, чтобы из памяти выводились все факты по поводу этого канала (для )
    return {"current_time": get_time(), 
            "current_date": get_date(), 
            "news_from_habr": get_habr_news(), 
            "current_processes_in_pc": get_processes(), 
            "currently_open_windows": currently_open_windows(),
            "current_volume": get_system_volume(), 
            "now_on_screen": get_screenshot_context()
        } # get_screenshot_context() # Возвращает объект Image из Pillow


if __name__ == "__main__":
    # Запуск асинхронного цикла.
    print("Запуск клиента Telegram...")
    
    # ПЕРВЫЙ ЗАПУСК: 
    # Клиент попросит вас ввести номер телефона, код и (возможно) 2FA-пароль. 
    # После этого будет создан файл сессии, и последующие запуски будут мгновенными.
    main()



========================================
# Содержимое файла: assistant_tools/skills_diagrams.py
========================================

# skills_diagrams.py
get_weather_scheme = {
    "name": "get_weather", # ВАЖНО: ИМЯ ДОЛЖНО СОВПАДАТЬ С ФУНКЦИЕЙ PYTHON
    "description": "Find the current weather in the specified city. This is necessary to answer weather questions with up-to-date data. If no city is specified, Lipetsk is the default location.",
    "parameters": {
        "type": "object",
        "properties": {
            "city_name": {
                "type": "string",
                "description": "The city for which the weather is needed. For example: Moscow.",
            },
        },
    },
}

search_in_google_scheme = {
    "name": "search_in_google",
    "description": "Searches for the given query in a search engine and opens a browser tab. Use this if you need to Google something or open a tab.",
    "parameters": {
        "type": "object",
        "properties": {
            "search_query": {
                "type": "string",
                "description": "Search query. For example: Who is Elon Musk",
            },
        },
    },
}

get_time_scheme = {
    "name": "get_time",
    "description": "Gets the current actual time.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

get_date_scheme = {
    "name": "get_date",
    "description": "Gets the current actual date.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}


make_screenshot_scheme = {
    "name": "make_screenshot",
    "description": "Takes a screenshot of the user's home screen and saves it to a file. Returns JSON with the path to the created file.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

save_to_memory_scheme = {
    "name": "save_to_memory",
    "description": "Saves a new fact or piece of information to Vega's long-term memory. Use this when the user provides important new information about themselves, their plans, projects, or asks you to remember something.",
    "parameters": {
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "The specific, concise fact to be saved. For example: 'The user's dog is named Rex.' or 'The user is working on a post-apocalyptic car combat game.'",
            },
        },
        "required": ["text"]
    },
}

lock_pc_scheme = {
    "name": "lock_pc",
    "description": "Locks the user's workstation.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}


get_windows_layout_scheme = {
    "name": "get_windows_layout",
    "description": "Returns the current keyboard layout in Windows. Returns a string such as 'ENG', 'RUS', and so on.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

move_mouse_scheme = {
    "name": "move_mouse",
    "description": "Moves the mouse cursor to the specified X and Y coordinates on the screen.",
    "parameters": {
        "type": "object",
        "properties": {
            "x": {
                "type": "integer",
                "description": "The X-coordinate to move the mouse to."
            },
            "y": {
                "type": "integer",
                "description": "The Y-coordinate to move the mouse to."
            }
        },
        "required": ["x", "y"]
    }
}

current_mouse_coordinates_scheme = {
    "name": "current_mouse_coordinates",
    "description": "Gets the current X and Y coordinates of the mouse cursor.",
    "parameters": {
        "type": "object",
        "properties": {}
    }
}

click_mouse_scheme = {
    "name": "click_mouse",
    "description": "Performs a mouse click. Can be left, right, middle, single, double, etc.",
    "parameters": {
        "type": "object",
        "properties": {
            "button": {
                "type": "string",
                "description": "The mouse button to click: 'left', 'right', or 'middle'. Default is 'left'."
            },
            "clicks": {
                "type": "integer",
                "description": "The number of times to click. Default is 1."
            }
        }
    }
}

scroll_mouse_scheme = {
    "name": "scroll_mouse",
    "description": "Scrolls the mouse wheel up or down.",
    "parameters": {
        "type": "object",
        "properties": {
            "amount": {
                "type": "integer",
                "description": "The number of units to scroll. Positive for up, negative for down."
            }
        },
        "required": ["amount"]
    }
}

drag_mouse_scheme = {
    "name": "drag_mouse",
    "description": "Drags the mouse from its current position to the specified X and Y coordinates. Useful for selecting text or moving items.",
    "parameters": {
        "type": "object",
        "properties": {
            "x_to": {
                "type": "integer",
                "description": "The destination X-coordinate for the drag."
            },
            "y_to": {
                "type": "integer",
                "description": "The destination Y-coordinate for the drag."
            }
        },
        "required": ["x_to", "y_to"]
    }
}

press_hotkey_scheme = {
    "name": "press_hotkey",
    "description": "Presses a combination of keyboard keys simultaneously. For example, ('ctrl', 'c') to copy.",
    "parameters": {
        "type": "object",
        "properties": {
            "keys": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "A list of keys to press together, for example: ['alt', 'f4']"
            }
        },
        "required": ["keys"]
    }
}

copy_to_clipboard_scheme = {
    "name": "copy_to_clipboard",
    "description": "Copies the given text to the system clipboard.",
    "parameters": {
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "The text to be copied."
            }
        },
        "required": ["text"]
    }
}

write_text_scheme = {
    "name": "write_text",
    "description": "Types out the given text in the currently active window. Works with any language.",
    "parameters": {
        "type": "object",
        "properties": {
            "text": {
                "type": "string",
                "description": "The text to be typed."
            }
        },
        "required": ["text"]
    }
}

system_command_scheme = {
    "name": "system_command",
    "description": "Executes a system command, such as shutdown or restart. EXTREMELY DANGEROUS.",
    "parameters": {
        "type": "object",
        "properties": {
            "command": {
                "type": "string",
                "description": "The system command to execute. For Windows: 'shutdown /s /t 1' for shutdown, 'shutdown /r /t 1' for restart."
            }
        },
        "required": ["command"]
    }
}

get_processes_scheme = {
    "name": "get_processes",
    "description": "Returns a clean list of user-run applications, filtering out system processes.",
    "parameters": {
        "type": "object",
        "properties": {}
    }
}

currently_open_windows_scheme = {
    "name": "currently_open_windows",
    "description": "Returns a list of titles of all currently open windows.",
    "parameters": {
        "type": "object",
        "properties": {}
    }
}

manage_window_scheme = {
    "name": "manage_window",
    "description": "Finds a window by its title and performs an action on it.",
    "parameters": {
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title (or part of the title) of the window to manage. For example: 'foobar2000'."
            },
            "action": {
                "type": "string",
                "description": "The action to perform: 'activate', 'minimize', 'maximize', or 'close'. Default is 'activate'."
            }
        },
        "required": ["title"]
    }
}

open_program_scheme = {
    "name": "open_program",
    "description": "Opens a program or file using its full path.",
    "parameters": {
        "type": "object",
        "properties": {
            "path_to_exe": {
                "type": "string",
                "description": "The full path to the executable or file to open. For example: 'C:\\Program Files\\...\\chrome.exe'."
            }
        },
        "required": ["path_to_exe"]
    }
}


kill_process_by_name_scheme = {
    "name": "kill_process_by_name",
    "description": "Forcibly terminates a running process by its name (e.g., 'chrome.exe'). DANGEROUS.",
    "parameters": {
        "type": "object",
        "properties": {
            "process_name": {
                "type": "string",
                "description": "The name of the process executable to kill."
            }
        },
        "required": ["process_name"]
    }
}

get_system_volume_scheme = {
    "name": "get_system_volume",
    "description": "Получает текущее значение громкости системы в процентах.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

set_system_volume_scheme = {
    "name": "set_system_volume",
    "description": "Устанавливает абсолютное значение громкости системы. Принимает значение от 0 до 100.",
    "parameters": {
        "type": "object",
        "properties": {
            "level_volume": {
                "type": "integer",
                "description": "Целевой уровень громкости в процентах (например, 50).",
            },
        },
        "required": ["level_volume"] # Явно указываем, что этот параметр обязателен
    },
}

decrease_volume_scheme = {
    "name": "decrease_volume",
    "description": "Уменьшает громкость системы на указанное значение. По умолчанию уменьшает на 10%.", 
    "parameters": {
        "type": "object",
        "properties": {
            "amount": {
                "type": "integer",
                "description": "Значение в процентах, на которое нужно уменьшить громкость (например, 15).",
            },
        },
        # "required" здесь не нужен, так как у amount есть значение по умолчанию (он опциональный)
    },
}

increase_volume_scheme = {
    "name": "increase_volume",
    "description": "Увеличивает громкость системы на указанное значение. По умолчанию увеличивает на 10%.",
    "parameters": {
        "type": "object",
        "properties": {
            "amount": { 
                "type": "integer",
                "description": "Значение в процентах, на которое нужно увеличить громкость (например, 20).",
            },
        },
    },
}

get_habr_news_scheme = {
    "name": "get_habr_news",
    "description": "Receives the latest news from Habr.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}

get_system_metrics_scheme = {
    "name": "get_system_metrics",
    "description": "Gets the current load, temperature of the video card, processor and RAM in the system.",
    "parameters": {
        "type": "object",
        "properties": {}
    },
}


========================================
# Содержимое файла: assistant_tools/utils.py
========================================

# utils.py
import random 
import threading
import pygame
import time

SOUNDS = {
    'system_startup': ('sounds/system_startup1.mp3', 'sounds/system_startup2.mp3', 'sounds/system_startup3.mp3',),
    'select': ('sounds/select1.mp3', 'sounds/select2.mp3', 'sounds/select3.mp3',),
    'hard_processing': ('sounds/hard_processing1.mp3',), # ДОБАВИТЬ СЮДА ЗВУКОВ
    'lauch_vector_database': ('sounds/lauch_vector_database1.mp3',), # ЗАМЕНИТЬ НАЗВАНИЕ ЗВУКОВ НА start1, start2

    'processing': ('sounds/processing1.mp3',), # ДОБАВИТЬ СЮДА ЗВУКОВ
    'start_embedding_model': ('sounds/start_embedding_model1.mp3',), 
    'search': ('sounds/search1.mp3', 'sounds/search2.mp3',), # ДОБАВИТЬ СЮДА ЗВУКОВ


    'error': ('sounds/error1.mp3', 'sounds/error2.mp3', 'sounds/error3.mp3', 'sounds/error4.mp3', 'sounds/error5.mp3', 'sounds/error6.mp3', 'sounds/error7.mp3',),
    'silent_execution': ('sounds/silent_execution1.mp3', 'sounds/silent_execution2.mp3', 'sounds/silent_execution3.mp3', 'sounds/silent_execution4.mp3', 'sounds/silent_execution5.mp3', 'sounds/silent_execution6.mp3',),
    'access_error': ('sounds/access_error1.mp3', 'sounds/access_error2.mp3',),
    'access_critical_error': ('sounds/access_critical_error1.mp3',),
    'execution': ('sounds/execution1.mp3', 'sounds/execution2.mp3',),
    'notification': ('sounds/notification1.mp3', 'sounds/notification2.mp3',),

}

# '': ('sounds/.mp3', 'sounds/.mp3', ),

try:
    pygame.mixer.init()
    print("Pygame mixer initialized successfully.")
except pygame.error as e:
    print(f"Fatal error: Could not initialize pygame mixer. Sound will be disabled. Error: {e}")
    pygame = None # Отключаем pygame, если он не смог запуститься

def _play_sound_worker(sound_name: str):
    if sound_name in SOUNDS:
        sounds = SOUNDS[sound_name] 
        pygame.mixer.music.load(random.choice(sounds))
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy(): # Ждать окончания
            time.sleep(1)
    else:
        print("Error: Non-existent sound selected for function 'play_sfx()'.")

def play_sfx(sound_name: str):
    if not sound_name:
        return

    sound_worker_thread = threading.Thread(target=_play_sound_worker, kwargs={"sound_name": sound_name,}) # Создаем, собственно, отдельный поток
    sound_worker_thread.start()

if __name__ == "__main__":
    play_sfx("silent_execution")
    time.sleep(2)
    play_sfx("silent_execution")
    time.sleep(2)
    play_sfx("silent_execution")
    time.sleep(2)
    play_sfx("silent_execution")
    time.sleep(2)
    play_sfx("silent_execution")
    time.sleep(2)
    play_sfx("silent_execution")
    time.sleep(2)
    play_sfx("access_error")
    time.sleep(2)
    play_sfx("access_error")
    time.sleep(2)
    play_sfx("access_error")
    time.sleep(2)
    play_sfx("access_error")
    time.sleep(2)


========================================
# Содержимое файла: assistant_vector_database/add_new_memory.py
========================================

from datetime import datetime
import uuid
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma  

# Эмбеддинг модель, чтобы превращать запросы пользователя в векторы и искать похожие в базе данных
embedding_model = HuggingFaceEmbeddings(
    model_name = "BAAI/bge-m3", # Можно выбрать intfloat/multilingual-e5-large - она более быстрая, но менее точная
    encode_kwargs = {"normalize_embeddings": True} # при создании векторов из текста делать нормализацию
    ) 

# Векторная база данных
vectorstore = Chroma(
    collection_name="assistant_database", # Называем коллекцию внутри базы данных так
    embedding_function=embedding_model, # # Прикрепление модели эмбеддингов
    persist_directory="""./assistant_chroma_db""", # Сохранять в эту папку
    )

def add_new_memory(new_text: str):
    """Принимает текст, генерирует для него уникальный ID, добавляет текущую дату в метаданные и сохраняет в Chroma."""
    current_date = datetime.now().strftime("%d.%m.%Y")
    record_id = str(uuid.uuid4()) # Генерируем уникальный ID для записи
    
    metadata = {"creation_date": current_date}

    # Добавляем текст, его ID и его метаданные в базу.
    # Важно: add_texts принимает списки, поэтому оборачиваем все в []
    vectorstore.add_texts(
        texts=[new_text], 
        ids=[record_id], 
        metadatas=[metadata]
    )
    
    print(f"New entry added to memory: '{new_text}'")

if __name__ == "__main__":
    while True:
        new_record = input("\nВведите новую запись, которую нужно добавить (введите '0' для выхода): \n>> ")

        if new_record != "0":
            add_new_memory(new_record)
        else:
            break


========================================
# Содержимое файла: assistant_vector_database/database.py
========================================

# database.py
from assistant_tools.utils import play_sfx
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma  
from datetime import datetime
from assistant_event_bus.event_bus import subscribe, publish
import uuid
from assistant_general import general_settings as general_settings

# Эмбеддинг модель, чтобы превращать запросы пользователя в векторы и искать похожие в базе данных
print("Initialization of the embedding model.")
play_sfx("start_embedding_model")
embedding_model = HuggingFaceEmbeddings(
    model_name = "BAAI/bge-m3", # Можно выбрать intfloat/multilingual-e5-large - она более быстрая, но менее точная
    encode_kwargs = {"normalize_embeddings": True} # при создании векторов из текста делать нормализацию
    ) 

# Векторная база данных
print("Initialization of vector database.")
play_sfx("lauch_vector_database")
vectorstore = Chroma(
    collection_name="assistant_database", # Называем коллекцию внутри базы данных так
    embedding_function=embedding_model, # # Прикрепление модели эмбеддингов
    persist_directory="""./assistant_chroma_db""", # Сохранять в эту папку
    )

def add_new_memory(new_text: str):
    """Принимает текст, генерирует для него уникальный ID, добавляет текущую дату в метаданные и сохраняет в Chroma."""
    current_date = datetime.now().strftime("%d.%m.%Y")
    record_id = str(uuid.uuid4()) # Генерируем уникальный ID для записи
    
    metadata = {"creation_date": current_date}

    # Добавляем текст, его ID и его метаданные в базу.
    # Важно: add_texts принимает списки, поэтому оборачиваем все в []
    vectorstore.add_texts(
        texts=[new_text], 
        ids=[record_id], 
        metadatas=[metadata]
    )
    
    print(f"New entry added to memory: '{new_text}'")

def find_records_in_database(**kwargs):
    """Ищет записи в векторной базе данных и форматирует результат в читаемую строку."""

    # Если запрос пуст
    query = kwargs.get('query')
    if not query:
        return

    results_with_scores = vectorstore.similarity_search_with_score(query, k=general_settings.NUM_RECORDS_FROM_DATABASE)

    # Если база пуста
    if not results_with_scores:
        result = {"original_query": query, "database_context": "No relevant records were found in the database."}
        print("No relevant records were found in the database.")
        publish("USER_SPEECH_AND_RECORDS_FOUND_IN_DB", result)
        return 
    
    formatted_lines = []

    print("\nSearching for records in the database:")
    
    for document, score in results_with_scores:
        if score <= general_settings.SIMILARITY_THRESHOLD:
            # Если запись ДОСТАТОЧНО похожа, обрабатываем ее
            print(f"Record is relevant enough (score: {score:.2f}, threshold: {general_settings.SIMILARITY_THRESHOLD})")
            
            date = document.metadata.get('creation_date', 'Date not found')
            text = document.page_content
            
            # Добавим оценку в вывод для наглядности
            formatted_lines.append(f"[Score: {score:.2f}] {date}: {text}")
        else:
            # Если запись НЕдостаточно похожа, мы можем ее проигнорировать
            print(f"Record is NOT relevant enough (score: {score:.2f}, threshold: {general_settings.SIMILARITY_THRESHOLD}). Skipping.")
            # Мы можем либо ничего не делать, либо добавить сообщение об этом
            # formatted_lines.append(f"[Not relevant enough, score: {score:.2f}]") 

    # Если после фильтрации не осталось релевантных записей
    if not formatted_lines:
        final_string = "Found some records, but none were similar enough to the query."
    else:
        # Соединяем только релевантные строки
        final_string = "\n".join(formatted_lines)

    result = {"original_query": query, "database_context": final_string}
    print(f"\nFound records in database for query '{query}': \n{final_string}")
    
    publish("USER_SPEECH_AND_RECORDS_FOUND_IN_DB", result)


def initialize_database():
    subscribe("USER_SPEECH", find_records_in_database)

if __name__ == "__main__":
    while True:
        new_record = input("Введите новую запись, которую нужно добавить (введите '0' для выхода): \n>>")

        if new_record != "0":
            add_new_memory(new_record)
        else:
            break



========================================
# Содержимое файла: assistant_vector_database/delete_memory.py
========================================

import sys
import os
from datetime import datetime
import uuid
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma  

embedding_model = HuggingFaceEmbeddings(
    model_name = "BAAI/bge-m3", # Можно выбрать intfloat/multilingual-e5-large - она более быстрая, но менее точная
    encode_kwargs = {"normalize_embeddings": True} # при создании векторов из текста делать нормализацию
    ) 

# Векторная база данных
vectorstore = Chroma(
    collection_name="assistant_database", # Называем коллекцию внутри базы данных так
    embedding_function=embedding_model, # # Прикрепление модели эмбеддингов
    persist_directory="""./assistant_chroma_db""", # Сохранять в эту папку
    )

# ВСТАВЛЯЕМ СЮДА ID ЗАПИСЕЙ, КОТОРЫЕ СТОИТ УДАЛИТЬ
ids_to_delete = [
    "a1470d29-35e1-4dab-ac9f-9a5f06d1bb40",

]


def delete_specific_records(ids: list):
    """Удаляет записи из ChromaDB по списку их уникальных ID."""
    if not ids:
        print("Список ID для удаления пуст. Ничего не сделано.")
        return

    print(f"Происходит удаление записей, количество: {len(ids)} ")

    try:
        # Получаем записи, чтобы показать, что именно мы удаляем
        records_to_check = vectorstore.get(ids=ids, include=["documents"])
        
        if not records_to_check['ids']:
            print("Ни одна из указанных записей не найдена в базе данных.")
            return

        print("\n--- Будут удалены следующие записи: ---")
        for i, doc_id in enumerate(records_to_check['ids']):
            doc_text = records_to_check['documents'][i]
            print(f"  - ID: {doc_id}")
            print(f"    Текст: {doc_text}")
        
        # Запрашиваем подтверждение
        confirm = input("\nВы уверены, что хотите продолжить? (y/n): \n>> ")
        if confirm.lower() != 'y':
            print("Удаление отменено.")
            return

        # Удаляем записи по их ID
        vectorstore.delete(ids=ids)
        
        print(f"\n✅ Успешно удалено {len(ids)} записей.")

    except Exception as e:
        print(f"\n❌ Произошла ошибка при удалении: {e}")

if __name__ == "__main__":
    delete_specific_records(ids_to_delete)


========================================
# Содержимое файла: assistant_vector_database/inspect_memory.py
========================================

# inspect_memory.py
import chromadb

client = chromadb.PersistentClient(path="assistant_chroma_db") 

def inspect_memory():
    """Выводит ВСЕ записи в базе данных"""
    try:
        collection_name = "assistant_database"
        collection = client.get_collection(name=collection_name)
        print(f"Successfully connected to collection: '{collection_name}'")

    except Exception as e:
        print(f"Error connecting to collection: {e}")
        print("Available collections:", [col.name for col in client.list_collections()])
        exit()

    try:
        all_records = collection.get(
            include=["metadatas", "documents"]
        )

        num_records = len(all_records['ids'])
        print(f"\n--- Found {num_records} records in V.E.G.A. memory ---\n")

        for i in range(num_records):
            record_id = all_records['ids'][i]
            document = all_records['documents'][i]
            metadata = all_records['metadatas'][i]

            print(f"--- Record ID: {record_id} ---")
            print(f"Document (Text): {document}")
            print(f"Metadata: {metadata}")
            print("-" * 20 + "\n")

    except Exception as e:
        print(f"An error occurred while fetching records: {e}")


if __name__ == "__main__":
    inspect_memory()


========================================
# Содержимое файла: main.py
========================================

# main.py
import threading
import time
from assistant_brain.brain import initialize_brain, generate_general_greeting
from assistant_brain.hotkeys_manager import initialize_hotkeys_manager
from assistant_output.voice_output_eng import SpeechModuleENG
from assistant_output.voice_output_rus import SpeechModuleRUS
from assistant_tools.utils import play_sfx
from assistant_vector_database.database import initialize_database
from assistant_general.general_settings import choose_language

play_sfx('hard_processing')

initialize_brain() # вызывает subscribe("USER_SPEECH", generate_response), чтобы не импортировать сразу весь brain
initialize_database()
initialize_hotkeys_manager()

while True: 
    print("\nPlease, choose language for V.E.G.A.")
    command = input("'1' - russian, '2' - english, '3' - exit \n\n>> ")

    if command == "1": # Если русский язык
        speech_module = SpeechModuleRUS()
        speech_module.start()
        choose_language("RUSSIAN")
        break

    if command == "2": # Если английский язык 
        speech_module = SpeechModuleENG()
        speech_module.start()
        choose_language("ENGLISH")
        break

    if command == "3":
        print("Exit from the V.E.G.A. system.")
        exit()

    else:
        print("Invalid mode. Please try again.")

generate_general_greeting()

while True:
    input_mode = input("\nSelect the input mode ('1' - voice, '2' - text, '3' - output): ")
    if input_mode == "1":
        from assistant_input.voice_input import SpeechListener
        play_sfx("select")
        speech_listener = SpeechListener()
        speech_listener.start()
        break 

    elif input_mode == "2":
        from assistant_input.text_input import text_input_loop
        play_sfx("select")
        text_thread = threading.Thread(target=text_input_loop)
        text_thread.daemon = True
        text_thread.start()
        break 

    elif input_mode == "3":
        print("Logout from the V.E.G.A. system")
        exit()

    else:
        print("Incorrect mode. Please try again.")

# ЛОГГИРОВАТЬ В КРАТКОВРЕМЕННУЮ ПАМЯТЬ - ЧТО ТАМ ХРАНИТСЯ, КОГДА ВЕГА ЧТО-ТО ЗАПИСЫВАЕТ ИЛИ УДАЛЯЕТ
# ЛОГГИРОВАТЬ В КРАТКОВРЕМЕННУЮ ПАМЯТЬ - при каких условиях Вегой было сказаны фразы (например, помечать в скобках "фраза была сказана функцией спонтанных мыслей")

# Передавать Веге названия всех плейлистов Foobar2000 и их содержание

# Скачать Rammstein в Foobar2000.
# Уменьшать громкость остальных звуков при речи Веги, чтобы не перебивала её

# def all_sensors_and_information(): не забыть

# Реализовать запрос "Вега, сделай средний по размеру пост в наш канал о новых технологиях, а то что-то давно в канал ничего не публиковали. Порыскай на сайтах с актуальными новостями о новых технологиях, которые мы не затрагивали в нашем канале. Потом скопируй в буфер обмена содержание поста."
# Для этого нужно:
# 1) Навык парсинга новостных сайтов (например, TechCrunch, The Verge, Wired, Ars Technica, Hacker News, Slashdot, Gizmodo, Engadget, CNET, ZDNet)
# 2) Навык создания постов для телеграм-канала (с возможностью копирования в буфер обмена), хотя можно и просто просить копировать в буфер обмена

# Дать возможность Веге исказть любую информацию/новости, не только с Hubr
# Например, при запросе "Вега, какие там новые нейросети вышли?" - она искала эту информацию

# Также сделать так, чтобы из памяти выводились все факты по поводу моего канала (для анализа текущего состояния и планирования новых публикаций)
# Также пригодится, чтобы Веге передавалась текущая переписка с помощью навыка (описать в промпте, когда нужно применять его)

# Создать лист текущих задач: при простом диалоге Вега не должна оправлять флажок, а вот для важных задач и тех, которые не решаются здесь и сейчас (по типу "Вега, мониторь этот сайт на предмет новой информации")
# Вега применяет навык и оправляет флажок в этот файл, который будет просматриваться каждые 2 минуты
# Для этого нужно:
# 1) Навык создания/удаления/просмотра текущих задач (флажков)
# 2) Фоновая функция, которая каждые 2 минуты проверяет наличие новых флажков и выполняет их (вызывает навык мониторинга сайта, навык поиска информации и т.д.)


# дать Веге инструмент вспомнить информацию из памяти САМОЙ
# Например, Вега может сказать: "Сэр, вы упоминали, что хотите написать статью о новых технологиях. Могу я помочь вам с этим?"


# Случайные проактивные взаимодействия:
# Раз в случайное количество времени дается запрос к Веге с перечей ВСЕХ текущих датчиков: погода, время и дата, новости, логи внутренних диалогов, текущие процессы на пк, контекст экрана, ... , ... , всю картину мира
# В итоге Вега будет говорить что-нибудь интересное. ПРомпт можно начать со слов 
# "Ты заскучала. Ты - операционная система Сэра, и вот датчики: ... , вот кратковременная память: ... . Ты можешь делать что угодно: например, вызвать функцию вспоминания чего-либо из базы данных, просмотреть телеграм канал Сэра, посмотреть новости, погоду, время и дату, текущие процессы на ПК, контекст экрана и т.д.
# Ты можешь делать предложения Сэру, рассказывать интересные факты, шутить, предлагать помощь в работе и т.д. Главное - проявлять инициативу и быть полезной."

# Написать отдельную функцию, которая получает прогноз погоды на ЧАС
#(Minutely Forecast) - для осадков на ближайший час
# Этот API-ответ дает очень детальный прогноз осадков на ближайший час с шагом в одну минуту. Это самый точный способ узнать, начнется ли дождь или снег в течение следующего часа.
# Особенности:
# Данные на 60 минут вперед.
# Только информация об интенсивности осадков.
# Недоступно для всех локаций в мире.

# Разобраться с проблемой, когда Вега, допустим, записывает в память, однако отвечает после function calling = None.
# Пример:
# 2025-10-18 21:12:17,890 - google_genai.types - [WARNING] - Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.
# V.E.G.A.: None
# Тут она вызвала добавить в память новый факт, но в итоге ответила ничего, так как когда находится вызов функции - в дальнейшем коде нет проверки на текстовые части
# Для этого нужно:
# 1) Либо в коде, который обрабатывает ответ от модели, нужно проверять, есть ли текстовые части, и если есть - выводить их, даже если есть вызов функции




# ПОМОГИТЕ, СЛИШКОМ МНОГО ЗАДАЧ!

try:
    while True:
        time.sleep(1)
        # Для фоновых задач

        # Анализ погоды
        # Что делает: Через API погоды (например, OpenWeatherMap) проверяет прогноз на сегодня и завтра

        # Анализ новостей
        # Что делает: Проверяет новости на наличие важных событий, которые могут повлиять на планы пользователя

        # Анализ календаря
        # Что делает: Проверяет календарь пользователя на наличие предстоящих событий или встреч

        # Анализ системных ресурсов
        # Что делает: Мониторит использование CPU, памяти и диска, чтобы предупредить пользователя о возможных проблемах

except KeyboardInterrupt:
    print("\nThe program is ending.")



========================================
# Содержимое файла: tests/test1.py
========================================

import asyncio
from pyrogram import Client

# Получаем эти данные на https://my.telegram.org/auth (API development tools)
API_ID = 22543405  # <-- ВАШ API_ID
API_HASH = "b49fd0e149ca3bdfaed576836566605b"

SESSION_NAME = "vega_telegram_session" # Имя сессии. Можно выбрать любое. Pyrogram создаст файл сессии (например, vega_telegram_session.session)
VEGA_CHANNEL_USERNAME = "@VEGA_and_other_heresy" # Канал, данные которого мы хотим получить (используем @username или ID)

async def _get_channel_data(client: Client, channel_username: str, limit_posts: int): # Функция должна быть асинхронной (async def)
    """Получает факты о канале, количество подписчиков и все посты. Сначала выводит самые актуальные посты, двигаясь к старым."""
    print(f"Запрос данных для канала: \n-> {channel_username}")

    try:
        chat = await client.get_chat(channel_username) # Все сетевые вызовы Pyrogram требуют 'await'
    except Exception as e:
        print(f"Ошибка при получении информации о чате: {e}")
        return None

    channel_info = {
        "channel_name": chat.title,
        "description": chat.description if chat.description else "No description available.",
        "subscribers": chat.members_count,
        "last_posts": []
    }

    print(f"-> Получено название: {channel_info['channel_name']} | Подписчиков: {channel_info['subscribers']}")

    # history — это асинхронный итератор, мы перебираем его
    history = client.get_chat_history(channel_username, limit=limit_posts) # Получаем историю сообщений (постов) канала
    async for message in history: # Проходимся по всем постам, используем 'async for' для асинхронного итератора
        post_data = {
            "id": message.id,
            "date": message.date.strftime("%Y-%m-%d %H:%M:%S"),
            "text": message.text.strip() if message.text else "[Media / No Text]"
        }
        channel_info["last_posts"].append(post_data)
    
    return channel_info

async def get_channel_data(channel_username: str = VEGA_CHANNEL_USERNAME, limit_posts: int = 0):
    """Основная асинхронная функция для запуска Telegram-клиента и получения данных канала."""
    async with Client(SESSION_NAME, API_ID, API_HASH) as client: # Используем 'async with' для корректного асинхронного запуска

        data = await _get_channel_data(client, channel_username, limit_posts=limit_posts) # Вызываем асинхронную функцию с 'await'

        if data:
            posts = []
            for post in data['last_posts']:
                text = post['text'].replace('\n', ' ')
                posts.append(f"ID публикации: {post['id']}; Дата публикации: {post['date']}; Текст публикации: {text}.")

            final_posts = "\n\n".join(posts)

            print(f"Название канала: {data['channel_name']}; \n\nПодписчики: {data['subscribers']}; \n\nОписание: {data['description']}; \n\nПосты: \n{final_posts}")
            return f"Название канала: {data['channel_name']}; \nПодписчики: {data['subscribers']}; \nОписание: {data['description']}; \nПосты: \n{final_posts}"

if __name__ == "__main__":
    print("Запуск клиента Telegram.")
    # Запускаем асинхронную функцию
    asyncio.run(get_channel_data())





========================================
# Содержимое файла: tests/test2.py
========================================

import asyncio
import datetime

async def toast():
    now1 = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"Toast made at {now1}")
    await asyncio.sleep(5)
    now2 = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"Toast finished at {now2}")

async def make_toast():
    print("Making toast...")
    await toast()


if __name__ == "__main__":
    asyncio.run(make_toast())




========================================
# Содержимое файла: tests/test3.py
========================================

# import os
# from fuzzywuzzy import process

# MUSIC_LIBRARY_PATH = "C:\\Users\\ivanc\\Desktop\\Project_V.E.G.A\\VEGA_core\\assistant_music\\music"

# def find_best_match(song_query: str):
#     """Проходит по всем файлам, находит лучшее совпадение и возвращает его."""
    
#     all_tracks_names = []
    
#     # ПРАВИЛЬНЫЙ СПОСОБ использования os.walk
#     for root, dirs, files in os.walk(MUSIC_LIBRARY_PATH): # В os.walk: root — путь к текущей папке; dirs — список названий подпапок в этой папке; files — список названий файлов в этой папке
#         for filename in files:
            
#             clean_name = os.path.splitext(filename)[0]
#             all_tracks_names.append(clean_name)

#     print("--- Список для поиска ---")
#     print(all_tracks_names)
#     print("\n")

#     best_match = process.extractOne(song_query, all_tracks_names)
#     print(f"Результат поиска: {best_match}")
#     return best_match

# while True:
#     song = input("Введите песню для поиска: ")
#     if not song:
#         break
#     find_best_match(song)






import requests

try:
    print("Пытаюсь подключиться к Google...")
    response = requests.get('https://www.google.com', timeout=10)
    print(f"Успех! Статус-код: {response.status_code}")
    print("Значит, базовый выход в интернет из Python работает.")
except Exception as e:
    print("\nПИЗДЕЦ! Опять ошибка!")
    print(f"Не удалось подключиться. Ошибка: {e}")


========================================
# Содержимое файла: tests/test4.py
========================================

import sys

print(sys.executable)




========================================
# Содержимое файла: vosk_model/vosk-model-ru-0.42/decode.py
========================================

#!/usr/bin/env python3

from vosk import Model, KaldiRecognizer, SetLogLevel
import sys
import os
import wave

SetLogLevel(0)

if len(sys.argv) == 2:
    wf = wave.open(sys.argv[1], "rb")
else:
    wf = wave.open("decoder-test.wav", "rb")

if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != "NONE":
    print ("Audio file must be WAV format mono PCM.")
    exit (1)

model = Model(".")
rec = KaldiRecognizer(model, wf.getframerate())
rec.SetWords(True)

while True:
    data = wf.readframes(4000)
    if len(data) == 0:
        break
    if rec.AcceptWaveform(data):
        print(rec.Result())
    else:
        print(rec.PartialResult())

print(rec.FinalResult())


